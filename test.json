[
  [
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": " 第回Machine Learning minutes   Microsoft AI   Build  Updates Azure Machine Learningサービスを中心に",
      "description": "「Machine Learning minutes 」で、Azure Machine Learningサ     ",
      "link": "https://satonaoki.wordpress.com/2019/06/30/ml15-azure-ml-build-2019-updates/",
      "updated": "2019-06-30 05:47:13"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": " Azure Council Experts  ACE 第回定例会  Microsoft Azureアップデート情報       ",
      "description": "最近か月のMicrosoft Azureのアップデートをまとめました。過去のアーカイブは、ACEのページから     ",
      "link": "https://satonaoki.wordpress.com/2019/06/18/ace-azure-update-35/",
      "updated": "2019-06-17 19:00:15"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": " de code 振り返りNight   Data Platform",
      "description": "「de code 振り返りNight  Sponsored by Qiita」で、de code      ",
      "link": "https://satonaoki.wordpress.com/2019/06/10/decode19-data-platform-recap/",
      "updated": "2019-06-10 11:39:29"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": "de code セッション「Build  Azure AI   Data Platform最新アップデート」",
      "description": "de code で、「Build  Azure AI     Data Platform      ",
      "link": "https://satonaoki.wordpress.com/2019/05/29/decode19-dp10-build-2019-azure-ai-data-updates/",
      "updated": "2019-05-29 09:36:34"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": "Build 基調講演ライブ配信 日本時間  火 深夜    ",
      "description": "Microsoftの年次カンファレンス「Build 」の基調講演のライブ配信が行われます。日本時間では     ",
      "link": "https://satonaoki.wordpress.com/2019/05/06/build-2019-keynote/",
      "updated": "2019-05-06 05:49:59"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": " 第回Machine Learning minutes   Microsoft AI Updates",
      "description": "Machine Learning minutes で、Microsoft AIの概要、最近のアップデートの     ",
      "link": "https://satonaoki.wordpress.com/2019/04/27/ml15min-microsoft-ai-2/",
      "updated": "2019-04-27 06:25:30"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": " Azure Council Experts  ACE 第回定例会  Microsoft Azureアップデート情報       ",
      "description": "最近か月のMicrosoft Azureのアップデートをまとめました。過去のアーカイブは、ACEのページから     ",
      "link": "https://satonaoki.wordpress.com/2019/04/24/ace-azure-update-34/",
      "updated": "2019-04-23 18:24:24"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": " Azure Council Experts  ACE 第回定例会  Microsoft Azureアップデート情報       ",
      "description": "最近か月のMicrosoft Azureのアップデートをまとめました。過去のアーカイブは、ACEのページから     ",
      "link": "https://satonaoki.wordpress.com/2019/02/15/ace-azure-update-33/",
      "updated": "2019-02-15 08:09:55"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": "Azure 周年鼎談 Azureは「第形態」に入った",
      "description": "「AzureユーザーのためのAzure情報サイトクラウドboost」のオープンに合わせて、対談記事が出ました     ",
      "link": "https://satonaoki.wordpress.com/2019/02/10/azure-9-years/",
      "updated": "2019-02-09 21:11:55"
    },
    {
      "name": "S/N Ratio (by SATO Naoki)",
      "category": "Azure",
      "title": "コンテナを活用する精鋭たちが登壇  Kubernetesの使いどころや本番環境での注意点とは 【Container X mas Party】",
      "description": "月に開催したイベント「Container X mas Party with flexy」のレポート記事が出     ",
      "link": "https://satonaoki.wordpress.com/2019/01/29/container-xmas-party/",
      "updated": "2019-01-29 07:33:53"
    }
  ],
  [
    {
      "name": "AWS",
      "category": "AWS",
      "title": "What s New with AWS   Week of July   ",
      "description": "In this new episode  join Jeff Barr to learn what s new with Amazon EC  AWS Control Tower and VPC Traffic Mirroring  For the slate of recent launches  check out our  What s New  page at   ",
      "link": "https://www.youtube.com/watch?v=me6dxq9olyo",
      "updated": "2019-07-05 19:59:12"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "from scratch  Resource Manager Controls Task Distribution with Multiple Amazon EMR Clusters",
      "description": "Learn more about This Is My Architecture at   \nLearn how  from scratch  builds their resource manager function to control task distribution by the type and load and task order by the priority  and to scale multiple Amazon EMR clusters \n\nHost  Mitsuharu Hamba  Solutions Architect  AWS\nCustomer  Hiroaki Idobata  CTO  from scratch",
      "link": "https://www.youtube.com/watch?v=nM-AkqNh7Yo",
      "updated": "2019-07-05 20:05:55"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Welcome to AWS  Central Eastern Europe   Meet Gergely  Irina  Alexander  Marta   Atilla",
      "description": "For all roles within Emerging Markets  CEE visit    \nAs AWS expands into new markets across Central Eastern Europe and CIS  our builders move across the continent s  to shape the future of their local markets from one of our CEE hubs ",
      "link": "https://www.youtube.com/watch?v=atbPuw6HNh8",
      "updated": "2019-07-05 18:45:11"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Monitor Resource Changes with Amazon CloudWatch Events",
      "description": "Learn more about AWS Management and Governance at    \nIn this video we show you how to monitor for AWS resource changes  automate alerts based on those changes and invoke an action ",
      "link": "https://www.youtube.com/watch?v=-rQku_AeN_Y",
      "updated": "2019-07-05 17:48:18"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Managing Patching for your Amazon Machine Images",
      "description": "Learn more about AWS Management and Governance at    \nIn this video we show you how to manage your patching rules  release the latest updates and create secure instances ",
      "link": "https://www.youtube.com/watch?v=h5cq462pvAU",
      "updated": "2019-07-05 17:48:14"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Enforce Compliance with AWS Config",
      "description": "Learn more about AWS Management and Governance at    \nIn this video we show you how to create rules in AWS Config  evaluate AWS resources and enforce compliance ",
      "link": "https://www.youtube.com/watch?v=X_fznJtSyV8",
      "updated": "2019-07-05 17:48:07"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Collect Metrics and Logs from Amazon EC instances with the CloudWatch Agent",
      "description": "Learn more about AWS Management and Governance at    \nIn this video we show you how you configure and deploy the CloudWatch Agent  collect metrics and logs from your Windows instances and provide actionable data ",
      "link": "https://www.youtube.com/watch?v=vAnIhIwE5hY",
      "updated": "2019-07-05 17:48:52"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Patching for your Amazon EC Instances",
      "description": "Learn more about AWS Management and Governance at    \nIn this video we show you how to automate your patching of your Amazon EC instances by using Systems Manager Patch Manager  Patch Groups and setting up a patch schedule ",
      "link": "https://www.youtube.com/watch?v=ABtwRb9BFY4",
      "updated": "2019-07-05 17:47:57"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "TrustSquare Swiss Blockchain Hackathon   AWS Sizzle Reel",
      "description": "Learn more about Amazon Managed Blockchain at    \nDevelopers  go build decentralized applications with Amazon Managed Blockchain  a fully managed service that makes it easy to create and manage scalable blockchain networks using the popular open source frameworks Hyperledger Fabric and Ethereum ",
      "link": "https://www.youtube.com/watch?v=zI_sZ8sNaUc",
      "updated": "2019-07-03 20:51:07"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Introduction to Amazon Aurora   Relational Database Built for the Cloud   AWS",
      "description": "Learn more about Amazon Aurora at    \nThis animated video of Amazon Aurora gives an overview of this built for the cloud relational database management system  RDBMS   Amazon Aurora is a MySQL and PostgreSQL compatible relational database that gives you the performance and availability of commercial grade databases at  th the cost ",
      "link": "https://www.youtube.com/watch?v=FzxqIdIZ9wc",
      "updated": "2019-07-03 20:11:12"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "How to Set up a Lambda Edge Function",
      "description": "Learn more about Lambda Edge and the most common use cases here  \nLearn how to set up an AWS Lambda Edge function  In  steps  you will learn how to add security and privacy headers to HTTP responses that Amazon CloudFront receives from your origin \n\nLambda Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application  which improves performance and reduces latency  With Lambda Edge  you don t have to provision or manage infrastructure in multiple locations around the world ",
      "link": "https://www.youtube.com/watch?v=_LdGsvrD9gU",
      "updated": "2019-07-03 21:59:23"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Being Sarah Nahm  A Day in the Life of a Software CEO",
      "description": "Learn more about AWS Startups at    \nSarah Nahm is the CEO at Lever  a software company in San Francisco that s building modern talent software to help companies power their next generation recruiting  For Nahm  a typical day is shaped by a series of highly intentional interactions with her team  Some of these are scheduled  some are serendipitous  all are crafted to prime her employees  and therefore her company  for success ",
      "link": "https://www.youtube.com/watch?v=UUMQMmawJxg",
      "updated": "2019-07-03 18:40:08"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Researchers Are Using Machine Learning to Screen for Autism in Children",
      "description": "To learn more about research projects like this that are enabled by AWS  see the AWS Machine Learning Research Awards website    \nDetecting and starting treatment of autism spectrum disorder  ASD  at an age of  to  months can increase a child s IQ by up to  pointsーin some cases moving them into the “average  child IQ range of    or above it ーand improving the child s quality of life significantly  Researchers at Duke University are using Machine Learning on AWS to create a faster  less expensive  more reliable  and more accessible system to screen children early for ASD ",
      "link": "https://www.youtube.com/watch?v=YQpTlnWYAqE",
      "updated": "2019-07-03 18:14:11"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "Innovate and Modernize on Your Terms  Deploy Oracle Workloads on VMware Cloud on AWS",
      "description": "Learn more about VMware Cloud on AWS at    \nDiscover how your organization can take advantage of the rapid scalability of VMware Cloud on AWS for your Oracle databases and applications ",
      "link": "https://www.youtube.com/watch?v=ON90LmH1XZg",
      "updated": "2019-07-03 20:04:53"
    },
    {
      "name": "AWS",
      "category": "AWS",
      "title": "How do I create an isolated Python  environment with Boto  on Amazon EC using virtualenv ",
      "description": "Find more details in the AWS Knowledge Center  \nLuke  an AWS Cloud Support Engineer  shows you how to create an isolated Python  environment with Boto  on Amazon EC using virtualenv ",
      "link": "https://www.youtube.com/watch?v=zwZ5hlxsLks",
      "updated": "2019-07-02 17:30:33"
    }
  ],
  [
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "AWS Floor News   June   Hebrew",
      "description": "Learn all about what s new in AWS this June \n\nAmazon Ground Truth   Set up and manage labeling jobs for highly accurate training datasets using active learning and human labeling \nAmazon Personalize Now Generally Available   \nAmazon Managed Streaming for Apache Kafka  Amazon MSK  is now Generally Available   \nAmazon Aurora Serverless MySQL  Now Supports Data API   \nAmazon Textract   Now Generally Available   ",
      "link": "https://www.youtube.com/watch?v=_Un1poEIPKw",
      "updated": "2019-07-03 20:37:36"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Serverless Streams  Topics  Queues    APIs  How to Pick the Right Serverless Application Pattern",
      "description": "  Evaluate and pick the event source for your serverless application built with AWS Lambda   Understand how to architect with the full serverless stack   Understand how to implement best practices and scaling patterns for serverless applications",
      "link": "https://www.youtube.com/watch?v=shQpw6xjkko",
      "updated": "2019-07-01 04:06:28"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "How AWS Migration Hub Helps You Plan  Track  and Complete Your Application Migrations",
      "description": "  Learn how to profile workloads and identify utilization patterns to make better migration decisions   Learn how to use AWS Migration Hub to generate recommendations for right sized EC instances   Learn how to analyze application dependencies collected from AWS Application Discovery Service and other data sources using Amazon Athena and Amazon QuickSight",
      "link": "https://www.youtube.com/watch?v=PVyWqViXieY",
      "updated": "2019-06-28 11:06:52"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Gain Control Over Your License Usage and Reduce Costs Using AWS License Manager",
      "description": "  Learn how to configure licensing rules and track different types of licenses across multiple AWS accounts   Understand best practices to attach license configurations to instances running on premises for central management of licenses   Learn how to use AWS License Manager for easier licensing true ups and audits",
      "link": "https://www.youtube.com/watch?v=r4X6YtSiFWw",
      "updated": "2019-06-26 23:12:55"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Developing Intelligent Robots with AWS RoboMaker   AWS Online Tech Talks",
      "description": "  Understand the basics of AWS RoboMaker   Explore key use cases for intelligent robotic applications   Explore the implementation of machine learning and reinforcement learning in robotics",
      "link": "https://www.youtube.com/watch?v=4y9X2CtJCxs",
      "updated": "2019-06-28 22:00:26"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "How to Build Your Cloud Enablement Engine with the People You Already Have   AWS Online Tech Talks",
      "description": "  Understand the differences between activity based and product based operating models  and how they inform migration paths to AWS   Understand why you need a Cloud Business Office and Cloud Platform Engineering team   Identify skills needed to staff your Cloud Business Office and Cloud Platform Engineering team",
      "link": "https://www.youtube.com/watch?v=k5jjRPgw_so",
      "updated": "2019-06-26 08:31:16"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Running Enterprise CI CD Workloads with Amazon EC Spot Instances and CloudBees",
      "description": "  Learn how to launch CD pipelines in the AWS cloud with EC Spot Instances   Learn how to use multi instance EC Auto Scaling Groups with a Kubernetes specific architectural style for continous delivery   Understand the benefits and cost saving potential of using Amazon EC Spot Instances",
      "link": "https://www.youtube.com/watch?v=us7bLV1EwhE",
      "updated": "2019-06-26 19:46:43"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Build Your Data Lake on Amazon S   AWS Online Tech Talks",
      "description": "  Learn why Amazon S is the ideal place to build your data lake   Understand which features to use to optimize your data lake for cost and performance   Understand which integrations can be leveraged  including FSx for Lustre",
      "link": "https://www.youtube.com/watch?v=ccIBYUjnt74",
      "updated": "2019-06-25 09:04:07"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Securing Your Devices from the Edge to the Cloud   AWS Online Tech Talks",
      "description": "  Learn how AWS IoT Edge services secure your hardware   Learn how AWS IoT Cloud Services secure your data   Learn how to mitigate and take action with AWS IoT services",
      "link": "https://www.youtube.com/watch?v=cPOk81pVWM0",
      "updated": "2019-06-25 02:01:22"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Replace Your On Premises VDI with End User Computing on AWS   AWS Online Tech Talks",
      "description": "  Learn why customers are moving to EUC solutions on AWS   Learn how to start moving your EUC solutions to AWS   Understand best practices when moving to EUC solutions on AWS",
      "link": "https://www.youtube.com/watch?v=P3DpEbk5jHk",
      "updated": "2019-06-24 20:59:12"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Create a Digital Asset with Amazon Sumerian and AWS IoT   AWS Online Tech Talks",
      "description": "  Learn how to import and animate digital assets of a physical machine inside Amazon Sumerian   Learn how to create a bi directional control system using AWS IoT and Amazon Sumerian   Learn how to show alerts and troubleshoot walk throughs in Sumerian based on anomalies and failures on physical machines",
      "link": "https://www.youtube.com/watch?v=4XEY_TgxkEI",
      "updated": "2019-06-20 21:35:37"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Container Security  and Beyond   AWS Online Tech Talks",
      "description": "  Learn about running containers securely   Learn about handling sensitive data and permission management when working with containers   Learn about auditing best practices",
      "link": "https://www.youtube.com/watch?v=Cp4rdlsQORo",
      "updated": "2019-06-21 01:17:04"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Unify Your Data Warehouse and Data Lake with AWS   AWS Online Tech Talks",
      "description": "  Learn how to break data silos by quickly making all of your data in Amazon S available for analysis with Amazon Redshift for data warehousing workloads  while also being able to flexibly use other analytical engines such as Amazon Athena for ad hoc queri   Learn how to analyze data between the data warehouse and your data lake together in the same SQL query using Amazon Redshift spectrum  and learn about some recent updates to the feature   Learn how to break information silos among lines of business in your organization by sharing data in your data lake and data warehouse without needing to duplicate it",
      "link": "https://www.youtube.com/watch?v=KdqWb6M0TiM",
      "updated": "2019-06-21 15:28:32"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Root Cause and End user Impact Analysis Using AWS X Ray   AWS Online Tech Talks",
      "description": "  Go beyond basic use cases and learn how to extract deeper value from X Ray   Learn how to leverage X Ray to track and alert on performance trends  and understand customer impact   Learn best practices to monitor serverless applications",
      "link": "https://www.youtube.com/watch?v=Adah1LeaPWg",
      "updated": "2019-06-20 00:32:32"
    },
    {
      "name": "AWS - Webinar Channel",
      "category": "AWS",
      "title": "Learn How Amazon com Uses Amazon GuardDuty to Protect Its Infrastructure   AWS Online Tech Talks",
      "description": "Join us for this customer showcase to learn how one of AWS  largest customers  Amazon com  uses Amazon GuardDuty to detect and remediate threats  You will learn about the threat detection and remediation capabilities of Amazon GuardDuty and best practices to deploy  manage  and operationalize Amazon GuardDuty  The speakers will walk through real world threat scenarios and answer any questions about recommended practices regarding deploying threat detection and response capabilities \n\nLearning Objectives \n   Explore a real world case study of how Amazon com uses Amazon GuardDuty\n   Understand best practices to deploy  manage  and operationalize Amazon GuardDuty\n   Learn how Amazon com acts on Amazon GuardDuty findings and learn about remediation workflows",
      "link": "https://www.youtube.com/watch?v=dOAdH2uSJS0",
      "updated": "2019-06-20 22:11:42"
    }
  ],
  [
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "【AWS Black Belt Online Seminar】AWS Summit Tokyo Osaka 振り返り 今年前半の重要アップデートまとめ",
      "description": "AWS公式オンラインセミナー   \n過去資料  ",
      "link": "https://www.youtube.com/watch?v=p6XU4Pg7dcw",
      "updated": "2019-07-05 13:46:49"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "AWS Service Update   ",
      "description": "AWS無料オンデマンドセミナー≫\n過去資料≫",
      "link": "https://www.youtube.com/watch?v=7TqqNk3ip28",
      "updated": "2019-07-05 04:55:13"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "appsyncを使ったServerlessアーキテクチャ",
      "description": "急速に進化しているソフトウェアエコシステムの中で、GraphQLとServerlessのつの技術が際立って注目されています。フロントエンドでは、GraphQL型システムによる堅牢性はもちろんのこと、APIを利用するうえで全く新しい体験を得ることができます。一方、バックエンドではServerlessによってランタイムの管理をサービスに任せることができ、インフラを意識することなく開発にフォーカスできるようになりました。GraphQLとServerlessの概念をわかりやすく説明し、これらがRESTの課題をいかに解決するか、RESTからGraphQLに変わる際に設計がどう変わるのかなどポイントをしぼって解説します。\n\n※年月日AWS Expert Online for JAWS UGでのセッション内容です。\n\nAWSについて詳細は、。",
      "link": "https://www.youtube.com/watch?v=A83M-JDM6x8",
      "updated": "2019-07-04 14:49:07"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "FOLIO様におけるAWS活用事例",
      "description": "年月に創業した株式会社FOLIOは、国内株を取り扱うオンライン証券会社としては、およそ年ぶりに誕生したFintechスタートアップです。テーマ型投資という新しいビジネスモデルを掲げる同社が選んだのはAWS。様々な統制やセキュリティ基準への対応が必要な金融業において、『AWS Fintechリファレンスアーキテクチャ』を活用できたことは大きかったと、SRE部部長の澤田氏は語ります。証券システムにおいてはまだ珍しいと言える、マイクロサービスアーキテクチャの採用、ブルー グリーンデプロイメントによる運用などの先進的な取り組みを推進しています。\n\nAWS Fintechリファレンスアーキテクチャについて詳細は、。",
      "link": "https://www.youtube.com/watch?v=01KbP8Dx2B0",
      "updated": "2019-07-04 06:53:31"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "【AWS Black Belt Online Seminar】Amazon MQ",
      "description": "AWS公式オンラインセミナー   \n過去資料  ",
      "link": "https://www.youtube.com/watch?v=-qHefQ3JoX8",
      "updated": "2019-07-03 20:52:15"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "SAP Intelligent Enterpriseを実現するためのAWSサービス活用  AWS Summit Osaka ",
      "description": "河原哲也\nアマゾンウェブサービスジャパン株式会社技術統括本部ソリューションアーキテクト\n\n基幹系アプリケーションSAP S HANAを支えるSAP本稼働認定Amazon ECインスタンスの最新情報、またSAP社が掲げるインテリジェントエンタープライズの実現に向けたSAPシステムと各種AWSサービスの組み合わせによるデジタルトランスフォーメーションの実装例を説明します\n\nAWSの詳細については。",
      "link": "https://www.youtube.com/watch?v=KgJHDTBTFvs",
      "updated": "2019-07-03 15:43:48"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "AWS Summit Osaka   ダイジェスト",
      "description": "年月日 木 に開催されたAWS Summit Osakaのダイジェストです。当日会場へお越しいただけなかった方もダイジェストでお楽しみください。\n\nAWS Summitは、クラウドコンピューティングコミュニティが一堂に会して、アマゾンウェブサービス AWS に関する情報交換やコラボレーション、学習を行うことができる日本最大級のカンファレンスです。世界ヵ国都市で開催され、あらゆる業界や学習レベルの技術者が、AWSを利用することでいかに自社のビジネスを迅速に革新し、柔軟で信頼性の高いソリューションを大規模に提供できるかを発見できる機会です。\n\nAWSについての詳細は、。",
      "link": "https://www.youtube.com/watch?v=uqdV8JtQouo",
      "updated": "2019-06-27 12:17:26"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "【AWS Black Belt Online Seminar】Amazon DocumentDB  with MongoDB Compatibility ",
      "description": "AWS公式オンラインセミナー   \n過去資料  ",
      "link": "https://www.youtube.com/watch?v=RTfCVlo1EoA",
      "updated": "2019-06-27 23:42:32"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "リフト シフトから始めるレガシー脱却への挑戦大規模コンテンツ配信サービスの移行実例  AWS Summit Tokyo ",
      "description": "伊達乾\nニフティ株式会社WEB事業部WEBサービス開発グループシニアエンジニア\n添野翔太\nニフティ株式会社WEB事業部WEBサービス開発グループ\n\nニフティでは、これまでもポータルサイトならびにを超えるウェブサービスのインフラ基盤にクラウドを採用していた。しかし、レガシーなアーキテクチャのシステムのため運用負荷が高く、開発生産性の向上も困難であった。そういった課題を解決できるようなモダンなシステムに作り変えるべく移行を進めている。芸能、国内、海外など気になるニュースやオリジナルニュースコンテンツを配信し、掲載媒体社数社超えの niftyニュースの移行をベースに、クラウド間移行の実例とぶつかった課題を紹介する。\n\nAWSの詳細については。",
      "link": "https://www.youtube.com/watch?v=kJiMh77ySJ0",
      "updated": "2019-06-24 13:10:37"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "AWS Summit Tokyo    Day ダイジェスト",
      "description": "年月日 金 に開催されたAWS Summit Tokyo Day のダイジェストです。\n当日会場へお越しいただけなかった方もダイジェストでお楽しみください。\n\nAWS Summitは、クラウドコンピューティングコミュニティが一堂に会して、アマゾンウェブサービス AWS に関する情報交換やコラボレーション、学習を行うことができる日本最大級のカンファレンスです。世界ヵ国都市で開催され、あらゆる業界や学習レベルの技術者が、AWSを利用することでいかに自社のビジネスを迅速に革新し、柔軟で信頼性の高いソリューションを大規模に提供できるかを発見できる機会です。\n\nAWSについての詳細は、。",
      "link": "https://www.youtube.com/watch?v=T753VKlwKHE",
      "updated": "2019-06-24 01:21:09"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "【AWS Black Belt Online Seminar】Dive deep into AWS Chalice",
      "description": "発表内容に関しまして、 t \nに訂正がございましたので、併せて修正させていただきます。\n訂正内容 \n  誤  バイト型ではなくて文字列型\n  正  バイト型ではなくて dict 型",
      "link": "https://www.youtube.com/watch?v=u4LKbQZawaQ",
      "updated": "2019-06-27 13:02:16"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "「クラウドで実現するJTBのビジネス基盤」年企業の決断  AWS Summit Tokyo ",
      "description": "田村直樹\n株式会社JTB Web販売部システム統括部長\n山北育英\n株式会社JTB Web販売部システム担当部長\n\n『マーケットイン思考』と『データドリブン』の徹底により、お客様ニーズを捉えたサービス提供とマーケティング高度化し、ビジネスの最大化を実現するための基盤造りを目指している。\nJTBホームページを基軸としてあるべき姿を実現させるため、データに基づいた販売サイクルを進行させるべく、アプリケーションとインフラのボーダレスが特徴的なAWSを最大限活かし、理想のビジネスを実現したいと考えている。\n\nAWSの詳細については。",
      "link": "https://www.youtube.com/watch?v=z_lDYYBanzc",
      "updated": "2019-06-19 09:48:18"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "基調講演 デジタル変革の最前線で選ばれ続けるクラウドへデジタライゼーション時代のカスタマーサクセス  AWS Summit Tokyo ",
      "description": "長崎忠雄\nアマゾンウェブサービスジャパン株式会社代表取締役社長\n朝日宣雄\n三菱電機株式会社リビング・デジタルメディア事業本部リビング・デジタルメディア技術部長\n平野未来\n株式会社シナモンCEO\n前島一就\nニフティ株式会社取締役 兼 執行役員 兼 CIO 兼 ITシステム統括部長\n\nアマゾンウェブサービス AWS は、「地球上で最もお客様を大切にする企業」であり、お客様の成功である「カスタマーサクセス」を年以上徹底的に考え、使い続けて頂くためのよりよい新機能、新サービス、機能改善を行い、その約 がお客様からの意見や要望に基づいて実現されています。\n革新的な進化を続けるITを提供しお客様の多岐に渡る挑戦を支え続けるAWSが、なぜお客様から選ばれ続けられるクラウドであるのかという理由と、デジタライゼーションを実現しカスタマーサクセス クラウドによりお客様の事業の差別化による成功 を起こすためのテクノロジーのトレンドを、お客様事例をまじえてご紹介します。\n\nAWSの詳細については。",
      "link": "https://www.youtube.com/watch?v=mYJOn8l1x20",
      "updated": "2019-06-18 10:35:10"
    },
    {
      "name": "AWS - Japan",
      "category": "AWS",
      "title": "【AWS Black Belt Online Seminar】AWS Config",
      "description": "アマゾンウェブサービス AWS は現在、低コストを実現する拡張性をもったクラウドインフラプラットフォームで、世界ヵ国の数十万に及ぶビジネスを支援しています。米国、欧州、アジア、そして日本におけるデータセンターで、あらゆる業種のお客様に次のようなメリットをご活用いただいております。\n\nAWSについて詳細は、。",
      "link": "https://www.youtube.com/watch?v=vnqX0gMj6jw",
      "updated": "2019-06-27 21:17:00"
    }
  ],
  [
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "動的機能モジュールからコードにアクセスするパターン",
      "description": "この記事はWojtek Kaliciński nbsp によるAndroid Developers   Mediumの記事 Patterns for accessing code from Dynamic Feature Modules を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   この投稿では、プロジェクトで動的機能モジュールを使う際によく発生する問題を解決する方法を紹介します。その問題とは、コンパイル時に、ベースとなるアプリは動的機能モジュール内に定義されたコードにアクセスできないことです。これを行うには、リフレクションを使わなければなりませんリフレクションという言葉を見ただけで逃げ出したくなるデベロッパーもいるでしょう。しかし、リフレクションの呼び出しを回行うだけで、必要なものに安全にアクセスできるいくつかのエレガントなソリューションが存在します。そしてなんと、その数をゼロにできる賢い最適化テクニックも存在します 利用できるアプローチは何種類かあります。ここでは、ServiceLoader、リフレクション、そしてDaggerと連携する方法について紹介します。サンプルのモジュールアプリは、GitHubで公開しています。動的機能モジュールを使う理由昨年、AndroidアプリバンドルとGoogle Playのダイナミック配信のリリースにより、デベロッパーがアプリのサイズを小さくしやすくなり、リリースプロセスが効率化されました。アプリバンドルのメリットの詳細もご覧ください。しかし、アプリバンドルへの切り替えは最初のステップに過ぎません。この新しいフォーマットにより、コードベースをモジュール化してさらにカスタマイズした体験をユーザーに提供する可能性が開かれます。Google I O では、オンデマンドモジュールと条件付きモジュールが一般公開版として利用できるようになったことをお知らせしました。これによって、アプリの一部をいつインストールするかを決めることができます。オンデマンドモジュールは、ユーザーがアプリのオプション機能を利用したときに、Play Core APIを使って新しいコードやリソースをインストールします。条件付きモジュールが利用できるかどうかは、ハードウェア機能やソフトウェア機能、ユーザーの国、Android OS nbsp のバージョンなどのユーザーの端末構成に応じてインストール時に決まります。こういったカスタマイズ可能な新しいタイプのモジュールに共通しているのは、com android dynamic feature Gradleプラグインを使っていることです。これを、動的機能モジュールまたはDFMと呼びたいと思います。アプリをAndroidアプリバンドルとして配布する場合、Playストアがこれらのモジュールをユーザーの端末に配信します。この配布は、個別のAPKとして適切なタイミングで行われます。ライブラリから動的機能モジュールへライブラリモジュールをDFMに移行するには、プロジェクト構造に重要な変更を行う必要があります。この変更によって、ベースとなるcom android applicationとすべてのダイナミック機能との間の依存関係が反転します。動的機能モジュール 上部 はベースとなるアプリに依存。アプリは通常のライブラリに依存。ライブラリを使用する標準のプロジェクト構造では、ベースとなるcom android applicationモジュールがcom android libraryモジュールに依存します。つまり、ベースモジュールは、ライブラリで定義されたすべてのクラスを自由に使うことができます。しかし、DFMを使う場合、ベースとなるcom android applicationは、com android dynamic featureモジュールに依存されることになります。つまり、DFMはベースモジュールとライブラリで定義されたすべてのクラスを使うことができますが、コンパイル時にベースとなるアプリからDFMで定義されたコードを参照することはできません。DFMから生成されたAPKがアプリにインストールされると 配信メカニズムがインストール時、オンデマンド、条件付きのどれであっても 、実行時にClassLoaderがコードを利用できるようになります。オンデマンドモジュールを使う場合は、必ずPlay Core LibraryのSplitCompatを使うようにしてください。ユースケースにもよりますが、この問題を解決する明らかなソリューションがいくつか存在します。DFMで行うのがActivityを起動するだけの場合、単純にstartActivityに名前を渡すだけで済みます Stringとして渡します。コンパイル時にクラスにアクセスできないので、 class記法は使えません 。動的機能モジュールのサンプルで使っているのは、このアプローチです。Play Core Libraryを使ってオンデマンドモジュールをインストールする方法を知りたい方も、こちらを確認してみてください。googlesamples android dynamic features以降のセクションでは、ベースモジュールでインストール済みのDFMからカスタムクラスを読み込んでアクセスするつの方法を紹介します。本記事で紹介するサンプルの完全なコードは、動的コード読み込みサンプルにあります。googlesamples android dynamic code loadingプロジェクトのセットアップここで紹介するサンプルコードは、ユーザーがボタンを押すとカウンターが増えるという単純なアプリです。カウンターはアプリが再起動するたびにリセットされます。ただし、カウンターの状態をストレージに保存するコードを含むオンデマンドモジュールがインストールされると、再起動してもリセットされなくなります。もちろん、実際のアプリでこのようなことは行わないでしょうが、試してみることができる単純な事例として紹介します。動的に読み込むコードを作成する際には、これを応用することができます。サンプルコードは、共通部分をメインのソースセットに含め、動的にコードにアクセスする各アプローチをバリアントとして実装する構成になっています。ベースモジュールの「app」には、以下の共有クラスが含まれています。UIを含むMainActivity。ロジックを含むAbstractMainViewModel。ただし、各ビルドバリアントでサブクラス化し、abstract fun initializeStorageFeature  の実装を提供する必要がある。つのアプローチは、Android nbsp Studioのビルドバリアントを選択することによって切り替えます。storageモジュールは、オンデマンド配信用に設定された動的機能モジュールです。この記事で注目するのは、ベースとなるapp nbsp モジュールからstorageモジュール内にあるクラスにアクセスする方法です。インターフェースと実装を分ける最初に、動的機能のコードで使用するインターフェースを作成する必要があります。このインターフェースは、機能で実装されているすべての関数を直接呼び出せるように、appモジュールで定義します。今回のストレージ機能のインターフェースは簡単で、整数の保存と取得を行える必要があります。インターフェースの実装はDFMモジュールの定義に任せるので、実装者は自由にコンストラクタを決めることができます。今回のストレージ機能にはContextが必要で、他の依存性 たとえばLoggerなど もリクエストするかもしれません。そのため、実装は次のようになります。StorageFeatureを実装した具象クラスのインスタンスを取得しやすくして、インスタンス化を実装者に任せられるように、ストレージ機能を提供するProviderインターフェースも定義します。このインターフェースにはget  メソッドだけがあり、必要な依存関係を与えると、すぐに使うことができるStorageFeatureを返します。このインターフェースもベースモジュールに格納し、実装はDFMに含めます。インターフェースの定義はこちらから確認できますが、ここで紹介したコードとは微妙な違いがあり、機能の依存性を保持する別のインターフェースDependenciesを使っています。これは、後ほど登場するDaggerセットアップを使う場合に役立ちますが、他のアプローチには不要です。アプローチ 直接リフレクションを呼び出すベースとなるアプリモジュールでStorageFeatureインスタンスを取得する最も単純な方法は、回だけリフレクションを呼び出すことです。これを読みながらサンプルを確認している方は、reflectDebugビルドバリアントに切り替えてください。まず、reflectソースセットのMainViewModel実装を見てみましょう。ここでは、標準のClass forName  を使ってProviderのクラスハンドルを取得しています。次に、KotlinのobjectInstance機能によって、とても美しい方法でシングルトンインスタンスを取得しています。これが可能なのは、ProviderがKotlin nbsp のobjectとして宣言されているからです。Kotlinを使っていない方やobjectを使いたくない方は、通常のClass newInstance  呼び出しを使うこともできます。Providerへの参照を取得できたら、もうリフレクションの呼び出しは必要ありません。単純にget  を利用してStorageFeatureインターフェース経由で機能のコードを呼び出すだけです。この呼び出しでは、コンパイル時に安全性が保証されます。アプローチ  ServiceLoaderを利用する次のアプローチでは、直接リフレクションを利用するのではなく、ServiceLoaderを使ってインターフェースの実装を読み込みます。このアプローチは、同じインターフェースを持つ複数の実装を読み込みたい場合に特に便利です。残念なことに、デフォルトのServiceLoaderにはAndroidのパフォーマンスへの重大な影響があります。そのため、最新バージョンのR Android Gradle Plugin  以降に含まれる Xブランチ を利用しており、コード圧縮と最適化を有効にしていない限り、このアプローチを利用するべきではありません。ServiceLoaderとR最適化を利用すると、最終的なバイトコードからリフレクションを完全に削除することができます。ServiceLoaderはとても簡単に利用できます。次にMainViewModelの実装を示します StudioでserviceLoaderReleaseビルドバリアントに切り替えてください 。Providerを取得するには、検索するインターフェースを最初の引数として渡してServiceLoader load  を呼び出します。R最適化を有効にするには、次のつの条件を満たす必要があります。つの引数があるバージョンの nbsp load  を呼び出す必要がある。両方の引数でクラス定数 Javaでは class、Kotlinでは  class java を使用する必要がある。返されたServiceLoaderに対して、iterator  以外のメソッドを呼び出してはいけない。返されたServiceLoaderに対して反復処理を行うことで、見つかったすべてのインターフェース実装のインスタンスを取得することができます。ServiceLoaderは、DFMのMETA INFフォルダに格納する必要があるファイルに基づいて検索する場所を認識します。このファイルの名前はインターフェース名に一致し、ファイルの中身は実装クラスの名前になっている必要があります。ServiceLoaderは、Providerをインスタンス化するので、引数のないコンストラクタが存在しなければなりません。R最適化が行われていることを確認するには、Android StudioのAPK Analyzer内にあるDEX Viewerを使い、ベースDEXファイルからServiceLoaderクラスの参照を検索します。Rが正常に働いている場合、何も見つかりません。R最適化が有効になっていない、または動作していない場合のDEX Viewerの出力例。ファイル内にServiceLoader load  が残っており、アプリのコードから呼び出されている。ServiceLoader load  の呼び出しが表示されている場合 上のアニメーション 、最適化が動作していないため、呼び出しスレッドでディスクI Oオペレーションが発生し、アプリがフリーズする可能性があります。そのため、ServiceLoaderパターンを使う場合は、必ず最適化が有効になっていることを確認してください。アプローチ  Daggerを組み込む番目のアプローチでは、Dagger を組み込んでDFM側でオブジェクトグラフをインスタンス化する方法を紹介します。これは、daggerDebugビルドバリアントで確認できます。ここまで見てきたのは、とても単純なケースです。機能のコードにContextさえ与えれば、StorageFeatureオブジェクトを作成して返すことができました。実際の依存関係ははるかに複雑かもしれません。そのような場合、適切な依存関係をインスタンス化して nbsp DFMに渡すには、Daggerなどの依存性注入フレームワークを使うのが便利です。Gradleモジュールの依存関係のセットアップ方法との兼ね合いで、DFM内にあるベースDaggerコンポーネントのサブコンポーネントを使うことはできません。しかし、コンポーネント依存関係という別のメカニズムを使って、親コンポーネントから必要な依存関係を取得することができます。依存関係の宣言は、機能コンポーネントで行います。そして、StorageFeature Provider実装を使って機能コンポーネントをインスタンス化し、StorageFeatureを返します。他に必要なのは、必要な依存性を渡す方法だけです。この部分もDaggerに行わせたいので、ベースコンポーネントにDependenciesインターフェースを実装させます。これにより、アプリをコンパイルするたびに、Daggerが不足している依存関係があるかどうかを通知してくれるようになります。たとえば、ベースコンポーネントにLoggerオブジェクトを確実にバインドしたいので、StorageFeature Dependenciesの一部として機能コンポーネントに渡すことができます。そして最後に、ベースモジュールにカスタムの Providesメソッドを作成します。これは、リフレクションを使ってStorageFeature Providerを取得し、必要な依存関係を渡して、結果をキャッシュします。この例は、アプリでDaggerと動的機能モジュールを連携させるつの方法ですが、多くの部分は現在のDaggerのセットアップにかかっています。このサンプルの考え方を利用したり、皆さんの現在のアーキテクチャに当てはまるものをお使いください。たとえば、モジュールで結果をキャッシュするのではなく、注入側でdagger Lazy lt StorageFeature gt を使うこともできるでしょう。どのアプローチを使う場合でも、アプリにDFM APKをインストールした後、StorageFeatureオブジェクトグラフのみをインスタンス化する必要があります。DFMが条件付きまたはオンデマンドで配信される場合は、最初にSplitInstallManager installedModulesを使って確認するようにしてください。詳しく学びたい方へモジュール化は非常に幅広いトピックです。単純なクラスの読み込みだけで終わるような内容ではなく、アプリ全体のアーキテクチャやエンドユーザーが目にする最終的なUXなどにまで話が及びます。この記事では、リフレクションを使ってDFMからコードを読み込むというつの具体的な問題を解決する方法に注目しました。アプリのモジュール化について理解を深め、適切なアーキテクチャを見つけたいという方は、Google I O nbsp  でのYigit BoyarとFlorina Muntenescuのトークをご覧ください。この記事で説明した方法と同じテクニックを実際のアプリで使う際に行われたコードの変更についてもご覧ください。検索データソースを別々のモジュールに分割by florina muntenescu ·プルリクエスト  ·android plaidまた、Ben Weissが汎用的な記事を公開しています。これは、少し前にPlaidをモジュール化したケーススタディです。Plaidのパッチワークーモノリスからモジュールアプリへこの記事で紹介した動的にコードを読み込むサンプルは、Play Core Libraryを使っています。Playからオンデマンドモジュールを配信するには、リンク先のドキュメントのページにも書かれているように、APIを熟知して正しく実装する必要があります。Android、Play、Kotlinなどの知識を見逃さないように、Androidデベロッパーをサブスクライブしてください AndroidデベロッパーReviewed by Yuichi Araki   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/8028657220557818986/comments/default",
      "updated": "2019-07-08 01:26:27"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "TFXパイプラインとML Metadataでリサーチから本番環境に",
      "description": "この記事はJarek Wilkiewicz nbsp によるTensorFlow   Mediumの記事 From Research to Production with TFX Pipelines and ML Metadata を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   投稿者  Jarek Wilkiewicz  TFX nbsp チームを代表して 本番環境でコードを実行させている方なら、バージョン管理やソフトウェア構成管理 SCM 、継続的インテグレーションや継続的デプロイ CI CD など、ソフトウェアエンジニアリングに関するベストプラクティスはおなじみのものでしょう。これらは、長年にわたって発展を遂げ、今では当然のものと思われています。効率的なアルゴリズム実装を書く方法がソフトウェアエンジニアの旅の始まりにすぎないのと同じく、通常、機械学習 ML モデルのコードは、本番環境にデプロイする必要があるシステム全体のコードのわずか でしかありません¹。Googleは、残りの を改善するための作業も長年にわたって続けています²。その作業の成果であるTensorFlow Extended TFX³ は、優れたソフトウェアエンジニアリング慣習によるメリットを、急速に拡大するMLの領域に導入することを目指すものです。今後何回かのブログ投稿では、TFXの新機能に着目し、MLモデルを構築して本番環境にデプロイする際にTFXがどう役立つかを紹介したいと思います。つい先日までは、基盤となるTFX ライブラリ  TensorFlow Data Validation、TensorFlow Transform、TensorFlow Model Analysis、TensorFlow Serving だけがオープンソースとして公開されていました。つまり、デベロッパーは、このライブラリを使って独自のMLパイプラインコンポーネントを構築しなければなりませんでした。現在は、下の図に示すように、完全なTFX MLパイプラインを作成するために複数の既成コンポーネントを使えるようになり、それらを調整して高レベルのPython APIを活用した多くの一般的なMLユースケースに利用したり、Apache AirflowやKubeflowなどのお好みのオーケストレーションシステムを使って実行したりできます。TFXパイプラインが実行されると、依存するアーティファクトパイプラインコンポーネント 例 トレーニングデータ や手続き 例 ボキャブラリやモデル は、ML Metadata MLMD、別のGoogleのオープンソースプロジェクト によって追跡されます。ML Metadataはスタンドアロンライブラリとして利用でき、TFXコンポーネントの一部にもなっているので便利です。MLMDを使うと、アーティファクトの系統 トレーニングしたデータモデルなど や、あるアーティファクトから作成されたすべてのアーティファクト 特定のデータセットでトレーニングしたすべてのモデルなど を見つけることができ、さまざまなユースケースを実現できます。こういったことがどのように組み合わせられているのかを理解するために、Google I O  でのプレゼンテーション「TensorFlow Extended TFX  機械学習パイプラインとモデルの理解」をご覧ください。次回のTFXブログ投稿では、TFXパイプラインコンポーネントについてさらに詳しく説明します。それまでの間、TFXデベロッパーチュートリアルをお試しください。データセットの調査から完全に動作するMLパイプラインまで、一般的なML開発プロセスをたどることができます。TFXについて質問がある方は、Stack Overflowからご連絡ください。GitHubのバグレポートやプルリクエストも歓迎します。また、一般的な議論は、tfx tensorflow orgにお送りください。   Sculley  D   Gary Holt  Daniel Golovin  Eugene Davydov  Todd Phillips  Dietmar Ebner  Vinay Chaudhary  Michael Young  Jean François Crespo and Dan Dennison  “Hidden Technical Debt in Machine Learning Systems   NIPS nbsp       Tushar Chandra  “Sibyl  A System for Large Scale Machine Learning at Google   IEEE DSN  Dependable Systems and Networks  conference keynote  Atlanta  GA  June th nbsp      Denis Baylor  Eric Breck  Heng Tze Cheng  Noah Fiedel  Chuan Yu Foo  Zakaria Haque  Salem Haykal  Mustafa Ispir  Vihan Jain  Levent Koc  Chiu Yuen Koo  Lukasz Lew  Clemens Mewald  Akshay Naresh Modi  Neoklis Polyzotis  Sukriti Ramesh  Sudip Roy  Steven Euijong Whang  Martin Wicke  Jarek Wilkiewicz  Xin Zhang  and Martin Zinkevich    TFX  A TensorFlow Based Production Scale Machine Learning Platform  In Proceedings of the rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  KDD     ACM  New York  NY  USA     DOI   Reviewed by Khanh LeViet   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/4437206594485791549/comments/default",
      "updated": "2019-07-05 04:00:19"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "AMPの誤解を払拭する",
      "description": "この記事はCemal Buyukgokcesu nbsp によるThe AMP Blogの記事 Debunking Common AMP Myths を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   AMPは、高速なサイトを簡単に作ることができるすばらしい仕組みです。年に登場して以来、ウェブのいたるところで活用され、数千万のドメインで数十億のページを提供するまでになっています。報道やeコマースの領域では、企業が自社のウェブサイトにAMPを実装し、たくさんの成功例が生まれています。しかし、AMPプロジェクトが拡大するにつれて、さまざまな誤解や都市伝説などが流布しているようです。そこで、この機会を活用してAMPについての誤解を解いていきたいと思います。﻿誤解  AMPはGoogleのみによるプロジェクト。事実  AMPは、Googleが他の企業やウェブコミュニティのメンバーとともに主導するオープンソースプロジェクトです。このプロジェクトの開発には、AMPデベロッパーや企業、個人の貢献者が参加しています。ここ年間でAMPの貢献者は人を超えましたが、その はTwitter、Pinterest、Yahoo、Bing、eBayなど、Google以外の企業に勤めています。AMPは、新しいガバナンスモデルに移行しました。新しいモデルでは、エンドユーザーなどの自身ではコードに貢献できない方を含むコミュニティのすべての構成員と、Twitter、Microsoft、Pinterest、The NY Times、Washington Post、AliExpressをはじめとする多くの企業の代表者に明示的な発言権が与えられています。誤解  AMPはGoogle comでしか動作しない。事実  AMPページには、ウェブのあらゆる場所、すなわち、すべての配信プラットフォームや端末からアクセスできます。ユーザーは、配信プラットフォーム 例 検索エンジン 上のリンクやサイトからAMPページにアクセスできます。利用可能な状態ならばデフォルトでモバイル上にAMPページを常時表示するプラットフォーム 例  Google、Bing、LinkedIn、Twitter、Yahoo JP、Baidu。プラットフォームのリストはこちら が複数あります。一部のプラットフォーム 例  google com、bing com では、追加の処理を行ってコンテンツをキャッシュすれば、ユーザーエクスペリエンスがさらに高速になります。誤解  AMPはモバイルでしか動作しない。事実  AMPは、あらゆる画面サイズで動作するように、レスポンシブを意識して設計されています。 いまやAMPはAMPであり、Accelerated Mobile Pages 高速モバイルページ を指すものではなくなっています。AMPはモバイルだけのものではありません。PCやタブレットなど、さまざまな種類の端末で動作するだけでなく、とても便利なレスポンシブデザイン機能を備えています。AMPはモバイルや遅いハードウェア、遅いネットワークに優しく設計されているので、PCで使う場合よりもスマートフォンで使う場合の方がより速さを実感できるはずです。なお、サードパーティプラットフォーム向けの機能には、モバイルエクスペリエンス専用として設計されているものもある点は理解しておきましょう 例  Googleのトップストーリーカルーセル 。詳細については、こちらの記事をご覧ください。誤解 すべてのAMPページには、非AMP版のページが存在する。事実  AMPページは、非AMP版と関連付けることもできますが、必須ではありません。  場合によっては、同じページに対して非AMP版とAMP版の両方を持ちたいこともあるでしょう。AMP移行の初期段階で、テスト版のAMPと非AMP版の両方をサポートしたい場合は特にそうです。しかし、これは必須ではありません。AMPがビジネスにとって適切なソリューションだと思うなら、同じコンテンツのつのバージョンを管理する必要はありません。つのページだけを選択でき、選ばれるのがAMPページであっても問題ありません。つまり、各ドキュメントでつのバージョンを構築、管理、監視することで、 非AMPページとAMPページの両方を構築する場合に比べて メンテナンスコストを下げられます。詳細はこちらをご覧ください。誤解  AMPのランディングページは構築が大変。事実 ほとんどのタイプのページやユースケースにおいて、AMPのランディングページは週間以内で構築できます。 私たちが接触した開発チームの は、AMPのランディングページを週間以内で構築したと言っています。とはいえ、AMPの開発にかかる労力はページのタイプによって異なります。日以内でできるAMPページもあれば、もっと時間がかかるものもあります。さらに開発時間を短縮したい方は、amp devで無料のAMPテンプレートを確認してみてください。誤解  AMPは、パブリッシャーや静的なウェブサイト専用である。事実  Google検索のクリックのうち、 以上がニュース以外のAMPページによるものです。AMPは、多くの開発者、パブリッシャーやウェブサイト、配信プラットフォームやテック企業の協力によって誕生しました。最初にAMPがリリースされたときは、主にパブリッシャーに採用されましたが、現在は広告主やeコマース企業もAMPを活用して高速化によるメリットを得ています。こちらのサイトで成功事例をご覧いただけます。誤解  AMPはインタラクティブな操作をサポートしていない。事実  AMPコンポーネントを使えば、デザインのカスタマイズやインタラクティブな操作を実現できます。AMPが最初にリリースされたとき、見た目のデザインに制限がありました。しかし、オープンソースコミュニティとの協力を通してAMPプロジェクトが拡大する中で、デザインのカスタマイズやインタラクティブな操作を実現する新しいコンポーネントが開発されています。現在では、ほとんどのインタラクティブな操作がサポートされています。リッチメディア  AMP コンポーネントは増加を続けており、足りないと感じればどなたでも貢献できます。サードパーティ P 統合 既に多くが利用できるようになっており、その数は現在も増加中です。BMW、AliExpress、La nbsp Repubblica nbsp の nbsp Rep nbsp などの企業は、AMPですばらしいインタラクティブ性を実現しています。中には、サイトのほとんどのページでAMPを使っている企業もあります。誤解  AMPはeコマースのウェブサイトをサポートできない。事実  AMPはeコマースにうってつけです。AMPはウェブページを高速化し、高速化したページによって購入へのコンバージョンが促されるからです。AMPがリリースされたとき、最初にそれを採用したのはパブリッシャーでした。その後、AMPプロジェクトが拡大する中で、インタラクティブな操作を実現する新しいコンポーネントが構築されています。インターネットでAMPプラットフォームの採用が拡大するスピードには、私たちも驚かされます。AMPを利用すると、高速で美しいeコマース体験を構築できます。詳細については、ブログ投稿「Getting started with AMP for e commerce eコマース向けのAMPを始めよう 」や「eコマースにAMPのスピード感を取り入れよう」をご覧ください。誤解  AMPページは最新コンテンツを提供できない。事実  AMPページのコンテンツを最新に保つ方法は数多くあります。AMPで最新のコンテンツを提供する場合は、デフォルトのAMP Cacheメカニズム stale while revalidate を使うことも、キャッシュ更新機能を使うことも、動的コンポーネント amp listなど を使うこともできます。実装の計画をうまく立てれば大きな成功につながることは、多くの大規模eコマース企業が実証しています。誤解  AMPはセキュリティが不十分。プライバシーが保護されない。事実  AMPフレームワークは、プライバシーとデータのセキュリティを確実に保護できるように構築されています。通常、AMPのランディングページはGoogle AMP Cacheから提供されます。Google AMP Cacheは、AMPドキュメントを検証して高速かつ確実に提供するために、AMP版のランディングページをキャッシュしています。Google AMP CacheとAMP JavaScriptは、ユーザーをトラッキングしないCookieのないドメインから提供されます。また、AMPにはセキュリティレビュープロセスがあり、新しいAMPコンポーネントがリリースされる際には必ずレビューが行われています。詳細については、ブログ投稿「Privacy and user choice in AMP s software architecture AMPソフトウェアアーキテクチャにおけるプライバシーとユーザーの選択 」をご覧ください。誤解  AMPページでは、非AMPページほどコンバージョンが起こらない。事実 正しく最適化されたAMPページは、多くの場合、非AMP版よりも高い成果をあげることができます。amp devにもあるように、多くの広告主やパブリッシャーがAMPで成功を収めています。Forresterの調査によると、サイトでAMPを実装すれば、AMPページの販売コンバージョン率が 、AMPサイトのトラフィックが前年比 、訪問者当たりのページ数が 増加しました。いくつかの理由によって、AMPページが非AMPページよりも成果が低く見えることがあります。成果が上がらないと感じたときは、以下を確認してみてください。測定とトラッキングの問題 次のつのセットアップガイドに従ってAMPページにアナリティクスを設定していることを確認します。AMPページにアナリティクスを追加するAMPページと非AMPページをまたいでセッションを統合する一貫性のないランディングページ  AMPページと非AMPページの外見が異なる場合、コンバージョン率に影響する可能性があります。AMPのパフォーマンスを正確に評価するには、どちらのランディングページも同じ外見と機能である必要があります。また、ユーザーフレンドリーなランディングページを提供することも心がけてください。AMPのテストや評価を行う際に留意すべき点については、こちらのブログ投稿をご覧ください。投稿者  Googleモバイルウェブ部門グローバルプロダクトリーダー、Cemal Buyukgokcesu Reviewed by Chiko Shimizu   Developer Relations Team    ",
      "link": "http://developers-jp.googleblog.com/feeds/7205471370167002262/comments/default",
      "updated": "2019-07-05 01:15:11"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "TensorFlowモデル最適化ツールキットーPruning API",
      "description": "この記事はTensorFlow   Mediumの記事 TensorFlow Model Optimization Toolkit ー Pruning API を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   TensorFlowモデル最適化ツールキットーPruning APIモデル最適化ツールキットは、機械学習モデルを最適化するもので、初心者でも経験を積んだデベロッパーでも使うことができます。このツールキットをリリースしてから、いくつかの新しいアプローチやツールを追加するというロードマップを実現するための作業を続けてきました。本日は、重みのプルーニング 剪定 を行う新しいAPIについて紹介します。重みのプルーニング機械学習プログラムの最適化には、さまざまな形態があります。幸いにも、ニューラルネットワークでは、このような目的を達成する各種の変換を柔軟に行うことができます。そのような最適化のつに、計算に必要なパラメータや演算の数を減らすものがあります。これは、ニューラルネットワークのレイヤー間のつながり パラメータ を削除することによって実現します。重みのプルーニングを行うAPIは、Kerasをベースに構築されています。そのため、このテクニックはどんな既存のKerasトレーニングプログラムにも、とても簡単に適用できます。このAPIは、今後リリースされる他の最適化テクニックと合わせて、モデル最適化ツールキット用の新しいGitHubリポジトリの一部となる予定です。重みのプルーニングとは重みのプルーニングとは、重みテンソルの不要な値を削除することを表します。実際には、ニューラルネットワークのパラメータの値をゼロにすることで、ニューラルネットワークのレイヤー間の不要なつながりと見なしたものを削除します。ニューラルネットワークが変化に対応できるように、この処理はトレーニングプロセスで行います。重みのプルーニングが役立つ理由重みのプルーニングによってすぐに得られるメリットは、容量の圧縮です。疎なテンソルは圧縮効果が高いので、プルーニングを行ったTensorFlowチェックポイントか変換済みTensorFlow Liteモデルに単純なファイル圧縮を適用すれば、モデルの格納サイズや転送サイズを減らすことができます。たとえば、このチュートリアルでは、MNIST用の 疎なモデルを MBから nbsp  MBに圧縮する方法を示しています。さらに、さまざまな実験から、重みのプルーニングは量子化とも互換性があり、両方のメリットが得られることもわかっています。先ほどのチュートリアルでは、トレーニング後に量子化を行うことで、プルーニングしたモデルを MBからわずか MBに圧縮する方法も示しています。今後、TensorFlow Liteに疎表現と疎計算のファーストクラスサポートを追加して圧縮のメリットを実行時メモリにも拡大し、パフォーマンスの改善を実現したいと考えています。疎なテンソルでは、ゼロになった値の計算は不要で省略できるため、パフォーマンスが向上します。各種モデルにおける結果このテクニックが異なるタイプのモデルにうまく適用できるかを検証するため、イメージ処理用の畳み込みベースのニューラルネットワークから再帰型ニューラルネットワークを使う音声処理まで、さまざまなタスクで実験を行いました。次の表は、いくつかの実験結果を抜粋したものです。異なるモデルやタスクでの値をゼロにする割合 sparsity とその結果。動作の仕組みKerasベースの重みプルーニングAPIには、簡単ですが幅広く適用できるアルゴリズムを使っています。このアルゴリズムは、トレーニング中に値の大きさに基づいて繰り返しつながりを削除するように作られています。まず、最終的に値をゼロにする割合 例    と、プルーニングを行うスケジュール 例  ステップ目からステップ目までの間、ステップごとにプルーニングを行う を指定します。オプションで、プルーニング構造 例 個々の値に適用するか、ある形状の値のブロックに適用するか を設定することもできます。プルーニングを行わないテンソル 左 、xのブロックでプルーニングを行ったテンソル 中央 、xのブロックでプルーニングを行ったテンソル 右 の例。トレーニングが進むと、プルーニングルーチンがスケジューリングされて実行され、値をゼロにする割合が達成されるまで、最も小さい値 ゼロに近い値 の重みを削除 ゼロを設定 します。プルーニングルーチンがスケジューリングされて実行されるたびに、現在値がゼロになっている割合が再計算されます。これは、 から始まり、プルーニングスケジュールの終わりに最終的な目標が達成されるまで、スムーズなランプアップ関数に従って徐々に増加します。プルーニングに使うランプアップ関数の例。ステップからステップまでプルーニングを行うようにスケジューリングし、最終的に値をゼロにする割合を nbsp  とした場合。スケジュールと同じく、ランプアップ関数も必要に応じて調整できます。たとえば、ある収束レベルが達成されたステップでトレーニング手続きを開始したり、すべてのトレーニングステップが行われる前にプルーニングを終了するようにトレーニングプログラムをスケジューリングし、最終的に値をゼロにする割合を実現できた段階でシステムを微調整できると便利な場合もあるでしょう。こういった設定の詳細については、チュートリアルやドキュメントをご覧ください。トレーニング手続きの最後では、「プルーニングされた」Kerasレイヤーに対応するテンソルには、そのレイヤーで最終的に値をゼロにする割合に応じてゼロが含まれることになります。テンソルにプルーニングを適用する様子を表したアニメーション。黒いセルは、ゼロでない重みが存在する場所を示す。トレーニングプロセスが進むにしたがって、値がゼロになる割合が増加する。新しいドキュメントとGitHubリポジトリ先ほどもお知らせしたように、重みプルーニングAPIは、機械学習モデルの実行効率や表現効率を上げることを目的としたテクニックを集めた新しいGitHubプロジェクトおよびリポジトリの一部になる予定です。機械学習のこの領域に興味がある方や、モデルを最適化するリソースを必要としている方は、ぜひこのプロジェクトにスターを付けてください。この領域の重要性を踏まえ、tensorflow org model optimizationに新しいサブサイトを作成して関連するドキュメントやリソースを公開しています。ぜひすぐに試してみてください。フィードバックも大歓迎です。また、今後の発表を見逃さないように、このブログもフォローしてください 謝辞  Raziel Alvarez、Pulkit Bhuwalka、Lawrence Chan、Alan Chiao、Tim Davis、Suyog Gupta、Jian Li、Yunlu Li、Sarah Sirajuddin、Daniel Situnayake、Suharsh SivakumarReviewed by Khanh LeViet   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/933600749897530627/comments/default",
      "updated": "2019-07-03 04:32:21"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "EfficientNet モデルスケーリングとAutoMLで最高精度を達成したGoogleの画像認識技術",
      "description": "この記事はMingxing TanとQuoc V  Le nbsp によるGoogle AI Blogの記事 EfficientNet  Improving Accuracy and Efficiency through AutoML and Model Scaling を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   投稿者  Google AIスタッフソフトウェアエンジニア、Mingxing Tan、Google AI主席サイエンティスト、Quoc V  Le畳み込みニューラルネットワーク CNN の開発では、まずは一定のリソースコストを投入し、その後さらに多くのリソースが利用可能になった段階でスケールアップし精度を上げていく方法が一般的です。例えば nbsp ResNet nbsp では、レイヤー数を増やすことでResNet からResNet までスケールアップできます。また最近発表された nbsp GPipe nbsp では、ベースラインCNNを倍スケールアップすることで nbsp ImageNet nbsp top 精度 を達成しています。こうしたモデルのスケールアップでは、CNNの深さや幅を適当に増やしたり、学習や評価時の画像解像度を大きくしたり、といったやり方が行われます。確かにこうした方法でも精度は向上しますが、手作業による面倒なチューニングが必要なうえ、その性能がベストであるとは言いにくいです。もし、CNNの精度と効率をより改善するための「スケールアップの原則」があればどうでしょうか。私たちがICMLで発表した論文「EfficientNet  Rethinking Model Scaling for Convolutional Neural Networks EfficientNet 畳み込みニューラルネットワークモデルのスケーリングを再考する 」では、モデルの新たなスケーリング手法を提案しています。具体的には、シンプルかつとても効率的な複合係数 compound coefficient を使い、より統制された手法でCNNをスケールアップします。従来のアプローチでは、幅、深さ、解像度などのネットワークの次元を任意の大きさにスケールアップしていました。一方、私たちの手法では一定のスケーリング係数を使ってそれぞれの次元を均一にスケールアップしていきます。この新たなスケーリング手法と、最近のAutoMLの進展を活用し、EfficientNetと呼ばれるモデル群を開発しました。このモデルは現在の最高水準の精度を上回り、効率も最大で倍向上しています 小型化と高速化 。複合モデルスケーリング CNNをスケールアップする優れた手法今回の研究では、ネットワークのスケーリングによる効果を理解するため、モデルの各次元を変化させた場合の影響を系統的に調査しました。その結果、個々の次元をスケーリングするとモデル性能は向上しますが、モデル全体として改善の効果が最も大きくなるのは、利用可能なリソースをネットワークのすべての次元 幅、深さ、イメージの解像度 に均等に割り当てた場合であることがわかりました。複合スケーリング手法ではまず、ある一定のリソース制約 例  FLOPS nbsp を倍に設定する のもとでグリッドサーチを行い、ベースラインとなるネットワークの個々の次元間の関係を見つけます。これにより、各次元に対する適切なスケーリング係数が決まります。その後この係数を適用し、希望のモデルサイズまたは計算リソースに合わせてベースラインネットワークをスケールアップします。さまざまなスケーリング手法の比較。ネットワークの個々の次元を任意にスケーリングする従来型のスケーリング手法 b  d とは異なり、複合スケーリング手法ではすべての次元を均一的かつ一定の原則に基づいてスケールアップするこの複合スケーリング手法では、従来のスケーリング手法に比べて、モデルの精度と効率がコンスタントに改善していきます。たとえば nbsp MobileNet nbsp ではImageNet精度が 向上、 nbsp ResNet nbsp では 向上しました。EfficientNetのアーキテクチャモデルのスケーリングの効率性は、ベースラインネットワークの設計にも大きく依存します。そこで、性能をさらに改善すべく新しいベースラインネットワークも開発しました。精度と効率 FLOPS の両方を最適化するAutoML MNASフレームワークによりニューラルアーキテクチャ検索を行うことで実現しています。その結果として得られたアーキテクチャは、MobileNetVやMnasNetと同じようにMobile Inverted Bottleneck Convolution MBConv を使っていますが、利用する計算リソースの増加にともないわずかに大きなものになっています。その後、このベースラインネットワークをスケールアップすると、EfficientNetと呼ばれるモデル群が得られます。ベースラインネットワークであるEfficientNet B用のアーキテクチャは、シンプルでクリーンな設計です。スケーリングや汎用化も容易です。EfficientNetのパフォーマンスImageNet nbsp を対象に、EfficientNetをいくつかの既存CNNと比較しました。EfficientNetモデルは、既存のCNNよりも高い精度と優れた効率の両方を実現しており、パラメータのサイズと計算量が桁少なくなっています。たとえば高精度版のEfficientNet Bは、ImageNetのtop で 、top で という最高レベルの精度を実現しています。また、CPUによる推論で比較すると、それまでのGpipeと比べてサイズが分の、速度は倍。また、一般的な nbsp ResNet とEfficientNet Bを比べると、計算量はほぼ同じですがtop 精度はResNet の から   向上 に増加しています。モデルのサイズと精度の比較。EfficientNet Bは、AutoML MNASが作成したベースラインネットワーク。一方のEfficient BからBは、ベースラインネットワークをスケールアップしたもの。特に、EfficientNet Bは、トップで 、トップで という最高レベルの精度を実現。同時に、既存の最高のCNNと比べてサイズが分のになっている。EfficientNetはImageNetで高い性能を発揮しますが、使いやすさを考えると、他のデータセットで転移学習した場合の性能も重要です。この点を評価するため、広く使われているつの転移学習データセットでEfficientNetをテストしました。EfficientNetは、CIFAR    、Flowers   など、つのうちつのデータセットで最高レベルの精度を実現しました。中にはパラメータ数がひと桁少なくなくなった例もありました 最大で分の 。これにより、EfficientNetは転移学習でも性能を発揮できることがわかります。今回の研究によりモデルの効率を大幅に改善できたことから、今後のコンピュータビジョン関連のタスクにおいて、EfficientNetが新しい基盤として活用される可能性も考えられます。そこで、すべてのEfficientNetモデルをオープンソース化しました。これによって機械学習コミュニティが広く恩恵を受けられることを願っています。EfficientNetのソースコードとTPU nbsp 用の学習コードは、こちらから確認できます。謝辞  Hongkun Yu、Ruoming Pang、Vijay Vasudevan、Alok Aggarwal、Barret Zoph、Xianzhi Du、Xiaodan Song、Samy Bengio、Jeff Dean、とGoogle Brainチームに格別の感謝を捧げます。Reviewed by Kaz Sato   Staff Developer Advocate  Google Cloud   ",
      "link": "http://developers-jp.googleblog.com/feeds/2205815582426389402/comments/default",
      "updated": "2019-07-02 04:32:07"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Android Enterprise Recommendedの対象をマネージドサービスプロバイダ MSP に拡大",
      "description": "この記事は nbsp Vincent Ricci  Global Head of Android Enterprise Business Development  Service Providers  nbsp によるGoogle Blogの記事 Android Enterprise Recommended expands to include Managed Service Providers を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。企業向けモバイルプラットフォームとしてAndroidを採用する企業が増えています。Googleとしては、十分な専門性とサポート体制を備えたパートナーを、お客様が簡単に見つけることができるよう取り組みを進めています。その一環として、Android Enterprise Recommendedプログラムの対象を、マネージドサービスプロバイダ MSP やMSP事業を手がける企業まで拡げることにしました。MSPとは、企業が世界各地で使用する大量のデバイスを、企業に代わってプロビジョニング、管理、サポートする企業のことです。MSPのAndroid Enterprise Recommendedプログラムでは、認定MSPパートナーが顧客企業のモバイル展開を効果的に支援できるよう、Googleの最新のベストプラクティスに基づくトレーニングも実施します。Android Enterprise Recommended MSPパートナーとしてはこれまでに、Accenture、Brodos、Cognizant、DXC Technology、Econocom、Honeywell Enterprise、Mobile Mentor、Mobility MEA、Offshore Tech、SCC、SHI、Skywire、Stratix、Tech Data、Vox Mobileが認定されています。今後もMSPパートナーを増やし、プログラムを拡充していく予定です。Android Enterprise Recommended MSPは以下の要件を満たす必要があります 詳細な要件はこちらで確認できます 。Androidの実装とサポートに関わるGoogleのトレーニングを受け、技術的な専門性とリーダーシップを兼ね備えた専任の担当者を配置していることAndroid Enterprise Recommended企業向けモバイル管理 EMM システムについて十分な経験を積んでいると認められることGoogleと密接な協力関係にあること 担当のアカウントマネージャーが割り当てられている、Googleパートナーエスカレーションデスクを時間日利用できるなど Androidプロダクトの機能について常に最新の情報を入手し、年ごとのプログラム再認定に必要なトレーニングを受けていること年前にAndroid Enterprise Recommendedプログラムを開始したときに目指したのは、企業向けの厳しい要件を満たすことがGoogleによって検証されているAndroidデバイスやサービスを、企業の皆様が簡単に見つけられる仕組みを作ることでした。ナレッジワーカー向けデバイスから始まったAndroid Enterprise Recommendedプログラムですが、その後頑丈なデバイスが追加され、先月はEMMプロバイダにまで対象を拡大しました。こうしたつひとつの積み重ねにより、お客様がさらに安心してAndroidを導入でき、一貫性をもってスムーズに展開できる体制を整えてまいります。Android Enterprise Recommendedを開始して以来、プログラムをご利用のお客様から多くの反響をいただいております。IDCが実施した調査によると、Android Enterprise Recommendedを利用する欧州のお客様の が、展開しているAndroidデバイスは他のデバイスに比べ安全で、企業利用にふさわしいと回答しています。Googleでは、今後もプログラムを拡大しつつそのレベルを着実に引き上げ、皆様にAndroidへの投資に大きな価値を見い出していただけるよう努めてまいります。Reviewed by Ben Seab   Regional Manager  Android Enterprise",
      "link": "http://developers-jp.googleblog.com/feeds/8191390494885189330/comments/default",
      "updated": "2019-07-02 03:19:52"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Maker Faire TokyoのGoogleスペースに展示しませんか ",
      "description": "年月日 土 、月日 日 に東京ビックサイトで開催されるMaker Faire Tokyo にGoogleスペースを出展します。このスペースでは、デベロッパーが開発したGoogleのテクノロジーを用いた作品を展示する予定です。自らが開発された作品をGoogleスペースで展示したいとお考えの方はぜひご応募ください。応募条件Maker Faire Tokyo開催期間中、終日参加できる方。展示に関する費用、交通費などはご自身で負担いただきます。展示品の内容により、終日の展示ではなく、幾つかの作品を時間ごとに展示する方式になる場合があります。応募作品の条件Googleのテクノロジーを用いていること。 mm x mmのテーブルの上で展示可能なもの。手荷物として持参できるもの。ネットワークなしで展示可能なもの。スペースとしてネットワークは用意しません。WiFiルータなどの持参は可能ですが、混雑する会場内ではネットワークが不安定になる場合があります。危険物や液体を用いていないこと。詳細は以下のページの から をご参照ください nbsp 出展の要項応募方法年月日 金  までに以下のご登録フォームに必要事項をご記入ください。ご登録内容を考慮のうえ、 作品を選考します。選考結果は月日 月 までにご連絡します。Maker Faire Tokyo Googleスペース展示登録皆さまとご一緒にMaker Faire Tokyoで素晴らしい作品を展示することを楽しみにしてます。ぜひご応募ください。Posted by Takuo Suzuki   Developer Relations Team",
      "link": "http://developers-jp.googleblog.com/feeds/5055055011904842105/comments/default",
      "updated": "2019-07-01 06:35:08"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Androidセキュリティの進化に関するGartnerの分析",
      "description": "この記事は nbsp Dave Kleidermacher  Vice President of Security for Android  Play  Chrome OS  nbsp によるGoogle Blogの記事 Gartner s analysis on the progress of Android security を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。モバイルセキュリティが常に話題になる昨今、さまざまなプラットフォームの相対的な強みや、所定のセキュリティ要件にどのように対応しているのかを評価することは不可欠です。Gartnerの「モバイルOSとデバイスのセキュリティ プラットフォームの比較」は、企業のIT部門やユーザー向けにGartnerが推奨するセキュリティ基準に基づいて、最近のモバイルOSやデバイスプラットフォームが企業データの効果的な保護という高まるニーズにどのように対応しているのかを評価した、包括的なレポートです。GartnerのアナリストであるPatrick Hevesi氏は、このレポートでAndroid、iOS、Windows のセキュリティ機能を評価しています。また、Samsung Knoxデバイス、Microsoft Surface Pro、Google Pixelファミリーといったハードウェアへの実装についても分析しています。セキュリティ機能の評価は個のカテゴリ別に分けられ、コアOSのセキュリティ機能やデバイスのセキュリティ制御といったトピックも含まれています。下の動画では、レポートの内容に加え、長年にわたるAndroidセキュリティの進化、何十億人ものユーザーを守るためのAndroidの取り組みについて紹介しています。 この動画をご覧いただければ、Androidセキュリティの特徴や、セキュリティを重視する企業にとってプラットフォームに関する透明性やオープン性がなぜ重要なのかがわかります。こうした原則は、Androidがモバイルセキュリティにとって第一の選択肢になることを裏付けています。Reviewed by Ben Seab   Regional Manager  Android Enterprise",
      "link": "http://developers-jp.googleblog.com/feeds/6626103980715561589/comments/default",
      "updated": "2019-07-01 04:49:40"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Android Enterprise Recommendedで最適な企業向けデバイスを見つける",
      "description": "この記事は nbsp Bernie Hsu  Head of Android Enterprise Device Partnerships によるGoogle Blogの記事 How companies are finding the right device with Android Enterprise Recommended を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。先日のモバイルワールドコングレスでは、ハードウェアパートナー各社が出展する最新デバイスを目の当たりにし、Androidが形成するモビリティの未来を展望することができました。Android Enterprise Recommendedプログラムの開始にあたって目指したのは、用途の広い強力なソリューションによってデバイスを検証し、企業の皆様に安心してご利用いただけるモバイルエコシステムを提供することでした。ナレッジワーカー 一般社員 向けデバイスから始まった検証も年が経過し、その対象範囲は頑丈なデバイス、企業向けモバイル管理 EMM システム、最近ではマネージドサービスプロバイダ MSP にまで拡がっています。ここでは、Android Enterprise Recommendedを活用することにより、社員が安心して使用できるデバイスの選択肢を拡げ、コストの削減と組織の活性化を実現している事例をいくつかご紹介します。デバイスの選択肢を拡げる企業アプリケーションソフトウェア大手SAPは、社員が使用できるデバイスとして幅広い選択肢を用意するとともに、Android Enterpriseが提供する最新の管理機能を活用したいと考えました。同社が所有する社員用デバイスは台にも上り、デバイス上のビジネスデータと個人データを分けて管理するための解決策として選んだのがAndroid Enterpriseの仕事用プロファイルでした。SAPグローバルサービスオーナーとしてモバイル事業を担当するJarmo Akkanen氏によると、Android Enterprise Recommendedに移行したことで厳しいセキュリティ要件を満たすデバイスを安心して選択できるようになり、デバイスの導入にかかる時間も大幅に短縮できるようになったといいます。同氏は次のように語ってくれました。「どこにいても仕事ができるという選択肢を、できるだけ早く社員に提示したかったのです。GoogleのAndroid Enterprise Recommendedプログラムに出会ったことで、会社で所有、管理するスマートフォンの選択肢が大きく拡がりました。」低コストでスピーディーな導入SAPのITチームでも、このデバイス戦略の一環として、さまざまな価格の高品質デバイスの総所有コストを削減できないか検討することにしました。Android Enterprise Recommendedデバイスとして、ハイエンドモデルからお手頃なモデルまで幅広い選択肢を提供していたのがNokiaです。SAPはNokiaを採用することで、企業向けの厳格な基準を満たすデバイスを、幅広い価格帯から柔軟に選択できるようになりました。コストを削減するうえでもうつ重要なのが導入にかかる時間です。SAPでは、すべてのAndroid Enterprise Recommendedデバイスをゼロタッチ登録に移行することで、社員用のデバイスを事前の手動セットアップなしでまとめて導入できるようにしました。社員は、必要なアプリが自動的にインストールされた状態で、管理設定も完了した状態のデバイスを受け取ることになります。最適なデバイスを見つける英国に本拠を置く金融機関Yorkshire Building Society YBS では、管理されていないデバイスや、オペレーティングシステム OS や管理ソリューションが古いままアップデートされていないデバイスが大量にあり、ITチームとして管理の枠組みを刷新する必要性を感じていました。iOSベースとAndroidベースの両方の手法を検討した結果、モバイル端末ITインフラストラクチャーをAndroid Enterprise Recommendedデバイスに移行することを決断しました。社員用の新しい標準スマートフォンとしてNokia  Plusを採用し、社内アプリストアを開発してmanaged Google Playを通じてアプリを配信することにしました。YBSのエンドユーザーコンピューティングデリバリーマネージャーのAndrew Ellison氏によると、新しいスマートフォンへの移行は社員からも評判が良く、一貫性のあるソフトウェアと高性能なハードウェアの組み合わせにより、ストレスのない快適なモバイル環境を実現できているといいます。「Androidに移行したおかげで、社員に快適なユーザーエクスペリエンスと、より充実したサービスやソフトウェアを提供できるようになりました」と述べるEllison氏。「Android Enterpriseなら企業所有のデバイスにおいても、セキュリティや管理のしやすさを犠牲にすることなく、社員一人ひとりに最適なユーザーエクスペリエンスを提供できます。」ここで紹介した企業は、Androidを採用した事例のほんの一部です。Googleでは、さらに多くの企業やパートナーの皆様にAndroidの潜在能力をご活用いただけるよう取り組んでまいります。Reviewed by nbsp Ben Seab   Regional Manager  Android Enterprise",
      "link": "http://developers-jp.googleblog.com/feeds/3245696499989378171/comments/default",
      "updated": "2019-07-01 04:41:33"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Androidでコルーチン パートI  背景を理解する",
      "description": "この記事は nbsp Sean McQuillan  Android Developer Advocate  nbsp によるMedium Blogの記事 Coroutines on Android  part I   Getting the background を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。この記事は、Androidでのコルーチンの使用に関する連載の一部です。この記事では、コルーチンの仕組みと、コルーチンにより解決される問題について焦点を当てます。このシリーズの他の記事 Androidのコルーチン パートII  使ってみるAndroidのコルーチン パートIII  実際の処理コルーチンにより解決される問題Kotlinのコルーチンは、Android上で使用して非同期のコードを簡素化できる、新たなスタイルの同時実行処理を導入します。コルーチンがKotlinに導入されたのはバージョンですが、その概念はプログラミング言語の黎明期までさかのぼります。コルーチンの使用の可能性を探った最初の言語は年のSimulaです。ここ数年、コルーチンの注目度は高まっており、現在ではJavascript、C 、Python、Ruby、Goといった数多くの主要なプログラミング言語に組み込まれています。Kotlinのコルーチンは、大規模なアプリの構築に用いられてきた定評ある概念に基づいています。Androidにおいて、コルーチンは次のつの問題への優れた解決策となります。長時間実行タスク 実行に長時間を要し、メインスレッドをブロックするタスクです。メインセーフティ メインスレッドから問題なく任意の中断関数を呼び出せるようにすることができます。上記の項目を詳しく確認し、よりわかりやすい形でコードを作成するのにコルーチンがどう役立つかを見てみましょう。長時間実行タスクウェブページの取得やAPIとのやり取りにはいずれも、ネットワークリクエストが含まれます。同様に、データベースからの読み取りやディスクからの画像の読み込みには、ファイルの読み取りが含まれます。こうした種類の処理を長時間実行タスクと呼びます。極めて長い時間がかかり、処理が終わるまでアプリに動作を停止して待たせるタスクです。最新のスマートフォンにおけるコードの実行速度がネットワークリクエストと比較してどれだけ速いかは、理解しづらいかもしれません。Pixel で CPUサイクルに要する時間はわずか秒以下であり、これは人間にとって極めてわかりにくい値です。しかし、ネットワークリクエストをまばたき回、約ミリ秒 秒 と考えれば、CPUの動作速度がどれだけ高速であるかを簡単に理解できます。まばたき回分、あるいはそれよりも低速なネットワークリクエストの間に、CPUは億サイクル超を実行できます。Androidではすべてのアプリにメインスレッドがつ存在します。メインスレッドはUIの処理 ビューの描画など やユーザー操作の調整の役割を担っています。このスレッド上に過度に多くの処理が生じると、アプリはハングまたは処理速度が低下しているように見え、ユーザーエクスペリエンスを損ねます。すべての長時間実行タスクは、メインスレッドをブロックすることなく実行する必要があり、ブロックによってアプリで「ジャンク」 フリーズしたアニメーションなど が表示されたり、タップイベントへの応答が遅延したりしないようにする必要があります。ネットワークリクエストをメインスレッド外で実行するための、一般的なパターンはコールバックです。コールバックはライブラリにハンドルを提供し、ライブラリは将来のある時点でそのハンドルを使ってコードを呼び出して処理を戻すことができます。コールバックを使用したdeveloper android comの取得は以下のようになります。class ViewModel  ViewModel     fun fetchDocs     get  developer android com     result   gt  show result      getはメインスレッドから呼び出されていても、別のスレッドを使ってネットワークリクエストを実行します。その後、ネットワークからの結果が利用可能になると、メインスレッド上でコールバックが呼び出されます。これは長時間実行タスクを処理するための優れた方法であり、Retrofitなどのライブラリではメインスレッドをブロックせずにネットワークリクエストを行うことができます。長時間実行タスクにコルーチンを使用するコルーチンは、fetchDocsなどの長時間実行タスクの管理に使うコードを簡素化する方法のつです。長時間実行タスクのコードがコルーチンでどう簡素化されるかを確認するために、前出のコールバックの例をコルーチンを使って書き直してみます。   Dispatchers Mainsuspend fun fetchDocs     nbsp   nbsp    Dispatchers IO nbsp   nbsp val result   get  developer android com   nbsp   nbsp    Dispatchers Main nbsp   nbsp show result    次のセクションで以下を参照suspend fun get url  String    withContext Dispatchers IO          このコードはメインスレッドをブロックしないのでしょうか。ネットワークリクエストを待機せず、ブロックもせずに、どのようにしてgetからの結果を返すのでしょうか。実は、コルーチンが、Kotlinにこのコードを実行しつつ決してメインスレッドをブロックしない方法を提供しているのです。コルーチンは、標準の関数につの新たなオペレーションを追加することによって構築されています。呼び出しとリターンに加えて、コルーチンでは中断と再開が追加されます。中断 現在のコルーチンの実行を一時停止し、すべてのローカル変数を保存する再開 中断されたコルーチンを、一時停止した場所から続行するこの機能は、Kotlinによって、関数のsuspendキーワードで追加されています。中断関数は、他の中断関数から、またはlaunchなどのコルーチンビルダーを使って新しいコルーチンを開始することによってのみ呼び出せます。中断と再開を連動させることで、コールバックを置き換えます。前出の例では、getがネットワークリクエストを開始する前にコルーチンを中断します。関数getは引き続き、メインスレッド外でネットワークリクエストの実行を担います。その後、ネットワークリクエストが完了したら、コールバックを呼び出してメインスレッドに通知するのではなく、自身が中断したコルーチンを単に再開できます。Kotlinがどのように中断と再開を実装してコールバックを置き換えるかを表すアニメーション。fetchDocsがどのように実行されるかを見ることで、中断の仕組みを確認できます。コルーチンが中断されると必ず、現在のスタックフレーム 実行中の関数とその変数を管理するためにKotlinが使用する場所 がコピーされ、後で使用するために保存されます。コルーチンが再開すると、スタックフレームは保存された場所からコピーされて、実行が再開されます。アニメーションの中ほどで、メインスレッド上のすべてのコルーチンが中断されると、メインスレッドは画面の更新やユーザーイベントの処理を自由に行えるようになります。中断と再開を連動させることで、コールバックを置き換えます。とても便利な機能です。メインスレッド上のすべてのコルーチンが中断されると、メインスレッドは他の処理を自由に行えるようになります。デベロッパーが、メインスレッドをブロックするネットワークリクエストそのものに見えるシンプルなシーケンシャルコードを書いたとしても、コルーチンはデベロッパーの意図したとおりにコードを実行し、かつ、メインスレッドのブロックを回避します。次に、コルーチンを使ってメインセーフティを実現する方法と、ディスパッチャを見てみましょう。コルーチンによるメインセーフティKotlinのコルーチンにおいて、的確に記述された中断関数は常にメインスレッドから安全に呼び出せます。どのような実行内容であっても、そうした関数は常に、すべてのスレッドに対して自身への呼び出しを可能にする必要があります。ただし、デベロッパーがAndroidアプリで行う処理のなかには、低速すぎるためにメインスレッド上で実行できないものが多数あります。ネットワークリクエスト、JSONの解析、データベースへの読み取りや書き込み、大きなリストの単なる繰り返し処理など、こうした処理はすべて、実行速度が遅くなりユーザーに「ジャンク」が表示される原因となる可能性があるため、メインスレッドの外で実行する必要があります。suspendの使用は、Kotlinに関数をバックグラウンドスレッドで実行するようにという指示ではありません。コルーチンはメインスレッド上で実行されます。実際、UIイベントへの応答でコルーチンを起動する際にDispatchers Main immediateを使うのは適切な方法です。それにより、メインセーフティを必要とする長時間実行タスクを最終的に行わなくても、ユーザー向けのすぐ次のフレームで結果を利用できます。コルーチンはメインスレッド上で実行されます。中断はバックグラウンドを意味するものではありません。メインスレッドで実行するには低速すぎる処理を行う関数をメインセーフにするために、デベロッパーはKotlinコルーチンに対して、DefaultまたはIOのいずれかのディスパッチャで処理を実行するよう指示できます。Kotlinでは、すべてのコルーチンは、メインスレッド上で実行している場合であっても、ディスパッチャ内で実行する必要があります。コルーチンは自身を中断させることができ、ディスパッチャはそうしたコルーチンを再開する方法を知る存在です。コルーチンを実行すべき場所を指定するために、Kotlinでは、デベロッパーがスレッドのディスパッチに使用できるつのディスパッチャを用意しています。Dispatchers MainAndroidでいうメインスレッド。UIとやり取りして軽量の処理を実行する中断関数を呼び出すUI関数を呼び出すLiveDataを更新するDispatchers IOメインスレッド外。ディスクおよびネットワークIOに最適化されているデータベース ファイルの読み取り、書き込みネットワーク  Dispatchers Defaultメインスレッド外。CPUに負荷がかかる処理向けに最適化リストの並べ替えJSONの解析DiffUtils 中断関数、RxJava、またはLiveDataを使用する場合、Roomはメインセーフティを自動的に提供します。  Kotlinコルーチンで使用する場合、RetrofitやVolleyなどのネットワークライブラリは独自のスレッドを管理し、コードにおいて明示的なメインセーフティを必要としません。上記の例に続けて、ディスパッチャを使ってget関数を定義してみましょう。getの中でwithContext Dispatchers IO を呼び出して、IOディスパッチャ上で実行するブロックを作成します。そのブロック内のコードはすべて、常にIOディスパッチャ上で実行されます。withContextはそれ自体が中断関数であるため、コルーチンを使用して動作し、メインセーフティを提供します。   Dispatchers Mainsuspend fun fetchDocs        Dispatchers Main val result   get  developer android com      Dispatchers Main show result     Dispatchers Mainsuspend fun get url  String       Dispatchers IO withContext Dispatchers IO       Dispatchers IO   ブロッキング処理を実行        Dispatchers Mainコルーチンによって、スレッドのディスパッチを細かく制御しながら実行できます。withContextは、コールバックを使って結果を返すことなく、コードの任意の行を実行するスレッドを制御できるため、データベースからの読み取りやネットワークリクエストの実行といった極めて小さい関数に適用できます。そのため、withContextを使って、すべての関数をMainを含むあらゆるDispatcherで安全に呼び出せるようにすることをおすすめします。そうすることで、呼び出し側において、関数を実行するためにどのスレッドが必要となるかを検討する必要がなくなります。この例では、メインスレッド上で実行中のfetchDocsが、ネットワークリクエストをバックグラウンドで実行するgetを安全に呼び出すことができます。コルーチンは中断と再開をサポートしているため、メインスレッド上のコルーチンは、withContextのブロックが完了するとすぐに、その結果を使用して再開します。的確に記述された中断関数は常にメインスレッドから安全に呼び出せます メインセーフ 。すべての中断関数をメインセーフにすることをおすすめします。中断関数でディスクやネットワークに関わる処理やCPU負荷の高い処理を行う場合は、withContextを使って、その関数をメインスレッドから安全に呼び出せるようにします。RetrofitやRoomといった、コルーチンをベースとするライブラリはこのパターンに従っています。コード全体をこのスタイルに従って記述すれば、コードがより簡素になり、アプリケーションロジックとスレッドに関する懸念とが混ぜ合わさるのを避けることができます。一貫してこのスタイルに従えば、コルーチンをメインスレッド上で自由に呼び出して、簡単なコードでネットワークやデータベースのリクエストを実行し、同時に、ユーザーに「ジャンク」が表示されないことが保証されます。withContextのパフォーマンスメインセーフティの提供において、withContextのパフォーマンスはコールバックやRxJavaと同等です。状況によっては、withContextの呼び出しのパフォーマンスを、コールバックにおいて可能なパフォーマンス以上に最適化することさえ可能です。関数がデータベースに対して回の呼び出しを行う場合、デベロッパーはKotlinに対して、回の呼び出しすべてを含んでいる外側のwithContextで回、切り替えを行うよう指示できます。その後、データベースライブラリがwithContextを繰り返し呼び出しても、withContextは同じディスパッチャ上にとどまり、高速パスに従います。さらに、Dispatchers DefaultとDispatchers IOの切り替えは、可能な限りスレッドの切り替えを避けるよう最適化されています。次のステップ本稿では、コルーチンがどのような問題の解決に適しているかを見てきました。コルーチンはプログラミング言語においては極めて古い概念ですが、ネットワークとやり取りするコードを簡素化できることから、最近になって注目度が高まっています。Androidでは、コルーチンを使って次のつのよくある問題を解決できます。ネットワークやディスクからの読み込み、サイズの大きいJSONの結果の解析といった、長時間実行タスクのコードを簡素化します。コードの読み取りや作成を難しくすることなく、緻密なメインセーフティを実行し、誤ってメインスレッドをブロックすることがないようにします。次回は、Androidでコルーチンを使って、画面から開始したすべての処理を管理する方法を紹介します。ぜひご覧ください。Posted by Yuichi Araki   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/8240988037275163302/comments/default",
      "updated": "2019-07-01 04:02:41"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google Play Indie Games Festival トップ作品決定 おめでとうございます",
      "description": "「Google Play Indie Games Festival 」トップ作品が、本日月日に開催されたファイナルイベントで決定し、表彰されました。おめでとうございます トップ受賞作品 アルファベット順、五十音順   ・Infection  感染  CanvasSoft・MeltLand  メルトランド    Masataka Hakozaki・くまのレストラン  Daigo Studioファイナルイベントでは、まずトップ作品の中から、審査員による審査と一般参加者投票によりトップを決定。その後、トップデベロッパーによるゲームの紹介を元に、一般参加者および審査員の採点を実施し、トップが決定しました。トップ作品 上記作品を除く。アルファベット順、五十音順   ・Lunch Time Fish   SoftFunk HULABREAKS・ReversEstory   ilili barcode・カミオリ  TeamOrigami・キグルミキノコQ bit  第章   クラシックショコラ・クマムシさん惑星ミクロの地球最強伝説  Ars Edutainment・テラセネそれでも君を照らしたい  SleepingMuseum・ペルセポネ  Momo pi特別賞の「集英社少年ジャンプ 賞」および「集英社キャラクタービジネス室賞」は集英社様の審査員により、「エイベックス賞」はエイベックス・ピクチャーズ様の審査員により、そして「ゴジラ賞」は東宝様の審査員により、トップの中から選出されました。集英社少年ジャンプ 賞受賞作品 ・ペルセポネ  Momo pi集英社キャラクタービジネス室賞受賞作品 ・はむころりん  illuCalab エイベックス賞受賞作品 ・くまのレストラン  Daigo Studioゴジラ賞受賞作品 ・相撲巻  SumoRoll横綱への道  Studio Kingmo年月日から月日に実施したオンライン投票により、「オンライン投票最優秀賞」が決定しました。オンライン投票最優秀賞受賞作品 ・ALTER EGO  株式会社カラメルカラム受賞された皆様おめでとうございます 各賞の賞品のつとしてGoogle Play Indie Games Festival 特設ページをGoogle Playに公開し、選出された作品の掲載を行っています。改めてご応募くださったデベロッパーの皆様、イベントへ参加いただいたゲームファンの皆様、審査員をはじめ関係者の皆様ありがとうございました。これからも、ゲームデベロッパーのアイディア、こだわり、思いが込められたゲームや作品を幅広く、多くの方に知っていただけるよう活動を続けていきます。Posted by Tomoko Tanaka   Developer Product Marketing Manager  Google Play",
      "link": "http://developers-jp.googleblog.com/feeds/7319907767097295521/comments/default",
      "updated": "2019-06-30 06:16:12"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google Ads APIの「クリック数の最大化」戦略用の目標予算項目作成の廃止について",
      "description": "この記事はCory LisenoによるGoogle Ads Developer Blogの記事 Sunsetting the creation of target spend field for Maximize Clicks strategies in Google Ads API を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   年月日より、APIの「クリック数の最大化」入札戦略の目標予算フィールドを廃止します。これは、AdWords APIとGoogle Ads APIのすべてのバージョンに影響し、以下の操作が行えなくなります。既存の標準戦略またはポートフォリオ戦略で目標予算フィールドを設定する廃止された項目があるポートフォリオ戦略をキャンペーンに与える今年中に、目標予算フィールドが無視され、日の予算を使って支出が管理されるようになります。この変更に対応するために、「クリック数の最大化」入札戦略に使う予算の額を指定し、キャンペーンで目標予算項目を使わないように移行する必要があります。以降では、APIの使用方法への影響について説明します。   影響を受ける目標予算フィールド Google Ads API campaign target spend target spend microsbidding strategy target spend target spend micros AdWords API Campaign BiddingStrategyConfiguration TargetSpendBiddingScheme spendTargetSharedBiddingStrategy TargetSpendBiddingScheme spendTarget変更点の説明初めて目標予算フィールドを設定する変更操作は、すべてエラーとなります。現在値が設定されている目標予算フィールドの更新は可能ですが、それまで空だった項目に新しい値を設定することはできません。また、入札戦略の目標予算フィールドに値が設定されている場合、その戦略をキャンペーンにあた操作でエラーがスローされます。新しいキャンペーンで目標予算を管理するには、キャンペーン予算の利用をお勧めします。許可されないケースでは、エラーがスローされます。許可されない操作を行うと、次のいずれかのエラーが発生します。OPERATION NOT PERMITTED FOR CONTEXTUNSUPPORTED FIELD IS SETこの変更やその他のAPI機能について質問がありましたら、フォーラムからご連絡ください。   Google Ads APIチーム、Cory LisenoReviewed by Thanet Knack Praneenararat   Ads Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/2623851608910186440/comments/default",
      "updated": "2019-06-27 04:16:39"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google Cloud Next   in Tokyo『体験エリアExpo』の詳細を公開しました",
      "description": "月日 火  月日 木 の日間にわたって開催するGoogle Cloud Next   in Tokyoまで、残りか月あまりとなりました。本日は、月日 水 および月日 木 にザ・プリンスパークタワー東京地下Fにて開催する体験エリアExpoについてご紹介します。体験エリアExpoでは、『Data Analytics』『AI』『Retail』『Workplace Transformation』『Security』『Migration  amp  Modernization』『Dev Zone  App Dev  amp  Dev Ops 』『Game』のつの体験テーマに沿った約のGoogleブース、ならびに独自のサービスをご紹介するスポンサーブースを多数設置する予定です。今回は、その中から特に注目のブースをピックアップしてご紹介します。Data AnalyticsLocation Intelligence nbsp 位置情報を活用したデータ分析プラットフォームこちらのデモでは食品の配達ゲームを通して、Google Maps Platformの機能をご体感いただくことができます。ゲーム画面上に表示される複数の配達先に、どのようなルートを選択すれば効率よく配達できるかを競い合います。AIExplore Cloud TPUGoogleが開発したAIプロセッサのアーキテクチャGoogleが開発したAIプロセッサ「Cloud TPU」のメカニズムを学べるデモです。CPU、GPU、そしてTPUのアーキテクチャの違いや、TPUがGPUより優れた費用対効果を実現できる理由を理解できます。また、TPUで構成されたAIスーパーコンピュータ「Cloud TPU Pod」のテクノロジーや応用分野を解説しています。PA GO powered by Edge TPU nbsp PA GOは、パナソニックがGoogleの機械学習技術を活用して開発する、子どものための新しい知育デバイスのコンセプトモデルです。PA GOを家の外に持ち出して、草や花、動物や虫など、外の世界で出会ったさまざまなモノを内蔵カメラで撮ると、AIがそれらが何かを音声で教えてくれます。PA GOの実装には、Googleの機械学習ライブラリTensorFlowとAIプロセッサEdge TPUによるオンデバイスでの画像認識や、Googleナレッジグラフによるモノの情報探索が利用されています。 協力会社 パナソニック株式会社 Workplace Transformation nbsp Reimagine Workstyle with AIG SuiteとChromebook、Hangouts MeetハードウェアとJamboardが、時も場所も選ばず、セキュアで効率的なクラウドコラボレーションを実現。AI技術がもたらすスマートワークをぜひ体験してください。ミニセッションも開催します。SecurityCloud Armor Fortified CastleCloud Armorをご利用いただくことで、外部からの有害な攻撃を防ぐことができます。Webサイトを城に見立て、そこに攻撃を仕掛けてくる忍者や侍を、Cloud Armorを使って簡単に守れる様子をゲーム感覚で体験できるブースです。Migration  amp  ModernizationHybrid  amp  Multi Cloud CityAnthosやKubernatesについて詳しくない方でも、スクリーン上に構築された仮想都市を通じて、ハイブリッドなコンテナ環境、構成管理、Service Meshの構築など、Anthosの概念を体験しながら理解できます。AppDev  amp  DevOpsLife of Your Code with Cloud Runコンテナ化したアプリケーションを、Cloud BuildでビルドしCloud Runにデプロイする一連のCI CDワークフローをインタラクティブに体験できます。DevOps with Google Cloudデジタル時代を生き抜くためのビジネスを創出する際、DevOpsを行うことによるメリットは大いにあります。本ブースではDevOpsによる効果とGCPを使った実装方法をデモを通じてお伝えします。ぜひ、上記をご参考に、Google Cloud Next   in Tokyoの体験エリアExpoにお越しいただければ幸いです。会場で皆さまにお会いできますのを、Google Cloud社員一同楽しみにしております。イベント申し込み amp セッション登録イベント概要イベント名  Google Cloud Next   in Tokyoウェブサイト 日程  年月日 火 ・月日 水 ・月日 木 時間 月日 火 Bootcamp 有料          予定 DevDay     予定 月日 水 開場   予定 基調講演     予定 セッション    予定 月日 木 開場   予定 基調講演     予定 セッション    予定 Next   in Tokyo Night     予定 会場  ザ・プリンスパークタワー東京および東京プリンスホテル注意事項※本カンファレンスは、Google Cloudの製品・サービス導入を検討されているエンドユーザー企業、団体、教育機関、政府自治体向けのイベントです。※十分な座席数をご用意しておりますが、定員を超えた場合、エンドユーザー企業様優先の抽選とさせていただきます。※イベントの当日登録は承ることが出来ません。前もってご登録くださいますよう、お願いいたします。※ご不明な点は、FAQをご確認ください。ーお問い合わせ先Google Cloud Next Tokyo運営事務局gc nexttokyo info google com",
      "link": "http://developers-jp.googleblog.com/feeds/8932625431035506232/comments/default",
      "updated": "2019-06-26 06:07:23"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google広告の「掲載位置」入札戦略から「インプレッションシェア」入札戦略への移行について",
      "description": "この記事はMike Cloonan nbsp によるGoogle Ads Developer Blogの記事 Migrating from position to impression share bidding strategies in Google Ads を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   年月に、「検索ページの目標掲載位置」および「目標優位表示シェア」入札戦略を新しく追加する機能を停止します。これは、AdWords APIとGoogle Ads APIの両方に影響します。今年中に、これらの戦略が設定された既存のキャンペーンは、新しい入札戦略である「目標インプレッションシェア」に自動的に移行されます。「目標インプレッションシェア」では、柔軟で細かい制御ができるので、希望のインプレッションシェアや検索ページの掲載位置を最適化できます。以降では、APIの使用方法への影響について説明します。Google Ads APIユーザーBiddingStrategyTypeとして、TARGET OUTRANK SHAREやPAGE ONE PROMOTEDではなく、TARGET IMPRESSION SHAREを使うようにします。TARGET OUTRANK SHAREやPAGE ONE PROMOTEDが削除された後、これらを作成しようとすると、エラーが発生します。AdWords APIユーザーTARGET OUTRANK SHAREやPAGE ONE PROMOTED入札戦略が削除された後、AdWords APIを使ってこれらを追加しようとすると、エラーが発生します。どちらのAPIでも、古い入札戦略を使っている既存のキャンペーンは、すべて自動的に「目標インプレッションシェア」を使うように移行されます。AdWords APIは「目標インプレッションシェア」入札戦略をサポートしないので、この機能が必要な場合はGoogle Ads APIに移行することをお勧めします。この変更やその他のAPI機能について質問がありましたら、フォーラムからご連絡ください。Google Ads APIチーム、Mike CloonanReviewed by Thanet Knack Praneenararat   Ads Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/7737837144170990221/comments/default",
      "updated": "2019-06-26 01:29:17"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google Ads API v のお知らせ",
      "description": "この記事はNadine Sundquist nbsp によるGoogle Ads Developer Blogの記事 Announcing v  of the Google Ads API を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   本日は、Google Ads API v がリリースされたことをお知らせします。このバージョンでは、引き続きvをエンドポイントとして使用できますが、v の機能を利用するには、クライアントライブラリのアップデートが必要になります。主な機能は以下のとおりです。CampaignDraftおよびCampaignExperimentリソースにより、キャンペーンの下書きとウェブテストのサポートを追加 AdWords APIのキャンペーンの下書きとテストと同等 Campaign labelsを使い、キャンペーンリソースに関連付けられたすべてのラベルリソース名を取得する機能を追加利用できるリソースv のGoogle Ads APIを利用するにあたっては、以下のリソースが役立ちます。リリースノートガイド移行ガイドリファレンス更新版のクライアントライブラリとコードサンプルは、年月日までに公開されます。ご質問やサポートが必要なことがありましたら、フォーラムからご連絡ください。 Nadine Sundquist Google Ads APIチームを代表して Reviewed by Thanet Knack Praneenararat   Ads Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/5442503814051911519/comments/default",
      "updated": "2019-06-25 05:20:55"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "GoogleのAutoMLがKaggleDaysでの表形式データのコンペで第位に",
      "description": "この記事はYifeng Lu nbsp によるGoogle AI Blogの記事 An End to End AutoML Solution for Tabular Data at KaggleDays を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   投稿者  Google AIソフトウェアエンジニア、Yifeng Lu データベースのテーブルや表計算のシートなどの「表形式データ」に対する機械学習 ML の適用は、特に活発に研究や実用化が進んでいる領域です。小売、サプライチェーン、金融、製造、マーケティングなど、多くのビジネス分野において、不正検知や在庫予測など、表形式データにまつわる様々な課題が存在します。こうした課題をMLで解決するソリューションの開発にはMLの専門家が欠かせません。例えば、手作業による特徴量エンジニアリングやハイパーパラメータの調整などにより、最適なモデルを作成する必要があります。しかし、こういったスキルを持つ人材は希少であり、MLによる業務の効率的な改善は簡単ではありませんでした。こうしたビジネスや研究でのMLの導入を加速し、スケーラブルなものとするのが、GoogleのAutoMLです。AutoMLの初期の取り組みであるニューラルアーキテクチャ検索 NAS では、NasNetを通して画像認識の分野に革新をもたらしました。さらに、AmoebaNetなどの進化した手法や、エッジ向けのモバイルビジョンアーキテクチャであるMNasNetによって、AutoMLの特徴である「learning to learn」手法のメリットがさらに明らかになっています。そして先日、Googleは表形式データにlearning to learnのアプローチを適用し、次のつの特徴を備えるスケーラブルなエンドツーエンドAutoMLソリューションを開発しました。全自動 学習データと計算リソースを投入するだけで、すぐに利用できるTensorFlowモデルを出力します。その途中で人による作業は発生しません広く適用可能 表形式データを利用するあらゆる課題に適用できます高品質 AutoMLが生成するモデルの性能は、トップクラスのMLエキスパートが手作業で作ったモデルに匹敵しますこのソリューションを評価するため、Kaggleのハッカソンイベント nbsp KaggleDays SF Hackathon nbsp にAutoMLでエントリしました。これは nbsp KaggleDaysイベントの一部で、最大名のチーム組が時間半をかけて競うコンペです。ここで nbsp AutoMLがKaggleの参加者と初めて競い合ったのは、一連の自動車部品についての素材の特性とテスト結果の情報を与えて、製造における欠陥を予測する課題でした。相手にはKaggle progression system nbsp のMasterレベルの参加者やGrandMasterレベルの参加者もたくさんいました。しかし私たちのAutoMLのチームはほぼ日中首位をキープし、順位表にあるように最終的には僅差で位となりました。私たちのチームのAutoMLソリューションは、複数ステージのTensorFlowパイプラインで構成されています。第ステージは、自動特徴量エンジニアリング、アーキテクチャ検索、検索によるハイパーパラメータチューニングを担当します。第 nbsp ステージを経た有望なモデルは第ステージに送られ、交差検証とバギングが適用されたのち、優れたモデルが選択されます。その後、第ステージで得られた特に優れたモデルを組み合わせて、最終的なモデルとします。「Google AutoML」チームのワークフローは、他のKaggle参加者のワークフローとはまったく異なっていました。他の参加者がデータを分析してさまざまな特徴量エンジニアリングを試している間、私たちのチームはジョブをモニタリングしてその終了を待っているだけです。最終的に位となったソリューションのステージが終了するまでには、 nbsp  CPU時間の計算処理を必要としました。またコンテストの後に公開されたパブリックカーネルを調査したところ、手作業で設計した上位のモデルをAutoMLモデルで拡張すれば、MLエキスパートがさらに高性能のシステムを構築できることがわかりました。下の表が示すように、AutoMLにはMLエキスパートの能力を強化し、さまざまな課題に広く対処できるようにする可能性が秘められています。AutoMLモデルと他のKaggle参加者のモデルを組み合わせることでモデルをさらに改善できる可能性を表す順位表。「Erkut  amp  Mark  Google AutoML」には、位となった「Erkut  amp  Mark」と位となった「Google AutoML」のモデルが含まれている。Erkut Aykutlug氏とMark Peng氏は、XGBoostと独自の特徴量エンジニアリングを利用。一方のAutoMLは、自動の特徴量エンジニアリングとハイパーパラメータチューニングとともに、ニューラルネットワークと勾配ブースティングツリーの双方 TFBT を使った。Google Cloud AutoML Tablesこのコンペで利用したソリューションは、Cloud Next  でベータ版が提供開始された nbsp Google Cloud AutoML Tablesのメインアルゴリズムに採用されています。次の図に示すように、複数のKaggleコンテストを対象としたベンチマークテストではAutoML Tablesがコンスタントに高成績を記録しており、同様のサービスとしてはSoTA性能を達成しています。複数のKaggleコンテストを対象としたサードパーティによるAutoML Tablesのベンチマーク結果AutoMLの手法は、実際のビジネスにおけるさまざまな問題に対して応用できる可能性があります。既にいくつかのお客様が、企業内のさまざまな表形式データに対してAutoML Tablesを適用し、サプライチェーン管理やマーケティングのリードコンバージョン最適化などの用途に活用しています。表形式データにまつわるさまざまな課題の解決に、最先端のML技術を適用可能になったことを嬉しく思います。謝辞 このプロジェクトは、Google Brainチームのメンバー、Ming Chen、Da Huang、Yifeng Lu、Quoc V  Le、Vishy Tirumalashettyの尽力があってこそ実現できました。また、すばらしいインフラストラクチャとプロダクトのランディングに関して協力してくださったCloud AutoML TablesチームのDawei Jia、Chenyu Zhao、Tin yun Hoにも感謝します。魅力的なコンテストを開催してくださったWalter Reade、Julia Elliott、そしてKaggleの皆さんにも感謝いたします。 Reviewed by Kaz Sato   Staff Developer Advocate  Google Cloud   ",
      "link": "http://developers-jp.googleblog.com/feeds/8198045166569979154/comments/default",
      "updated": "2019-06-24 01:43:36"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google I O   Chromeとウェブの新機能",
      "description": "この記事はBen Galbraith、Dion Almaer nbsp によるChromium Blogの記事 Google I O   What s new with Chrome and the Web を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent   今年、ウェブは生誕周年を迎えました。なんとすばらしい年だったことでしょうか。ウェブプラットフォームは、シンプルなハイパーテキストドキュメントを支えるものから、デザインの最先端で臨場感あふれるダイナミックな体験を実現するものへと発展しています。この先も世界のニーズは進化を続けるでしょう。私たちが世界のウェブコミュニティに参加し続け、将来のニーズを満たすようにこのプラットフォームを適応させ、改善する動機は、その点にあります。私たちが集中的に取り組んでいるのは、ユーザーの信頼と安全を中心に据えつつ、ウェブを高速かつ強力なものにすることです。ウェブの即時性というビジョンウェブでは、スピードが重要です。ユーザーは読み込み速度に非常に敏感で、これがビジネスに直接的な影響を与えることがわかっています。そこで、ブラウザを高速かつ軽量にしつつ、デベロッパーの経験を活かして多くのことを実現するために、私たちは懸命に努力を続けています。起動時のボトルネックに注目することで、ローエンド端末でのChromeの起動時の読み込み速度を 短縮することができました。すべての端末の平均では、 の短縮になっています。さらに、スクロールのパフォーマンスが 向上し、Vによって、実際のアプリでJavaScriptのメモリ使用量が最大 減少しています。ブラウザの効率化に加えて、デベロッパーの重荷を取り除くための機能追加も行っています。その中からいくつかご紹介します。デベロッパーが独自のJavaScriptソリューションを作成しなくても優れたイメージ読み込みを実現できるように、イメージとiframeを直接ブラウザに遅延読み込みするlazy loading nbsp を追加します。デベロッパーがシンプルなHTML属性を追加するだけで、あとはChromeがすべて行ってくれます。高速なサイトでユーザーが別のページに移動する場合、Chromeが積極的にページをクリアする機能によって、ユーザーエクスペリエンスが低下する可能性があります。この白い「読み込み中ページ」が一瞬だけ現れるのを防ぐため、Paint Holdingという新しい試験運用版機能を導入し、ウェブのナビゲーションを改善します。この機能がAwwwardsのウェブサイトで実際に動作している様子をご覧ください。 nbsp ナビゲーションについて言えば、ユーザーがウェブを横断する方法を根本的に変革する新しいテクノロジー、Portals nbsp があります。Portalsは、iframeと同じように、コンテンツをページ内に直接埋め込むものです。しかし、標準のiframeとは違い、「アクティベート」してトップレベルのページになることができます。これにより、ウェブのあらゆる場所で瞬間的な遷移が可能になります。オリジナルページのUIのパーツを安全な方法で個別に保持できる高度な体験なので、当初のモデルの理想を維持したまま、シームレスなオーバーレイを提供できます。I Oでは、デベロッパーがPortalsや関連APIを使ってプリフェッチ、画面遷移の拡張、サイト間のコンテキスト情報の交換などをサポートする方法をお知らせしました。現在、Canary版のChromeでフラグを使うと、Portals APIを利用できます。この新しいプリミティブを使ってデベロッパーの皆さんがどんなものを構築するのか、楽しみにしています。特に期待を寄せているもうつのテクノロジーが、Web Packagingです。これは、ウェブデベロッパーとウェブサーバーとの間の新しい取り決めです。Web Packagingを使うと、ページ読み込みのモデルが変わり、ブラウザがオリジンサーバーからページをリクエストするモデルから、ピア端末を含めてどこからでも読み込めるモデルになります。これにより、コンテンツをプリロードしてページを即座に読み込むという柔軟性がブラウザに生まれます。さらに、この処理はプライバシーが保護された状態で行われます。このビジョンの第フェーズであるSigned Exchangeは、Chromeでデベロッパーが利用できるようになりました。ここで説明したのは、ウェブの即時性を上げる機能の一部です。しかし、それが成功するかどうかは、デベロッパーがウェブを高速にし、パフォーマンスを維持できるかどうかにかかっています。そこで、その助けとなるさまざまなツールを追加しました。年に導入したChrome UXレポートは、実際の指標を提供することにより、ユーザーが体験しているウェブページについて理解を深めてもらうためのものです。現在、このレポートには万近くのオリジンのデータセットが含まれており、Google Search Consoleの最新のスピードレポートなど、多くのツールで活用されています。このレポートは現在ベータ版で、こちらからプログラムへの登録やフィードバックの送信ができます。デベロッパーの皆さんにさらに幅広く実際の指標を提供するため、FirebaseチームはPerformance Monitoringツールを強化し、ウェブアプリをカバーするようになりました。また、パフォーマンスというレールから外れないように、多くのトップサイトがビルド環境で「パフォーマンス予算」を実装しています。そこで、Lighthouseにパフォーマンス予算を直接組み込み、本番サイトがパフォーマンスリグレッションの影響を受ける前に通知するようにしています。ウェブをさらに強力に私たちが目指しているのは、皆さんがウェブでユーザーに体験してほしいあらゆることを実現できるようにすることです。そこで、皆さんの体験をユーザーの体験に近づけることで機能面のギャップを埋めようと、昨年より懸命に作業を続けてきました。特に差し迫った重要なニーズに対処するために、コミュニティと密接に連携し、たくさんの機能を非常に速いペースで完成させています。中でもすばらしい機能に、ファイルシステムアクセス、ストレージの無制限割り当て、SMSベース認証などの機能があります。認証プロセスで「ワンタイムパスワード」が重要な部分を占めているマーケットで開発しているデベロッパーには、SMSベース認証が特に重要になります。この分野には引き続き力を注ぎたいと考えていますが、あなたのアプリをネイティブのシステム共有機能と連携させることができるWeb Share Target APIは既にリリースしており、本日のI OでリリースしたWeb Perceptionツールキットなどの体験を実現するShape Detection APIも開放しています。このツールキットを使うと、モバイルカメラを組み込んだり、ウェブサイトの利用効率をアップしたりできます。モバイルでのこのような強力な機能の提供に加えて、質の高いウェブエクスペリエンスを構築するデベロッパーのリーチを広げたいとも考えています。そこで、デベロッパーがウェブコンテンツをAndroidアプリに組み込めるようにするTrusted Web Activityをリリースしました。インド最大の手頃な価格のホテルネットワークであるOYO Roomsなどの企業は、既にTrusted Web Activityを使って軽量版エクスペリエンスを構築しています。これは、一部のマーケットのパートナーでよく見られるパターンです。しかし、私たちにとって一番うれしいのは、デスクトップ機能の進展です。Web Assemblyなどのテクノロジーや個々のメディア、そして生産性を向上するAPI群によって、新しいユースケースを数多く実現できています。HuluやGoogleのDuoは、現在のウェブで実現できるすばらしい事例を表しています。また、Slackが、オフライン対応したデスクトップアプリのウェブ版を今年中にリリースするために力を尽くしていることをうれしく思います。昨年、フルウィンドウのサポートや一般的なデスクトップアプリ機能を必要とするウェブアプリを実現するため、デスクトップPWAがChromeOSに対応しました。そして、このサポートはWindows、Mac、Linuxに広がっています。デスクトップPWAには目覚ましい勢いがあり、ユーザーが高品質なPWAを見つけて簡単にインストールできるように、私たちの役割を果たしてゆきたいと考えています。そこで、ChromeのUIのアドレスバー内に直接 インストール ボタンを作って見つけやすさを向上させました。これは、デベロッパーが忠実なユーザーに対するエンゲージメントを向上させ、すばらしい体験を構築できるようにするための歩です。この点については、さらに改善してゆきます。端末のフラグメント化が激しい中、ローエンドのフィーチャーフォンから大画面デスクトップまで、すべての端末で動作する体験を作ることができるか、確認する必要がありました。そこで、楽しいゲームProxxを制作しました。これは、UIにpreactを使い、ワーカーを使ってメインスレッド以外で多くの処理を行えるようにComlinkを利用しています。そして、すべての端末で動作します。制約の多い端末でも問題はありません。ウェブ上にあるので直接プレイすることもできますが、おそらく記事を読み終わってからの方がよいでしょう  来年中に、ウェブで次世代のゲーム、生産性、メディア、ソーシャルアプリを実現するさらなる可能性を実現したいと考えています。もちろん、ユーザーの安全性や信頼性といった中核原則はすべて維持します。透明性と選択肢とコントロールをユーザーの手にユーザーエクスペリエンスは私たちにとって非常に重要で、その中心にあるのはユーザーの安全性とプライバシーです。I Oでは、ChromeがCookieを扱う方法に関する重要な変更予定についてお知らせしました。これは、トラッキングに対する選択権を提供して、ウェブ全般のセキュリティとプライバシーを向上させるものです。今年中には、ユーザーにウェブのトラッキングに関する透明性と制御を提供できる新機能のプレビュー版をChromeに導入する予定です。私たちは、以上の変更によって、ウェブにおけるユーザーのプライバシーとセキュリティが向上すると信じています。しかし、それには時間がかかるということもわかっています。そのため私たちはウェブのエコシステムと連携し、Chromeがどのようにポジティブなユースケースをサポートし続けることができるかを理解し、よりよいウェブを構築してまいります。デベロッパーエクスペリエンスの改善ウェブプラットフォームのスコープは広がり、高速で安全で高機能を求めるユーザーからの声は高まっています。その中で、高品質なサイトを作るのは、難しくなるのではなく簡単になるべきです。そこで、web devを構築し、Lighthouseという測定ツールとガイドをか所にまとめました。このサイトを改善し、現在、パフォーマンス、安全性、アクセシビリティ、レジリエンスなどを実現する以上のわかりやすいガイドを参照できるようになっています。I Oでは、Reactを始めとして、皆さんが使うツールのガイドを作成する予定もお知らせしました。現在は、デベロッパーレポートや利用しているフレームワークについての改善のヒントを提供できるように、Lighthouseに同レベルのカスタマイズを追加する作業を行っています。Wordpress用のガイドは既に組み込まれています。年およびそれ以降で、さらに追加する予定です。この年間で、ウェブは長い道のりを歩んできました。デベロッパーコミュニティの皆さんや他のブラウザと協力して、多くの人がどこからでも情報やサービスにアクセスできるようにするという共通のミッションに向かえることは私たちのよろこびです。Google I O Extended  Recap Live Japan より、日本語で行われたセッション    Reviewed by Eiji Kitamura   Developer Relations Team",
      "link": "http://developers-jp.googleblog.com/feeds/277733240260599808/comments/default",
      "updated": "2019-06-21 01:37:14"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Android Qの対象範囲別ストレージ おすすめの方法と最新情報",
      "description": "この記事は nbsp Jeff Sharkey  Software Engineer  Seb Grubb  Product Manager によるAndroid Developers Blogの記事 Android Q Scoped Storage  Best Practices and Updates を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。アプリ間を相互に隔離するアプリケーションサンドボックスは、Androidの設計の根幹をなす仕組みのつです。Android Qでは、アプリケーションサンドボックスと同じ基本原理に則り、新たに対象範囲別ストレージを導入しました。ベータ版のリリース以降、Androidの改良につながる貴重なフィードバックを数多くお寄せいただきました。対象範囲別ストレージは、皆様からのご意見をもとに考案され、Android Qベータ版で採用することになった新機能です。この投稿では、アプリで対象範囲別ストレージのサポートを宣言する方法について説明し、皆様から寄せられた疑問を解消するためのおすすめの方法を紹介します。対象範囲別ストレージの導入に関する最新情報まず、対象範囲別ストレージの導入が既存のアプリに及ぼす影響ですが、現行のおすすめの方法に沿ってストレージにアクセスしているアプリについては、影響を最小限に抑えられると想定しています。しかしながら、一部のアプリでは複雑な変更が必要になるとの指摘も寄せられており、その場合の影響についてはもう少し時間をかけて評価する必要があると思われます。Googleとしては、こうした変更にアプリを対応させるには時間がかかることも承知しており、できる限りのサポートを提供できたらと考えております。ベータ版の次期リリースでは、Android  Pie APIレベル 以前をターゲットにするアプリの場合、デフォルトではストレージの動作は以前のAndroidバージョンと変わりません。対象範囲別ストレージを使用できるように既存のアプリを更新すると、APIレベル以前をターゲットにしているアプリでも、新しいマニフェスト属性を使用して、Android Qデバイス上で新しい動作を有効にできます。来年度のメジャープラットフォームリリースでは、ターゲットSDKレベルに関係なく、すべてのアプリで対象範囲別ストレージのサポートが必須となりますので、早めにご対応いただくことをおすすめします。対象範囲別ストレージに関するフィードバックや、アプリでの使用例に応じた改善提案などございましたら、引き続きお知らせください。フィードバックはこちらのアンケートから、バグや機能要望はこちらのフォームからお送りいただけます。一般的なフィードバックとおすすめの方法皆様からのフィードバックは大変貴重であり、Androidの設計に関わる意思決定にも役立てております。ここでは、皆様から多く寄せられた疑問を解消するためのおすすめの方法を紹介します。共有するメディアファイルの格納。アプリで取り扱うファイル 写真など を他のアプリと共有することが想定されており、アプリのアンインストール後も保持する必要がある場合は、MediaStore APIを使用してください。一般的なメディアファイル用のコレクションとして、Audio、Video、Imagesが用意されています。それ以外のファイルタイプについては、新しいDownloadsコレクションに格納できます。アプリからDownloadsコレクションのファイルにアクセスするには、システムファイル選択ツールを使用する必要があります。アプリ内部ファイルの格納。アプリで取り扱うファイルを他のアプリと共有することが想定されていない場合は、そのパッケージ専用のディレクトリに格納します。これによりファイルを整理しやすくなり、アプリをアンインストールした場合でもOSでクリーンアップを管理できます。Context getExternalFilesDir  の呼び出しも引き続き使用できます。権限とファイルの所有権の処理。アプリでMediaStoreを使用する場合、アプリ自体のファイルにアクセスするだけであれば権限は必要ありません。一方、他のアプリによって提供されたメディアにアクセスする場合は、権限をリクエストする必要があります。なお、アプリをアンインストールして再インストールした場合、そのアプリが以前提供したメディアにアクセスするには、ユーザーに権限をリクエストする必要があります。ネイティブコードとライブラリの処理。パターンとしては、まずJavaベースまたはKotlinベースのコードでメディアファイルを探し、そのファイルに関連付けられたファイル記述子をネイティブコードに渡す方法を推奨します。大量のファイルの効率的な処理。回のトランザクションでファイルを一括処理する必要がある場合は、ContentProvider applyBatch  の使用を検討してください。ContentProviderバッチ処理について詳しくは、こちらをご覧ください。システムファイル選択ツールとの統合。ワープロなどの文書アプリでは、ACTION OPEN DOCUMENTアクションやACTION GET CONTENTアクションでシステムファイル選択ツールを開くことができます。これらの違いについて詳しくは、こちらをご覧ください。ファイル管理アプリでは、アプリのコレクションをディレクトリ階層で管理するのが一般的です。ユーザーがディレクトリのサブツリーを選択できるようにするには、ACTION OPEN DOCUMENT TREEを使用します。返されたディレクトリ内のファイルを、アプリでさらに操作できます。この方法なら、ユーザーはインストール済みのどのDocumentsProviderインスタンスからでもファイルにアクセスでき、クラウドベースのストレージソリューションにも、ローカルバックアップ付きのストレージソリューションにも対応できます。対象範囲別ストレージについては、こちらのデベロッパーガイドでも詳しく解説しています。今後についてAndroid Qベータ版をこれほど多くの皆様にお試しいただき、チーム一同心より感謝申し上げます。あと数か月後の最終版リリースまで、引き続きテストへのご協力とフィードバックをよろしくお願いいたします。Android Qの新機能については、Google I O のセッション動画で詳しく解説しています。対象範囲別ストレージについては、月日の「What s new on Shared Storage 共有ストレージの新機能 」をご覧ください。Google I Oサイトには、他にもさまざまなセッション動画が公開されています。ぜひチェックしてください。Posted by Yuichi Araki   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/1753640009167112536/comments/default",
      "updated": "2019-06-20 01:13:39"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "INEVITABLE ja night開催レポート エクスペリエンスドリブンに向けた不可避な流れ 後編 ",
      "description": "「INEVITABLE ja night開催レポート エクスペリエンスドリブンに向けた不可避な流れ」の後編です。前編と合わせてご覧ください。テクノロジーの進歩で体験の質は向上するのか 小島  テクノロジーの話題に戻しましょう。テクノロジーが進歩したら、体験の質も向上するのでしょうか 深津  個人の考えとしては、テクノロジーが体験をよくするのではなくて、テクノロジーが飽和して、停滞して、もう違いが出ないという期間があり、しばしマンネリ化の期間を経て、初めて体験の質の向上が意識されるのかなと思ってます。小島  スペックではもう勝負がつかなくなったときということでしょうか。深津  そうです。あらゆるものの黎明期から成長期、たとえば「ウチの製品は解像度が倍高い」とか、「ウチの方が写真が倍多く撮れます」とか、そういったスペック競争が行われているときに、佇まいがどうこうと言い出すのは賢明ではありません。小島 クラウド以降、テクノロジーの飽和のスピードは速くなっていますよね。深津 そうですね。新しいテクノロジーが出ても、その先行優位性はせいぜい   年です。停滞の時間の方が長いですよね。したがって、差別化するためにエクスペリエンスが重視されているのだと思います。例えば、すごく手触りの良い、最高な乗り心地を提供する従来型の車と、すごく雑だけど安全に自動運転ができる車が出てきたとします。小島  今だったら、手触りの良さよりも安全な自動運転車を選びますよね。深津 その時点で、手触りの良し悪しに関係なく、そもそもそれが存在するかどうかで勝負が決まってしまうわけです。テクノロジーの黎明期は体験よりもテクノロジーへの投資が活発に行われますが、だんだんマンネリ化するにつれて体験側へシフトしてきます。たとえば、車と馬について考えてみましょう。最高な名馬と石炭で動くポンコツ自動車を比べるとします。もし車が世の中で初めてのもので、輸送や移動の手段が求められていたとすると、きっと車が勝ちますね。輸送手段としては馬は勝てませんからね。そこで、負けてる側は勝負のレイヤーをずらすために体験を使うわけです。小島 効率とか利益ではなく、馬に乗るのは優雅な趣味であるとかですか。深津  乗馬とかスポーツといった、実用性を諦めて体験に全振りすることで生き残るというのが、非テクノロジーの生存方法です。 inevitable experience driven vを元に作成深津  UXピラミッドと呼ばれるものに近いのですが、自分が仕事するときに、このクライアントと仕事をする意味あるかなとか、サービスがどの段階にいるかなと考えるときによくこの図に立ち戻ります。誰が悪いとかではなく、マーケット事情とかさまざまな要因で、日本の銀行システムや交通システムは、多くの人が「安全・安心に使える」というレイヤーで勝負をしています。結果的には、そこから先に投資する必要性が生まれなくなるわけです。小島  例えば、今では電車にはSuicaを使って乗車しますが、切符を買うということを考えたら駅にある切符販売機が絶対に強くて、成長する必要がないですよね。深津 市場を独占されてしまった産業は、UXを良くする必要はないよねみたいな感じで、安心・安全と言っていれば良いわけです。一方、Nike、Apple、Disneyという会社は、最上位層の「人生の意義・意味」に達していると思います。Nikeは何の靴を売っているかというよりは、スポーツすることのアイデンティティー、自分がこの靴を履いて試合に出ることが自分の生き様と重なる、こういうことを主張しています。小島 勝負靴というのもありますからね。深津  ここで、Googleという会社はちょっと特殊です。普通のプロダクトは「存在する」というレイヤーから上がってくるのですが、Googleは、テクノロジーの裏付けが物凄くあるので、いきなり真ん中あたりの安心に使えて生産性もアップしている、そんなレイヤー 「生産性・快適性」 から始まるわけです。すでに、その上のレイヤーも狙っていますよね。特に最近はこの上のレイヤーを狙う方向にシフトしてきている感じがしますね。小島 他の業種ではいかがでしょうか 深津  不動産関連も良い例ですね。タワーマンションについて考えてみましょう。安全・安心に使える 安心のオートセキュリティー、ガードマン常駐、耐震性に優れているわかりやすい使いやすい 間取り、システムキッチンがある生産性、快適性 マンション内にジムがある、駅から徒歩分嬉しい・楽しい・美しい 高級家具を完備、有名デザイナーが手がけましたなどここまでの段階で勝負がつかなくなると、夢羽ばたく魅惑のグリーンなんとかハイツみたいになっていくわけです。小島 つまり、単なる宣伝文句ではなくて、こういう地域ではマンションが飽和状態にあるということですね。これがマンションではなくて、他の商材であっても、関係者が経験を語りだしたらそのマーケットは飽和していると理解して良いですか 深津 そうですね。全員がそう言っていたら、高度に飽和した状態と言って良いと思います。小島  もう機能では勝てないわけですね。全く違う軸を立てるか、エクスペリエンスで勝負するか、そのどちらかしかないのですね。深津  全員が機能の話をしていたら、それは黎明期から普及期にいて、開発コストや技術で競争しているわけです。したがって、この段階であれば、技術への投資でまだまだ勝てるわけです。テクノロジーのライフサイクルから見たデザイナーとデベロッパーとの関係小島  黎明期、成長期、飽和期というお話がありましたが、デザイナーが力を発揮する時期と、デベロッパーやエンジニアが力を発揮する時期は、テクノロジーライフサイクルの中で異なるのでしょうか。深津 一番最初の段階では、でき上がったばかりのテクノロジーしか存在してなくて、誰も使い道をわかっていません。おそらく誰も想像もできないし、価値、魅力も十分に見出せていません。そこで、このスタート地点で、デザイナーあるいはサービス設計者が「このテクノロジーはこういうものであり、あなたの人生にこう寄与するんだ」と設計して、世に知らしめるわけです。ここはデザイナーが重要な役割を持ちます。 inevitable experience driven vを元に作成小島  デザイナーがしっかりと力を出すことが重要ということですね。例えば、ブロックチェーンがそうですよね。深津  ブロックチェーンは、実は黎明期よりもっと前の段階です。技術はだいぶ理解が深まっているので、近い将来、誰かがブロックチェーンでこの世の中がどう良くなるかを説明しなくてはならなくなります。しかし、現状は、その説明するプレイヤーがいないのが課題です。小島  ディープラーニングはいかがでしょうか 深津  ここは成長期です。技術開発の競争が続いている間は、デザイナーは整理整頓するか、せいぜい、デベロッパーやエンジニアの皆さんに頑張ってくださいと応援するくらいしかできません。小島  そして、飽和期になるわけですね。クラウドは今はこの辺ですかね。深津  そうですね、やがてそこからサイクルが生まれてきます。なお、始まりがデベロッパーからとは限りません。その手前であれば、SF小説家かもしれません。ただし、デザインする人と、エンジニアが出てきたときには、交互に役割がまわってきます。小島  テクノロジーのサイクルが早くなると、この関わり方も変わってきますよね。深津 自動車やそれ以前の活版印刷といった時代ですと、テクノロジーが普及するまで年はかかりました。年もかかる状況であれば、デザイナーとエンジニアは別々な人が担うのが普通です。しかし、そのテクノロジーが年で飽和するとしましょう。デザイナーをリストラしてエンジニアを雇おうとか、エンジニアをリストラしてデザイナーを雇おうとか、あるいは外注しようか、そうこう考えているうちに、サイクルが再び戻ってきてしまうこともありうるわけです。サイクルが早くなるほど、両方できる人、もしくは両方にブリッジできる人が重要になってきます。小島  人の入れ替えはなかなか難しいですし、そういった人を会社の中で育てるのもまた容易ではないですよね。深津  もし人材がいないなら、当面はアウトソーシングに頼りつつも、年後、年後を目標にそういう人材を企業の中に作っていけば良いのではないでしょうか。小島  最初の方で内製の方がいいよとおっしゃっていましたよね。このサイクルに耐えられる組織を持っていた方が強いと言えますか 深津  そうですね。また、外注することは、たまたま必要な職能に対しては良いと思います。たとえば、商品をたったつしか作らないのであれば、ビジョンを立てる人は最初だけいてくれれば良いわけです。体験に効く今後注目すべきテクノロジー小島  体験に効くテクノロジーについて、事前に深津さんに質問をしたところ、データ分析・データ解釈SaaSの普及サブスクリプションの普及を挙げられました。私は、VR AR、Gといった臨場感を生み出すもの、VUI、エージェント AI を考えていたのですが、まったく違いますよね。深津 先ほどの料理の話に戻すと、小島さんの考えは新しい調味料を増やす方向ですよね。しかし、私は、料理をする機会や食事をする機会を増やす、あるいはキッチンを広くする、そういった方向を注目しています。小島  では、データ分析とはどういうことなのでしょうか 深津 ある商品やサービスをお客様が満足している理由を、気持ちいいから、かっこいいから、楽しいからと説明したとしても、経営層やビジネス層にはさっぱり通じなかったりすることがありますよね。しかし、データ分析やデータ解釈の意義が広く伝わり、統計データが重要になってくると、こうすればこのサービスは長続きするよねとか、この人をこのタイミングで喜ばせればこのプロダクトの売れ行きは伸びるよねといったように、ビジネスをエンジニアリングの言葉で話せるようになるんです。小島  データで語るということですね。続いて、SaaSは 深津  テクノロジーが飽和する結果、UXの重要性が上がってくることは先程述べた通りです。しかし、バックエンドのシステムはほとんど同じでしょう。プッシュ通知のシステムも、決済システムも同じなのです。小島  つまり、同じなのだから、それでは差がつかないということですね。深津 差別化できることはやらねばとならないというように開発バイアスが上がるので、プラットフォームが均一化すればするほど、UXが重要になってきます。昔はレスポンスが良ければ、それで勝てていたのに、すぐに同程度の性能のものが登場します。しかも安価になっており、そこで戦うのは得策ではありません。その結果、エクスペリエンスで勝負をかけることになるわけです。小島 最後はサブスクリプションです。サブスクリプションが普及すれば、エクスペリエンスのニーズが高まるということですね。深津 これも持論なのですが、売り切り型でリピーター不要の商品ほど、マーケティング費に予算をかけますよね。早期に商品を魅力的に見せて、売り切ってしまう。全額回収したら、あとは知らんみたいな。そうすると、派手なコピー、派手な演出、派手なCMにどんどんリソースが注ぎ込まれるわけです。しかし、サブスクリプションでは、お客様との関係性が長期にわたりますから、サービスを大事に使ってもらおうという考え方に変わります。小島 パッケージソフトの会社が、自社商品をクラウド化して、月額課金でビジネスをするという話をよく聞きます。でも、それまでは、商品を売ったら即コスト分を回収だったことが、年年の間地道に継続しないといけないことになるわけです。もちろん、途中で利用をやめる人もいるので、やめさせない努力、施策が必要になってきます。深津  「〇〇の新機能はこれです 」という以前はよく聞いていたことが「〇〇より〇〇はより早く、落ちなくなりました」というところにエネルギーが注がれ、実際にそのサービスを使って目標を達成できるように指導することにリソースが割かれるようになったのです。小島  かつて、パッケージソフトにおけるアップデートというのはイコール新機能のことを指していた頃がありました。しかし、最近のモバイルアプリのアップデートは「このバグは修正されました」というのが多くて、どちらかといえばアプリの使い勝手を向上することが中心です。深津 そうなんです。エクスペリエンスというのは、少し先のこと、少し新しいことからもたらされるわけではないのです。もうすでに実施されていることなのです。テクノロジーが普及すればするほど、UXの重要性がますます高まります。今、この瞬間にもエクスペリエンスについて考えることが大切だと考えています。小島  なるほど、エクスペリエンスはまさに今の話であるわけですね。本日はエクスペリエンスに関わるいろいろなお話を伺うことができました。どうもありがとうございます。INEVITABLE TV  「CXO視点で見る、エクスペリエンスとテクノロジーの関係」深津さんをゲストにお迎えし、イベントでは取り上げることができなかったこと、語り尽くせなかったことを中心に、お話いただきました。 Posted by Takuo Suzuki   Developer Relations Team",
      "link": "http://developers-jp.googleblog.com/feeds/6766063034467060217/comments/default",
      "updated": "2019-06-19 01:58:08"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google Developers ML Summitを月日に開催します。",
      "description": "Googleでは、機械学習に特化したイベント「Google Developers ML Summit」を月日 木    AMより開催します。本イベントでは、GoogleのシニアフェローのJeff Deanをはじめとした、TensorFlowチームが来日し、TensorFlow、Cloud ML、ML Kit、など、Googleが開発者のみなさんに提供する機械学習ツールについてのセッションを行います。イベント概要名称 Google Developers ML Summit日時  年月日 木           会場 六本木アカデミーヒルズ〒 東京都港区六本木丁目番号六本木ヒルズ森タワーF 協力 TensorFlow User Group  GCPUGアジェンダ  AM受付開始  AM     PMキーノート  PM     PMランチ休憩ランチは会場でご用意いたします。  PM     PMブレイクアウトセッション nbsp GCP ML   ML Kitを中心としたCloudトラックと、TensorFlowトラックに分かれてセッションを行います。 各部屋の行き来は自由に行えます  アジェンダは変更になる可能性があります。申込方法本イベントへの申し込み、詳細につきましてはこちらのサイトをご覧ください。※参加可能な方には後日参加証を送付いたします。みなさまの参加をお待ちしております。Posted by Takuo Suzuki   Developer Relations Team",
      "link": "http://developers-jp.googleblog.com/feeds/1643532677661582166/comments/default",
      "updated": "2019-06-18 04:27:29"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "INEVITABLE ja night開催レポート エクスペリエンスドリブンに向けた不可避な流れ 前編 ",
      "description": "年月日に開催した第回目のイベントは、「エクスペリエンスドリブンに向けた不可避な流れ」をテーマに、ユーザーエクスペリエンスの分野で多くの実績を持つTHE GUILDの深津さんをお迎えし、あらゆるシーンでエクスペリエンスが重要視されている背景と、それを実現するテクノロジーについて、リアルなユースケースをもとに不可避な流れについて語っていただきました。THE GUILDという会社小島 会社名の「GUILD」は、傭兵的なという意味だそうですが、深津さんはこれまでどのようなお仕事に関わってこられたのでしょうか。深津 こんばんは、深津です。Flash系のデザイン、インタラクション、デベロップメントを経験した後、iPhoneアプリ開発に徐々に仕事の軸を移して、iPhone S向けアプリの設計から、本格的に関わるようになりました。実は、GUILDの会社のメンバーは全員フリーランスで、いわゆる社員ではありません。実際の仕事もプロジェクトごとに進行しています。小島 日経新聞社のアプリ開発に関わられていますよね。深津  年ほど前から、日経新聞社の電子版チームと仕事をしています。アドバイザリーという立場で、グロースだったり、ITだったり、デザインだったり、さまざまな側面で最新のノウハウをお伝えするのが私の役割です。小島 突然ですが、依頼主がアプリを内製しているか否かで、指導方法やアドバイスの内容は変わってきますか 深津 そうですね、やはり変わります。個人的には、お客様にアプリの内製化をお勧めしています。特にサービス事業を行っている会社には内製化は必要と説いています。小島 それは、事業とITが不可分な関係にあるからですか 深津  そうです。中核の事業に関係するものは内製化すべきです。しかし、その事業を補う部分については内製化にこだわらず、随時外注するやり方が良いと思います。小島  エクペリエンスを上げていく上でも、内製するかどうかはフィードバックやスピードにも関わってきますね。この点については後ほど議論しましょう。CXOという仕事深津  メインの仕事として、株式会社ピースオブケイクのCXOとしてnoteというサービスを設計をしています。小島  深津さんが書かれた「CXOってどんな仕事なの 」という記事、今日のテーマでもある「ユーザーエクペリエンス UX 」、「CXOって 」について触れられていらっしゃいますよね。深津  CXO   Chief Experience Officer、日本語で言うと最高体験責任者でしょうかね。この記事はあくまでも私個人の考えなので、一般的なCXOとは異なるかもしれません。そもそも、プロダクトを最初から最後まで全部いい感じにすることが大きなテーマになります。したがって、我々のチームではそもそもnoteって何か、noteが存在することで、世の中がどう変わるのかというところまでをチーム全体で一緒に考えています。たとえば、noteで仲間を増やすことはできるのか、ユーザーはnoteを使うことで最終的にどこに向かうのかといったことです。こうしたことを考えながら、実際できあがったサービスをレビューしていくわけです。小島  使っている人の、「これすごいよ」とか「イケてるよね」とか「使いやすいよね」とかを見ているんですね。深津  数字で測れないところもあわせて見ています。小島  体験を作るには、デザインの要素が大事ですよね。たとえば、絵コンテがあって、このUXにしたいということもありますね。深津  「弊社のサービスのUXをかっこよくしてください。」という依頼をいただくのですが、詳しくヒアリングをすると、アニメーションをかっこよくして欲しいという内容だったことも多々あります。でも、そうじゃないんですよね。小島  そうではないというのは 深津  アニメを作るというのはあくまでも部分の話であって、体験っていうのはもっと広いわけです。ユーザー体験とは、そのプロダクトに初めて出会って、脳内に記憶が始まった瞬間、レコードアプリでレコードボタンを押した瞬間から、そのアプリの噂、どんな感じ、どんな第一印象、欲しくなったんだっけ、実際に使ってどうだったんだっけ、カスタマーサポートは、、、そういうものを全部含めたものです。小島  見た目だけでは無いということですね。深津  見た目の部分は、それを使う瞬間、もしくは使うちょっと手前の「欲しいかな 」のあたりの話です。結局、脳内から関係性が消えて、そのプロダクトとの縁が完全に消えるまでは、体験の守備範囲なわけです。小島  泥臭いイメージがありますね。深津  泥臭いというか、横断的ですね。いろいろな立場、役割を持つ方々とつながっていきます。新しい職種、あるいは日本ではまだまだマイナーな職種ですね。小島  そうなると、このイベントの参加者の中で、CXOになりたいと思ってくれる方が入れば、このイベントは成功ですね。なぜ、今「体験」なのか 小島  体験が、なぜ今大事なのかという話題に進みましょう。「経験則から言えば、お客はいい体験をすると、その話を人に話す。しかし、悪い体験をすると人に話す。」 レジス・マッケンナ  こういう示唆があります。noteの開発でもこうしたことは気にされているのでしょうか 深津  いい体験をどれだけ増やしていくかとともに、悪い体験には出会わないようにすることを心がけています。小島  なるほど。この示唆がもし正しいとすると、人いい体験した人がいたとしても、人悪い体験した人がいたら、悪い体験の方が勝ってしまうっていうことですね。そして、体験の質も大事ですよね。コーンエクスペリエンスという図 Edgar Dale s “Cone of Experience   Audiovisual methods in teaching  rd ed   があります。人は実際に行動を起こしたものの方が強く記憶に残っていて、ちょっと見た程度ではすぐ忘れてしまう、というものです。つまり、エクスペリエンスを高めるためには、全員に何かしらの行動させなきゃいけないということになりますが、そういう理解でよいですか 深津  個人的には、コーンエクスペリエンスは誤解を生じやすい考え方と思っています。ここでは、読む体験よりも、実際に自分で触ってみたり運動したり活動する体験の方が、強い体験、濃い体験であるとしています。しかし、だからと言って、それが必ずしも良い体験というわけではありません。小島  体験の良さではない 深津  お味噌汁の話に例えると、単に読むことと実際に行動におこしたことで体験の濃さが違うというのは、味噌汁が薄い味か濃い味かという話にすぎないのです。美味しいかどうかという話とは別の次元です。小島  なるほど。いつも食べたい味なのか、特別な時に食べたい味なのかというと、また全然違いますね。深津  飲食店の場合は、そこは商売ですからガツンとくる味が求められます。しかし、人は毎日そういうものを食べたいと思っているかというと、ちょっと違いますよね。塩分が強すぎて体が壊れちゃいますからね。あと、夫婦生活でもそうですよね。小島  結婚ということですか 深津  結婚式は劇的な方が良いですけど、日々の夫婦生活で結婚式のテンションを維持し続けることはなかなか難しいですよね。小島  日も持たない気がします 笑 。深津  体験というのは、強ければ良いというものではないのです。適切なタイミングで適切な強さを与えた方が良いわけです。最初に印象の強いものを出して、次は定番のものを出し、珍しい味が来て、強い味のメインディッシュが来て、最後は甘いデザートといったように。この流れが良い感じに切れずに繋がっていくのが本来だと思います。付加価値と差別化を理解可能にすること小島  UXは大事になっていきますよね 深津  これからは、全体的にUXが大事になっていきます。では、なぜ大事かというと、結局テクノロジーというのは飽和しつつあって、テクノロジーとかスペックとか価格とかで勝負がつかなくなってくると、その上のレイヤーでの差別化が重要になるからです。小島  スピードが速い、解像度が高いといった観点で勝負できたことが、今はみんな一緒ということですね。深津  たとえば、解像度が億ピクセルのカメラと億ピクセルのカメラがあるとしましょう。このレベルになると、解像度では勝負にならず、撮っていて楽しいとか、使い甲斐があるとか、そういうところが重要になってきます。小島 機能差で勝負ができたことが、やがてそれだけでは勝負がつかなくなるということですね。深津  自動車、コンビニエンスストアなどの業界では、プレイヤーが提供するもののスペックや価格がほぼ同じレベルです。その結果、抽象的な場所での戦いになってきます。小島  加えて、テクノロジーは後発がキャッチアップしやすいので、先行していたとしてもどこかで付加価値とか差別化の視点を変えることが必要で、それがエクスペリエンスだということでしょうか。深津 そうですね。一方、スケールメリットを生かして、先行者が逃げ切れる業界もあります。そういう業界では、だいたい勝者は一人で、市場が独占されて他のプレイヤーが排除されてしまうため、UXは良くなりません。なぜならば、もう市場を独占しているのであえて新たに投資する必要がないからです。深津  テック企業と非テック企業でも格差がついています。小島  例えば、アマゾンと他のeコマースって同じように見えますが、テック企業かどうかという違いがありますよね。深津  スケールで何百万倍も違いますし、予算規模、配達個数も桁違いですから、最初からスペックでは勝負できないわけです。したがって、必然的にスペック以外のところに勝負の場所をずらすことになります。たとえば、「個しか生産できないすごい植物」とか「あなたのために心を込めてやりました」といった、違うレイヤーにずれていくわけです。小島 今治タオルで有名な愛知県今治にイケウチオーガニックという会社があります。ファンを大切にしていて、タオルを作る人とファンが交流できる場があります。ファンの方はそこで感動して、他の人にも今治タオルをお勧めするわけです。タオルの値段や個数だけではトップにはなれないのですが、違うところで勝負されているんですね。後編に続く。Posted by Takuo Suzuki   Developer Relations Team",
      "link": "http://developers-jp.googleblog.com/feeds/3338154949300830568/comments/default",
      "updated": "2019-06-18 02:02:49"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Google Play Indie Games Festival 年のトップ選出作品とプレゼン資料を紹介します",
      "description": "Google Play Indie Games Festivalは、昨年に引き続き回目の開催となりました。昨年どのようなゲームがトップに選出されたのか、改めてその作品と、ファイナルイベントでトップに選出されたデベロッパーの皆さまがゲームを紹介する模様を、資料と動画で振り返ります。どの作品も力作ぞろい。今年のトップ作品と合わせて、ぜひプレイしてみてはいかがでしょうか。Google Play Indie Games Festival トップ選出作品 ※ゲーム作品名・開発者名は発表時のものです旅かえる 株式会社ヒットポイントぼくとネコ  IGNITION MCraft Warriors  株式会社トランスリミットEnblox 置いて囲んで陣取りパズル 株式会社CFlatStrangeTelephone   HZ Softwarein dark  インダーク  ozumikanMillion Onion Hotel   Onion GamesPeko Peko Sushi   Hanaji Games合同会社クリスタル・クラッシュ【超攻撃的パズル合戦 】 株式会社コールド・フュージョンネコの絵描きさん  NukeninBQMブロッククエスト・メーカー  Wonderland Kazakiri inc ねぇAI、本当の事がしりたい コトリヤマ株式会社怪異掲示板とつのウワサ 株式会社エンタブリッジ思い出の食堂物語心にしみる昭和シリーズ 株式会社GAGEXリバーシクエスト   YokogosystemsNinja Flicker  デジタル創作同好会traPPARADE   個人キメラリコレクト 個人ねこかわいいぼくゆうれい 株式会社ハラペコーポレーションFrom     Serina NakajimaGoogle Play Indie Games Festival トップ選出ゲーム紹介 今年のトップ選出作品へのオンライン投票は、月日時まで 月日開催のファイナルイベントで授与される「オンライン投票最優秀賞」を決めるオンライン投票を来週 nbsp  月日時まで受け付けています。オンライン投票で選出されたタイトルは「オンライン投票最優秀賞」としてファイナルイベントにて賞が授与されます。あなたの票が最優秀賞作品を決めるかもしれません。投票をお待ちしています。Posted by Tomoko Tanaka   Developer Product Marketing Manager  Google Play",
      "link": "http://developers-jp.googleblog.com/feeds/6053210630264937854/comments/default",
      "updated": "2019-06-17 08:04:49"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Play Consoleの新しいレポートで定期購入を最適化",
      "description": "この記事はDaniel Schramm nbsp によるAndroid Developers Blogの記事 Optimize your subscriptions with new insights in the Play Console を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿code   background color  transparent    投稿者  Google Playプロダクトマネージャー、Daniel Schramm年ほど前にGoogle Playが始まって以来、定期購入は維持可能なモバイルアプリビジネスを作る上で欠かせない要素であり続けています。アメリカのGoogle Play収益トップアプリのうち、のプロダクトが定期購入を提供しています。マーケットが成熟する中で、成長を維持するためにますます重要になっているのは、デベロッパーが定期購入者のコンバージョン率と維持率の両方を最適化することです。それをサポートするために、Play Consoleから直接利用できる新しい分析機能を紹介します。定期購入維持率レポート Play Consoleでの定期購入維持率レポートデータの例。出典  Google内部データ。最近アップデートされた定期購入維持率レポートでは、定期購入者をどのくらい維持できているかだけでなく、無料試用サービスやお試し価格からのコンバージョンや、回目から回目の支払いへのコンバージョンも確認できます。SKU、国、定期購入の開始日によってつのコホートを設定できます。これは、A Bテストの成功度合いを評価する際に特に便利です。たとえば、無料試用サービス期間を変えた場合、それがどのようにコンバージョン率に影響するかを判断することができます。Play Consoleでの無料試用サービスコンバージョンデータの例。出典  Google内部データ。キャンセル時アンケートの結果既存の定期購入者の維持も、新しい定期購入者の獲得と同じくらい重要です。そこで、定期購入キャンセルレポートをアップデートし、自発的なキャンセルと自発的でないキャンセルについて、細かい分析を提供するようにしました。昨年のサブスクリプションセンターのリリースの際、キャンセル時アンケートが導入され、ユーザーがデベロッパーにキャンセルの理由をフィードバックできるようになりました。その結果は、Google Play Developer APIから利用できます。これを簡単に確認して監視できるように、日次の集計結果をPlay Console内に直接表示するようにしました。さらに、書き込まれた内容をCSVでダウンロードすることもできます。Play Consoleでのキャンセル時アンケート結果の例。出典  Google内部データ。ユーザーの復帰を後押しユーザーの支払いが失敗すると、自発的でないキャンセルが発生します。これは、キャンセル全体の分のを占めています。キャンセルレポートに新しく追加されたリカバリパフォーマンスカードを見ると、猶予期間中のユーザーやアカウントがホールドされているユーザーをどのくらい効率的に復帰できているかを把握できます。また、定期購入に復帰するまでの日数もわかるので、復帰を後押しするメッセージの効率を評価する際に役立ちます。Play Consoleでのアカウントのホールドからのリカバリパフォーマンスカードの例。出典  Google内部データ。アプリには、猶予期間とアカウントのホールドを設定しておきましょう。猶予期間とアカウントのホールドの両方を使っているデベロッパーは、定期購入ができない状態からの復帰率が倍以上増加し、 から になっています。猶予期間とアカウントのホールドの詳細もご覧ください。定期購入の維持率やキャンセルについてのレポートは、サブスクリプションページの下にあるリンクから確認できます。このページは、Play Consoleの売上レポートセクション内にあります。売上レポートにアクセスできない場合は、デベロッパーアカウントの所有者に売上データの表示パーミッションを付与してもらってください。Play Consoleでのアカウントのホールドからのリカバリパフォーマンスカードの例。出典  Google内部データ。以上の新しいレポートが、皆さんの定期購入ビジネスの最適化につながることを期待しています。 Reviewed by Takeshi Hagikura   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/7441309564266264215/comments/default",
      "updated": "2019-06-17 02:08:33"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "アプリの品質を改善してGoogle Playで見つけてもらいやすくする",
      "description": "この記事はKosuke SuzukiによるAndroid Developers Blogの記事 Improved app quality and discovery on Google Play を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。Google Playには、毎月か国以上から億人を超えるユーザーが、新しいアプリやゲームを求めて訪れます。Googleでは、Playストアでの発見がユーザーにとってすばらしい体験となるように、アプリやゲームの品質向上に継続的に取り組んでいます。その一環として、これから数週間の間にPlayストアの掲載位置とランキングのロジックを変更します。今回の変更により、品質の高い 技術的なパフォーマンスが高くコンテンツが魅力的な アプリやゲームが、ストア内の目立つ場所に優先的に掲載されるようになります。アプリの品質を改善するうえで特に重視すべき点は以下のつです。それぞれの点を改善するためのアドバイスに加え、そのために利用できるGoogle Play Consoleのツールも紹介しています。これらのツールを活用してユーザーの好みを理解し、技術的なパフォーマンスを把握し、最適なユーザーエクスペリエンスを提供してください。今後はアプリの品質が、Playストア内のどの場所に、どのようにアプリが掲載されるのかを左右することになります。常にアプリの品質に注意を向け、ユーザーが心から楽しめる魅力的なエクスペリエンスを提供してください。アプリのユーザーエクスペリエンスアプリは直感的に操作できるか、コントロールやメニューは使いやすいか、初めてのユーザーでもすぐに使えるか、全体のデザインは洗練されているか、ユーザーが長く使ってくれるだけのコンテンツが揃っているかなど、アプリのユーザーエクスペリエンスをもう一度検討してみましょう。品質に関するガイドライン 品質に関するガイドラインに基づいてさまざまなプラットフォームでテストすることにより、ユーザーの期待に応えられるだけでなく、Playストアでの掲載の機会も最大化できます。テスト版トラック アプリの初期バージョンをリリースして早期ユーザーからフィードバックを収集することで、完全リリースの前にアプリを改善できます。魅力的なコンテンツ ユーザーのニーズを満たすことにより、アプリへのロイヤリティを高めて継続的な利用を促すことができます。広告の配置 アプリに広告を掲載する場合は、適切な形式と配置を選択し、アプリ全体の広告エクスペリエンスを最適化します。アプリの安定性と技術的なパフォーマンス消費電力が抑えられているか、応答は速いか、効率的か、動作に問題はないかなど、アプリの技術的なパフォーマンスを確認してみましょう。つ星と評価しているユーザーの が、理由として安定性やバグを挙げています。Android Vitals  Android Vitalsダッシュボードでは、主な指標であるクラッシュ発生率、ANR発生率、過度のwakeup、停止した部分的なwake lock バックグラウンド に基づいて、アプリのパフォーマンスを把握できます。類似アプリを選択してベンチマークにすることで、同じカテゴリの他のアプリと比較することも可能です。Android Vitalsの指標の悪化は、ユーザーエクスペリエンスの低下につながり、Playストアへの掲載に悪影響を及ぼす恐れがあります。リリース前レポート アプリのどこに問題があるかをリリース前に特定し、品質をできる限り高めてからユーザーにリリースできます。リリース前レポートでは、自動化されたテストを実際のデバイスで実施して、レイアウトの問題の特定、クラッシュの診断、セキュリティ上の脆弱性の特定を行うことができます。効果的なストア掲載情報ストアの掲載情報が正確で効果的かどうかもアプリの品質の一部です。現在のストア掲載情報ページの第一印象はどうでしょうか。アプリの特長や便利な用途が、明確かつ正確に伝わるかどうか見直してみましょう。おすすめの方法 アプリのクリエイティブアセット タイトル、アイコン、スクリーンショット、動画など を強化し、アプリの説明をわかりやすく充実したものにすることで、アプリの魅力が正しく伝わるようにします。ユーザー獲得の機会をさらに拡げるには、すべてのページに動画 公開または限定公開で収益化していないもの を追加して、ユーザーがアプリの動作を視覚的に理解できるようにします。ゲームの場合は、アスペクト比 のスクリーンショットを枚以上掲載することをおすすめします。新しいアイコン仕様  月日までにアイコンを変更し、Playストアと調和したエクスペリエンスを提供します。評価とレビュー ユーザーによるレビューや評価を日頃から確認し、否定的なレビューにはできる限り返信します。デベロッパーからの返信を受け取ったユーザーは、おおよそ人中人が評価を星つ上げてくれることがわかっています。また、年月にはPlayストアに新しい評価方法が導入されることになっています。最近の評価がより重視されるようになるため、評価やレビューに注意を払うことが一層重要になります。ストア掲載情報のテスト機能 複数のバージョンのストア掲載情報ページを作成し、実際のGoogle Playユーザーを対象としたA Bテストを実施できます。有意な結果を収集するには、各コンポーネントを個別にテストするようにし、少なくとも週間は継続します。ストアのカスタム掲載情報 属性 国、インストール状態、事前登録の有無など が異なるユーザーグループ向けに掲載情報をカスタマイズします。これにより、たとえば既存のユーザーと使用をやめたユーザーに対し、それぞれ最適な注目機能や最新情報を紹介できます。ローカライズ 進出する市場を特定してアプリやストア掲載情報をローカライズすることで、世界中に展開するGoogle Playを最大限に活用できます。テスト機能を使用して、国ごとにストア掲載情報を最適化することも可能です。アプリアカデミーでは、Google Play Consoleを最大限に活用し、アプリの品質を改善するためのeラーニングプログラムを無料で提供しています。Reviewed by Tomoko Tanaka  nbsp Developer Product Marketing Manager  Google Play",
      "link": "http://developers-jp.googleblog.com/feeds/462548129525646090/comments/default",
      "updated": "2019-06-14 07:47:55"
    },
    {
      "name": "Google Developer Japan Blog",
      "category": "GCP",
      "title": "Androidにワイドカラーフォトが登場 対応するために知っておくべきこと",
      "description": "この記事は nbsp ソフトウェアエンジニア、Peiyong Lin nbsp によるAndroid Developers Blogの記事 Wide Color Photos Are Coming to Android  Things You Need to Know to be Prepared を元に翻訳・加筆したものです。詳しくは元記事をご覧ください。﻿現在のAndroidのカラーチャンネルあたりビットのsRGB色域では、ディスプレイやカメラの技術を十分に活かせない領域にさしかかっています。そこでAndroidでは、ワイドカラーフォトをエンドツーエンドで実現する、つまりビット数を増やして色域を上げる作業を進めています。これが意味するのは、ユーザーが豊かな色彩のシーンを撮影し、ワイドカラーの写真を友人と共有したり、スマートフォンに表示したりできるようになるということです。そしてAndroid Qではこれらを現実に近づける取り組みが始まり、ワイドカラーフォトが登場します。そのため、アプリが広色域に対応することがとても重要になります。本記事では、アプリが広色域に対応しているか、それを表示できるかを確認する方法について説明します。また、広色域の写真に対応するために必要な手順も説明します。本題に入る前に、なぜワイドカラーフォトが必要なのか、考えてみましょう。モバイル機器のディスプレイパネルやカメラセンサーは、年々進化を続けています。新しくリリースされるスマートフォンは、キャリブレーション済みのディスプレイパネルを搭載することが多くなるでしょう。その中には、広色域に対応したものもあります。最新のカメラセンサーは、sRGBよりも広い色域でシーンを撮影できるので、広色域の写真が生成されます。このつを合わせると、エンドツーエンドで実世界の鮮やかな色を表現する写真が実現できます。技術的に説明すると、皆さんのアプリにsRGBよりも広い色域を持つICCプロファイル Display P、Adobe RGBなど が埋め込まれた写真が加わることになります。ユーザーにとっては、写真のリアルさが増すことになります。Display PsRGBDisplay PsRGB上に示すのは、同じシーンをそれぞれDisplay PとSRGBで撮影したイメージです。この記事をキャリブレーション済みの広色域対応ディスプレイで読んでいる方は、両者に大きな違いがあることに気づくはずです。カラーテストアプリの対応状況を確認できる、種類のテストがあります。私たちは、つ目をカラーコレクトネステスト、つ目をワイドカラーテストと呼んでいます。カラーコレクトネステスト アプリが広色域に対応しているか広色域に対応したアプリは、色を能動的に管理します。つまり、あるイメージが与えられたとき、アプリは常にカラースペースをチェックし、広色域を表示できるかに基づいて変換します。そのため、たとえアプリが広色域を扱えなくても、色のずれが発生することはなく、sRGB色域を使ってイメージを正しく表示できます。次に示すのは、Display P ICCプロファイルが埋め込まれたイメージを正しい色で表示した例です。 しかし、色が正しくないアプリでは、カラースペースを正しく変換せずにイメージを操作したり、表示したりすることになります。その結果、色のずれが発生します。たとえば、下のようなイメージになります。全体的に色あせて、色がずれたように見えます。 ワイドカラーテスト アプリが広色域を表示できるか広色域を表示できるアプリは、広色域のイメージが与えられた場合、sRGBカラースペースに含まれない色を表示できます。次に示すのは、アプリが広色域を表示できるかをテストするために使えるイメージです。表示できる場合、赤いAndroidロゴが見えます。このテストは、Pixel やSamsung Galaxy Sなどの広色域対応端末で実行する必要があります。 対応が必要になる点広色域写真に対応するには、少なくともアプリが広色域対応テスト カラーコレクトネステスト に合格しなければなりません。アプリが広色域対応テストに合格したら、それは何よりです しかし合格しなかった場合のために、広色域に対応する手順を示します。将来の保証を含め、広色域対応のために重要な点は、アプリが外部イメージを取得する際に、それがsRGBカラースペースであると仮定しないことです。つまり、アプリはデコードしたイメージのカラースペースをチェックし、必要な場合には変換しなければなりません。これを行わないと、色のずれが発生し、パイプラインの途中でカラープロファイルが破棄されることになります。必須 正しい色を使う少なくとも、正しい色を使う必要があります。アプリが広色域を採用しない場合、すべてのイメージをsRGBカラースペースにデコードしたいはずです。そのためには、BitmapFactoryかImageDecoderを使います。BitmapFactoryを使うAPI でBitmapFactory OptionにinPreferredColorSpaceを追加しました。これを使うと、デコードしたビットマップのターゲットカラースペースを指定することができます。あるファイルをデコードする場合、色を管理するための一般的なスニペットは次のようになります。final BitmapFactory Options options   new BitmapFactory Options      Decode this file to sRGB color space options inPreferredColorSpace   ColorSpace get Named SRGB  Bitmap bitmap   BitmapFactory decodeFile FILE PATH  options  ImageDecoderを使うAndroid P APIレベル でImageDecoderを導入しました。これは、イメージをデコードする最新の手法です。apkをAPIレベル以上にアップグレードする場合は、BitmapFactoryおよびBitmapFactory Option APIを使う代わりに、こちらを使うことをおすすめします。次に示すのは、ImageDecoder decodeBitmap APIを使ってイメージをsRGBビットマップにデコードするスニペットです。ImageDecoder Source source   ImageDecoder createSource FILE PATH  try   bitmap   ImageDecoder decodeBitmap source  new ImageDecoder OnHeaderDecodedListener      Override public void onHeaderDecoded ImageDecoder decoder  ImageDecoder ImageInfo info  ImageDecoder Source source    decoder setTargetColorSpace ColorSpace get Named SRGB           catch  IOException e       handle exception  ImageDecoderには、最終的なビットマップを得る前に、エンコードされているビットマップのカラースペースを把握できるというメリットもあります。これを行うには、ImageDecoder OnHeaderDecodedListenerを渡してImageDecoder ImageInfo getColorSpace  をチェックします。アプリでのカラースペースの扱い方によっては、これを使ってエンコードされているコンテンツのカラースペースをチェックし、別のターゲットカラースペースを設定することもできます。ImageDecoder Source source   ImageDecoder createSource FILE PATH  try   bitmap   ImageDecoder decodeBitmap source  new ImageDecoder OnHeaderDecodedListener      Override public void onHeaderDecoded ImageDecoder decoder  ImageDecoder ImageInfo info  ImageDecoder Source source    ColorSpace cs   info getColorSpace       Do something           catch  IOException e       handle exception  詳しい使用方法については、こちらからImageDecoder APIを参照してください。既知のバッドプラクティス典型的なバッドプラクティスには次のようなものがありますが、これに限られるわけではありません。常にsRGBカラースペースを前提とする必要な変換を行わずにイメージをテクスチャとしてアップロードする圧縮時にICCプロファイルを無視するこれらは、いずれもユーザーが検知できる重大な結果、すなわち色のずれを引き起こします。たとえば、次に示すのは、色が正しくないアプリのコードスニペットです。   This is bad  don t do it final BitmapFactory Options options   new BitmapFactory Options   final Bitmap bitmap   BitmapFactory decodeFile FILE PATH  options  glTexImageD GLES GL TEXTURE D    GLES GL RGBA  bitmap getWidth    bitmap getHeight      GLES GL RGBA  GLES GL UNSIGNED BYTE  null  GLUtils texSubImageD GLES GL TEXTURE D        bitmap  GLES GL RGBA  GLES GL UNSIGNED BYTE  ビットマップをテクスチャとしてアップロードする前にカラースペースをチェックしていないので、カラーコレクトネステストで紹介した色がずれたイメージができあがります。 省略可能 ワイドカラーを表示できるようにするイメージを多用するアプリでイメージを正しく扱うには、以上の必須の変更点に加えて、イメージを完全な色域で表示するための追加手順を組み込みます。具体的には、マニフェストで広色域モードを有効にするか、Display Pサーフェスを作成します。アクティビティで広色域を有効にするには、AndroidManifest xmlファイルでcolorMode属性をwideColorGamutに設定します。これは、ワイドカラーモードを有効にするすべてのアクティビティで行う必要があります。android colorMode  wideColorGamut カラーモードは、アクティビティのプログラムで設定することもできます。これを行うには、setColorMode int メソッドにCOLOR MODE WIDE COLOR GAMUTを渡します。ワイドカラーコンテンツに加えて広色域コンテンツを描画するには、描画先となる広色域サーフェスを作成します。たとえばOpenGLでは、最初にアプリで次の拡張機能をチェックする必要があります。EXT gl colorspace display p passthrough EXT gl colorspace display p次に、サーフェスを作成する際に、カラースペースとしてDisplay Pをリクエストします。次のコードスニペットをご覧ください。private static final int EGL GL COLORSPACE DISPLAY P PASSTHROUGH EXT   x public EGLSurface createWindowSurface EGL egl  EGLDisplay display  EGLConfig config  Object nativeWindow    EGLSurface surface   null  try   int attribs       EGL GL COLORSPACE KHR  EGL GL COLORSPACE DISPLAY P PASSTHROUGH EXT  egl EGL NONE    surface   egl eglCreateWindowSurface display  config  nativeWindow  attribs     catch  IllegalArgumentException e     return surface  ネイティブコードで広色域を採用する方法について詳しく説明した投稿もご覧ください。イメージライブラリ用のAPIデザインガイドライン最後に、イメージのデコードやエンコードを行うライブラリを所有またはメンテナンスしている方は、少なくともカラーコレクトネステストに合格する必要があります。ライブラリを最新化する場合、APIを拡張して色を管理する際に次のつのことを強くおすすめします。新しいAPIを設定する場合や、既存のAPIを拡張する場合は、パラメータとして明示的にColorSpaceを受け取るようにすることを強くおすすめします。カラースペースをハードコーディングするよりも、明示的にColorSpaceパラメータを使う方が将来性が高くなります。すべての従来のAPIでは、ビットマップを明示的にsRGBカラースペースにデコードすることを強くおすすめします。昔はカラーマネジメントが存在しなかったので、Android  APIレベル までのAndroidはすべてを暗黙的にsRGBとして扱います。これにより、ユーザーは下位互換性を維持することができます。開発が終わったら、上のセクションに戻ってつのカラーテストを実行してください。Reviewed by Yuichi Araki   Developer Relations Team   ",
      "link": "http://developers-jp.googleblog.com/feeds/3443399630251356229/comments/default",
      "updated": "2019-06-13 02:04:36"
    }
  ],
  [
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Kaggle KernelsノートブックでBigQueryデータを分析",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。Google Cloudのエンタープライズ向けクラウドデータウェアハウスであるBigQueryにKaggleが統合されました。BigQueryをご利用のお客様は、超高速のSQLクエリを実行し、SQLで機械学習モデルをトレーニングし、Kernelsでそのモデルを分析できるようになります。Kernelsとは、無料で使えるKaggleのホステッドJupyterノートブック環境です。BigQueryとKaggle Kernelsを一緒に使うことで、直感的に扱える開発環境を使用してBigQueryデータにクエリを実行し、データの移動やダウンロードなしで機械学習を行えます。KernelsノートブックまたはスクリプトにGoogle Cloudアカウントをリンクすると、BigQuery API Clientライブラリを使ってノートブックで直接クエリを組み立て、BigQueryで実行するという形で、ほぼあらゆるタイプのデータ分析が可能になります。たとえば、Matplotlib、scikit learn、XGBoostなど最新のデータサイエンスライブラリをインポートすれば、結果を可視化したり、最先端の機械学習モデルをトレーニングしたりすることができます。さらに、Kernelsの無料使用枠は、GPUと最大 GBのRAMを使って時間実行できるという贅沢な内容です。Kernelsが提供する機能の詳細はKaggleのドキュメントをご覧ください。万人以上のユーザーを抱えるKaggleは、データサイエンティストたちがデータの探索や分析、その成果のシェアのために集まる世界的なオンラインコミュニティです。PythonかRのKernelsノートブックを起動すればすぐにコーディングを開始でき、ほかの人々が書いた万以上の公開Kernelsからヒントを得ることができます。BigQueryのお客様にとって何よりも大きいのは、クエリや分析をか所にまとめられるIDE 統合開発環境 、すなわちKaggle Kernelsを使えることでしょう。データサイエンティストの仕事は細切れになりがちで、従来はクエリエディタでクエリを実行し、そのデータを別の場所にエクスポートして分析を終えていましたが、Kernelsはそれらをシームレスにつないでつにまとめます。しかも、KaggleはKernelsを簡単に公開できるシェアのためのプラットフォームです。オープンソースの仕事を広めたり、世界最高レベルのデータサイエンティストたちと議論したりすることができます。KaggleとBigQueryの始め方BigQueryを初めてお使いになる方は、BigQueryサンドボックスでアカウントを有効にしてください。これにより、最大で GBのストレージ、か月あたり TBのクエリ処理、 GBのBigQuery MLモデル作成クエリを無料で使用できます 料金体系の詳細はBigQueryのドキュメントをご覧ください 。KernelsでBigQueryデータセットを分析するには、まずKaggleアカウントにサインアップします。サインインの後に、上のバーの“Kernels をクリックし、さらに“New Kernel をクリックすると、新しいIDEセッションがスタートします。Kernelsにはスクリプトとノートブックのつのタイプがあります。次に示すのはノートブックオプションの例です。次に、Kernelsのエディタ環境で、右側にあるサイドバーの“BigQuery をクリックし、続いて“Link an account をクリックして、KaggleアカウントにBigQueryアカウントを紐づけます。アカウントがリンクされたら、BigQuery API Clientライブラリを使ってご自分のBigQueryデータセットにアクセスできます。Kaggleで公開されているAmes Housingデータセットを使用して、これを試してみましょう。このデータセットは米国アイオワ州エイムズに建つ住宅の ほとんど すべての側面を記述する種の説明変数から成り、その最終販売価格も含まれています。このデータから情報を得るためにクエリを作ってみましょう。このデータセットにはどのような家のタイプが含まれているのでしょうか。また、セントラル空調を設置している家、設置していない家はどれくらいあるのでしょうか。クエリは次のとおりです。エイムズで最も一般的な家のタイプは平屋建てで、タイプに関係なく、ほとんどの家にセントラル空調が設置されていることがすぐにわかります。Kaggleには、このような形で探索できる公開データセットがほかにもたくさんあります。SQLクエリを使った機械学習モデルの構築データ分析のほか、BigQuery MLではSQLクエリを使って機械学習モデルを作成、評価できます。機械学習フレームワークやプログラミング言語に関する詳細な知識がなくても、データサイエンティストであれば、わずかなクエリだけで回帰モデルの構築や評価を行えるのです。ここでは、エイムズにある不動産の最終販売価格を予想する線形モデルを作ってみましょう。このモデルでは、居住スペースの広さ、築年数、条件全般、品質全般を入力としてトレーニングを行います。モデルのコードは次のようになります。 model       CREATE OR REPLACE MODEL   my example housing dataset ameshousing linearmodel  OPTIONS model type  linear reg   ls init learn rate    l reg   max iterations   AS SELECT  IFNULL SalePrice    AS label  IFNULL GrLivArea    AS LivingAreaSize  YearBuilt   OverallCond   OverallQual FROM   my example housing dataset ameshousing train     このように、つのクエリだけで、Kernelsの中にSQLベースの機械学習モデルを作りました。Kernelsを使用すれば、分析のためにもっと高度なクエリを作成したり、モデルを最適化して性能を高めたりすることができます。分析が完了したら、Kaggleコミュニティや、より広範なインターネットユーザーに向けてKernelsノートブックを公開することも可能です。トレーニング統計情報の取得やモデルの評価に関するワークフローの続きの部分については、『Tutorial   How to use BigQuery in Kaggle Kernels』をご覧ください。このチュートリアルはKernelsノートブックとして公開されています。モデルのトレーニングや評価を深く掘り下げる『Getting started with BigQuery ML』もあります。BigQueryとKaggle Kernelsの統合の詳細はKaggleのドキュメントをご覧ください。BigQueryを使ってSQL言語の基本を学べるKaggleのSQL micro courseも内容を一新しており、サインアップすることをお勧めします。この統合機能をぜひお試しください。  By Jessica Li  Product Manager  Kaggle and Jordan Tigani  Director  BigQuery  Google Cloud",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/3353125776558607337/comments/default",
      "updated": "2019-07-05 00:00:00"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート  G Suiteのお客様事例とGoogle社内での活用方法",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。月日の放送では、G Suiteの活用事例をテーマにお送りしました。実際にご活用いただいているお客様でどのように活用しているかを詳しく紹介するとともに、Google社内でどのように使っているか、さらに、G Suiteに関するよくある質問にお答えしました。G Suiteのお客様活用事例コラボレーションとコミュニケーションのプラットフォームとして、G Suiteは多数の企業にご利用いただいています。番組では、そうしたお客様の中から社を取り上げ、G Suite導入の背景や目的、導入後の効果、今後の方針についてご紹介しました。株式会社TBSテレビ全社を挙げて本格的な「働き方改革」に取り組んでいる株式会社TBSテレビ。TBSグループ、約人を対象とした、ITを活用した働き方改革プロジェクトを推進する一貫として、G Suiteの導入を決めました。年月から検討を開始。年月より社内への先行導入がはじまり。順次ユーザー数を増やして、同年月に全社展開に至りました。ドライブ、Gmail、カレンダー、ドキュメント、スプレッドシート、スライド、ハングアウトなどその利用機能は多岐に渡ります。横浜ゴム株式会社横浜ゴム株式会社は、年にG Suiteを導入。主にGmailを中心に利用し、その他の機能は、他のさまざまなシステムと併用する形で利用していました。その後、昨今の働き方改革を推進する流れの中で、より本格的にG Suiteを活用していく決断をされました。メールだけでなく、共有ストレージなど、多くの機能をG Suiteに置き換えており、Dialogflowを利用した社内チャットボットの運用もはじまっています。メタウォーター株式会社メタウォーター株式会社では、オンプレミスですべてのITインフラを構築、運用していましたが、年月にG Suiteの全社利用を開始し、現在は約アカウントがメールだけでなく、ドライブ、カレンダー、Hangouts Meetなどさまざまなものを使用しています。さらに、全社で計台のJamboardも導入。Jamboard上でWeb会議をしながら情報を共有するといった従来にない働き方を実践しています。たとえば、設計部門では、Googleドライブに設計図を保存しておき、Jamboard上で設計図を開き打ち合わせをすることで、効率的かつ効果的な作業を行なっています。G Suiteのよくある質問と回答続いて、番組では、G Suiteに関する次のつの質問に答えます。G Suiteの導入を検討している方だけでなく、すでに導入済みで本格的な活用を計画されている方にとってもお役に立つ内容です。G Suiteの新規ユーザーはどの程度いますか G Suiteの特徴は  クラウドのメリットとは G Suiteを導入する場合は既存のMicrosoft Oﬃce形式のファイルをすべてGoogleドキュメント形式などに変換する必要がありますか Google Driveをファイルサーバーのように使えますか 現在使っているシステムからの移行方法を教えてください。既存システムと平行稼働が必要な場合に使えるサービスはありますか G Suiteを導入したいのですが、個人Gmailのアクセスを制限するにはどのようにすればいいですか 年月日放送 nbsp  G Suiteのお客様事例とGoogle社内での活用方法をご紹介番組で説明した資料はこちらで公開しています。    Cloud OnAir  G Suiteの活用事例紹介とQ  amp  A 年月日放送  from Google Cloud Platform   Japan Cloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/7490025252490021802/comments/default",
      "updated": "2019-07-02 23:02:18"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "大和証券株式会社の導入事例 Google Cloud Speech to Textで面倒なテキスト入力を自動化。その成功を起点に今後の積極的なクラウド活用を推進",
      "description": "日本大証券会社の社として、国内金融・証券業界に大きな貢献を果たしている大和証券。求められるセキュリティ基準の高さから、クラウド移行が難しいとされる金融・証券業界ですが、大和証券はそこに向けた最初の一歩を踏み出しました。それが、Google Cloud Speech to Textを活用した音声によるテキスト入力の実現。その背景と、今後も見据えた狙いについて、同社システム企画部の皆さんにお話をお伺いしました。利用しているGoogle Cloud PlatformサービスGoogle Cloud Speech to Text、Cloud Translation写真左からシステム企画部兼大和証券グループ本社システム企画部主任川口哲平氏システム企画部ITソリューション課長兼大和証券グループ本社システム企画部ITソリューション課長、副部長武部啓造氏システム企画部兼大和証券グループ本社システム企画部主任鈴木宏美氏大和証券株式会社年に設立された「藤本ビルブローカー」を源流とする老舗証券会社。大和証券のほか、大和証券投資信託委託や大和総研、大和企業投資などを傘下に擁する大和証券グループ本社の基幹企業。グループとして掲げる企業理念は「信頼の構築」「人材の重視」「社会への貢献」「健全な利益の確保」。現在の従業員数は名 年月時点 。Google Cloud Speech to Textなら金融・証券業界独自の用語にもスムーズに対応「私が所属している先端IT戦略部 現在は組織改編によって「システム企画部」に は、先端テクノロジーを駆使して、業務の効率化を図る取り組みを企画する部署。今回お話しするGoogle Cloud Speech to Text Speech to Text は、従業員のテキスト入力をサポートするために導入を決定しました。」 武部さん 全国か所の拠点で、約名の従業員が働く大和証券では、取引情報、顧客情報の共有化のため、商談の記録をCRM Customer Relationship Management システムに細かく入力するという業務があります。しかし、その入力内容は個人差が大きく、特にキーボードタイピングを苦手とする従業員が入力に手間がかかりすぎることから、必要最低限の情報しか入力しなかったり、逆に慣れないキーボードで丁寧に入力しようとした結果、勤務時間が増加してしまう問題が発生していたといいます。「実はその少し前に、お客様からの電話内容を録音して音声認識技術でテキスト化するという取り組みをおこなっていたのですが、これをその問題の解決に使えるのではないかと考えました。ただ、その機能が社内で実際にどれくらい使われるかはまったくの未知数。そこで今回は、使った分だけの従量課金でコストを抑えられるクラウド型の音声認識エンジンを利用することにしました。」 武部さん そうして、さまざまな音声認識ソリューションを評価していくなかで、最終的に nbsp Speech to Textにいきついたのは、認識精度が高かったから。金融・証券業界では、「取引残高報告書」を「取残 とりざん 」と略すなど、一般に使われない独自の用語が多数存在していますが、Speech to Textはわずかな学習と独自辞書の追加で、さまざまな専門用語の認識をマスター。前後の文脈をもとに正しくテキスト化できたといいます。「方向性の確定後、実際に開発をおこなったのは大和証券グループのシステム開発会社である大和総研なのですが、名のエンジニアで約か月かけて、既存のCRMシステムにSpeech to Textを駆使したテキスト入力機能を組み込むことができました。年月の連休明けから各拠点に展開し、利用を開始しています。」 武部さん CRMシステムに組み込んだSpeech to Textを活用したテキスト入力機能「導入後の効果の検証はこれからなのですが、現場からは、これまでキーボードタイピングが苦手で入力が億劫だったという方から、より細かく商談内容を記録できるようになったという声が届いています。今後は、そうして情報量の高まった商談記録が実際のビジネスに活用されていくことに期待しています。」 鈴木さん 「なお、コストに関しては上限を設定しているほか、マイクをオンにしたまま放置してしまって無駄な利用料金が発生してしまわないよう、無音状態が続くと自動的に切れる設定にしています。」 武部さん 将来的にはクラウドでグループ各社、各拠点の情報共有を進めていきたい独特な金融・証券用語を正確にテキスト化できるSpeech to Textの導入によって、当初の課題をクリアした大和証券ですが、その成功から、新たな可能性が生じ、次なる取り組みが生まれていったと言います。「思った以上に精度が高かったため、CRMシステムへの入力だけでなく、広範な用途に使えるよう社内ポータルに音声認識システムを配置し、開放しました。ボタンを押して、マイクに向かって話すとその内容がテキストデータとして表示されます。それをメールやドキュメントなどにコピー ペーストして利用してもらうイメージです。また、聴覚障害のあるスタッフが社内研修用のビデオを見る際に、自動的に字幕が付くような仕組みも作りました。これまでは別途、字幕付きビデオを作っていたのですが、Speech to Textを活用すれば、動画の音声をリアルタイムにテキスト化し、画面上に表示することができます。精度面でもまったく問題ありませんし、リアルタイムの情報共有の実現と同時に、結果的にはその手間と費用の大幅削減にもつながりました。」 武部さん なお、武部さん曰く、Speech to Textの導入には、これから拡大していくクラウド活用を見据えたテストケースという意味合いもあったそうです。情報セキュリティがとりわけ厳しい金融・証券業界では、外部にデータを置くことがタブーとされています。そこで、まずはデータを外部サーバーに保存することなく利用できるAPIでクラウドサービスを使い始めることにしたのです。そして、現在はSpeech to Text以外の機能にも利用を拡大。具体的にはCloud Translationを社内の翻訳業務に導入しました。こちらも社内ポータルに「大和翻訳センター」という名称で配置し、誰でも使えるようにしています。「大和証券では、機密保持の観点から外部の翻訳サービスの利用も禁止されているため、これまで英語で書かれた契約書やレポートの翻訳が大きな負担になっていました。しかし、Cloud Translationなら、Speech to Text同様、社内システム上で問題なく利用できます。これまでも多くの社員にとって有用なレポートなどは、本部で翻訳し配布していたのですが、大和翻訳センター設置後は、コストや工数をかけずに、それぞれの社員が個別に必要な情報を素早く翻訳できるようになりました。」 川口さん 現在はこうしたAPIレベルでの活用に留まっている大和証券のGCP活用ですが、今後は、段階的にその活用を深めていきたいと武部さんは言います。「大和証券グループの大きな課題として、各社、各拠点が閉じたネットワークで業務をおこなっており、情報の共有が阻害されているというものがあります。やむを得ないことではあったのですが、今後はそうもいきません。セキュリティの問題をクリアにしつつ、クラウドへのデータ保存・共有などを実現していきたいですね。たとえば、現在は社内の地図データにクリッピングしている顧客情報をGoogle Maps Platform に移行できたらルート検索などの機能を組み合わせて使えるようになり、さらに業務効率が改善するのではないかと思っています。また、今後、AI技術の導入によって、大和証券グループが進めている働き方改革がさらに推進されていくことにも期待しています。」 武部さん 「大和翻訳センターの機能改善要望で最も多いのが、PDFや画像などに記載された英文 画像 の翻訳・テキスト化。今後、クラウド対応を進めていくなかで実現していきたいですね。」 鈴木さん 「また、検索機能もさらに活用していきたいですね。現在は拠点ごとに情報が散乱しており、その鮮度もまちまちです。誰もが最新のデータを簡単かつ即座に入手できる環境の構築が急務だと考えています。また、アクセスログをきちんと解析して、利便性の向上にも役立てていきたいです。」 川口さん その他の導入事例はこちらをご覧ください。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/2535477232641251174/comments/default",
      "updated": "2019-06-27 00:00:04"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート お客様事例紹介アサヒグループのデータとGCPの活用",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。月日の放送では、Google Cloud Platformを実際にご活用いただいているアサヒプロマネジメント株式会社にご出演いただき、データ活用やデータ分析の取り組みについて伺いました。データ活用の取り組みアサヒグループにおいて、財務・人事・総務など、グループ全体の管理の請け負うアサヒプロマネジメント。グローバル化が進み、事業規模も急激に拡大する中で、ITを含むさまざまな業務に関してグループの最適な戦略・施策を立案、実行する会社です。このアサヒプロマネジメント株式会社では、事業拡大を支えるためにどのようなデータを扱っているのか、データを何のために使っているのか、データの活用によってどのようなことが改善されたのか、これらの点について、同社の清水博さんにお話いただきました。清水さん曰く、社内に蓄積されたさまざまなデータ たとえばPOS販売実績データ だけでなく、メーカーや卸業者が持つ市場データや位置情報を組み合わせて分析し、より適切な戦略立案を進める上で、マーチャンダイジング MD 業務の革新がポイントになるそうです。アサヒプロマネジメントでは、MDの業務生産性向上を目指し、クラウドを利用した新システムを構築しました。その結果、各種データ・資料を一元管理、集計することで年間でMD一人あたりの作業時間を 削減することができたそうです。この結果を受けて、MDの業務内容を単純なデータ分析から、提案・検証および課題解決業務へとシフトさせることができたわけです。GCPの活用オンプレミスでの運用がまだまだ多いというアサヒプロマネジメントですが、クラウド化への方針転換は年ほど前に遡るそうです。IT部門として、グループ内のITシステムの品質を維持し、安定運用を継続することが第一と捉え、その実現の障害となることをいかに取り除くかを熟慮した結果、フルマネージドサービスの活用に至ったと、清水さんは語ります。しかし、一方で、クラウドへの移行に十分な知見・経験が社内の既存の開発体制だけでは不十分であり、移行には時間がかかると判断。クラウドに精通した外部リソースの活用で、クラウドへの移行を迅速に推進することができたそうです。フルマネージドなGCPを活用した新システムのもとで大きな効果を実証できた今、アサヒプロマネジメントでは、ますます変化し、多様化する消費者のニーズに対応するため、Systems of Engagement SoE とSystems of Records SoR を上手に連携したシステムの実現を目指していくとのことです。年月日放送 nbsp お客様事例紹介アサヒグループのデータとGCPの活用番組で説明した資料はこちらで公開しています。    Cloud OnAir お客様事例紹介アサヒグループのデータとGCPの活用年月日放送  from Google Cloud Platform   Japan Cloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/4392188941760739481/comments/default",
      "updated": "2019-06-21 00:16:55"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "BigQuery GISによる天文データのクエリ",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。多くの組織は、アナリティクスや販売数、業績評価指標などのデータで満たされた大規模なデータウェアハウスを運用しています。ただ、データはこうしたものに限られるわけではなく、たとえば星でいっぱいの夜空というような自然界も膨大なデータセットの供給源です。BigQuery GISは、もともとは地球上の地理空間データを利用するユーザーのニーズに応えるために設計されたものですが、その球面座標系と組み込みの変換関数は、球面座標を使用するもうつの分野、天文学にも非常に適しています。BigQueryが天文データセットの分析に適した優れたプラットフォームである理由は次のとおりです。BigQueryはオンライン分析 OLAP 向けに作られており、非トランザクショナルな膨大なデータセットの処理に最適化されています。毎年という周期 プロジェクトによって異なります で発行される天文カタログの操作の大半は、これに当てはまります。BigQueryは、BigQuery GISを使った球面幾何学でのクエリをサポートします。天球上にオブジェクトを配置するには球面幾何学が必要です。BigQuery GISは専用のデータベースプラットフォームとほぼ同じスピードで天文データのクエリを実行できるほか、フルテーブルスキャンに使用すると、もっと高速になることがあります。探求すべき天文データが足りなくなることはありません。たとえば、カタログデータは望遠鏡による観測結果を巨大なテーブルにまとめたものです。大規模なカタログデータセットのなかには数十億もの天体のさまざまな観測値を集めたものがあり、一部のデータセットには数時間とか数年といったスパンの観測値も含まれています。人工衛星に搭載された望遠鏡であるWISEやGaiaは、高解像度のイメージデータを地球に送ってきます。地上に新しく設置される巨大望遠鏡のLSSTもまもなく稼働します。LSSTは、観測データのカタログを年間にわたって公開することが義務づけられています。この投稿記事の後半では、この種のカタログデータをBigQuery GISで処理する方法を探ります。天球座標系の基礎BigQueryによる天文カタログデータのクエリ例を示す前に、GISのニーズをサポートするためBigQuery GISに実装されている幅広い関数群について簡単に説明しましょう。少しの間、地球を見下ろしてみる地球は球体ですが、地球上の位置がGPS 全地球測位システム で簡単に得られる緯度と経度の次元情報で表現されることについて考えてみましょう。GPSは「緯度と経度」の座標であなたの位置を特定し、あなたが行きたい場所に案内してくれます。移動距離を知りたいとしましょう。高校で習った幾何学を覚えている方なら、ピタゴラスの定理で移動距離を計算できると考えるかもしれません。近距離であればうまくいきそうな気がしますが、距離が長くなると話がややこしくなります。移動距離を計算するには、出発点と目的地の緯度および経度をユークリッド平面上のデカルト座標に変換し、角度をメートルまたはマイルに変換する必要があります。ところが、ユークリッド距離は平面幾何学の問題ですが、あいにく地球の表面は平面ではなく球面なのでピタゴラスの定理は使えません。数学的な問題の大半は古代ギリシャやイスラムの数学者たちが年前に解決していますが、この変換が大変なことは今でも変わりありません。幸いなことに、BigQuery GISはこの種の計算を実行できるGoogleのS Geometryライブラリを利用しており、上記の複雑な幾何学計算を単純な標準SQLで実行できます。地球上の地点間の距離だけでなく、領域やポリゴンなどを使った複雑な計算にも対応可能です。BigQuery GISは非常に強力で、非常に使いやすいものなのです。再び天空に向かって地球表面の幾何学を理解したところで、もう一度星空を見上げてみましょう。BigQuery GISは、地球上の物体の移動とまったく同じ基本概念に沿って天体の移動を追跡します。つまり、夜空における星の位置を指定するときには、地球上の物体の位置を指定する場合と同じように、緯度経度のような座標を割り当てます。宇宙空間内での星の位置は、この座標によって正確に指し示すことができます。とはいえ、宇宙空間は球体ではありません。宇宙は、恒星、銀河、ブラックホール、惑星、クエーサー、パルサー、星雲などが、文字どおり完全かつ次元的に無限に広がっている空間です。それらは何光年も離れたところに散らばっており、地球上の自宅から最寄りのGoogleオフィスへの行き方をGPSで調べるのとはまったく異なります。面白いのはここからです。上で挙げた天体は非常に遠いところにあるため、私たち人間は、近くにある天体と遠くにある天体を簡単に区別することができません。夜空を見上げたときの星のように、地球を中心とする大きな黒い球体の表面で光る点のようなものです。この投稿では天文学の歴史に深入りするつもりはありませんが、科学史に詳しい方であれば、古代ギリシャの科学者、そして彼らの知的後継者が天体を記述するときのモデルはまさにこのようなものだったことを思い出されるでしょう。興味のある方にはThomas S Kuhnの著書『The Structure of Scientific Revolutions』 科学革命の構造 をお勧めします。天空に戻りましょう。夜空とすべての天体が、地球を中心とする大きな球と区別がつかないなら、天体に緯度経度を与えるという先ほどの提案も合理的に見えてきます。実際、天文学者はそうしています。彼らの座標は赤経 ra および赤緯 dec と呼ばれ、緯度や経度と同じように機能します。赤経は、時分秒を使った古い記法で表現されることがあります。例を見てみましょう。ベガ 映画『コンタクト』で有名な星 は、赤経h m s、赤緯 °′″にあります。幸いなことに、現代の天文学データは、地理学データと同様に度を単位とする小数表現で座標を格納しており、その最近の記法によれば、ベガの赤緯  ° は米国カンザスシティの緯度 北緯度 と同じです。これは、カンザスシティでは毎日回ずつベガが真上に見えるということです 夜であればの話ですが 。赤経が歴史的に時間記法で表現されてきたのは、明らかにこの自転に由来します。ご覧のように、天文学では空を見上げるのに対して、地理学では地面を見下ろすという違いはあるものの、天球座標系と地理座標系はよく似ています。そこで、地球上の物体の位置を示すために緯度経度を使用するのと同じように、天球上の天体の位置を示すためにraとdecの球面座標を使用することは、有効な方法として確立されています 緩やかな形で 。ただし、次の点に注意することが重要です。天球は、その主旨からして真球なので、地球が少し平べったいこと 楕円体になっていること によるGISシステムの補正機能は無効にする必要があります。好都合なことに、BigQuery GISはデフォルトで真球を使用しています。天球の極は、地球の地理学的な極と一致します。星の位置を示す座標 ra、dec は固定されたままです。天文学者が実行すべきクエリにはさまざまなものがありますが、LSSTの例がこちらにいくつか示されています。この投稿ではWISEデータでの例を示します。例とデータセットWISEデータセットには、天体のマルチエポック または時系列 データをまとめたテーブルが含まれています。なかでも面白い例が、食変光星である「こと座ベータ星」の光度曲線です。BigQuery AllWiseデータセットに含まれるこの光度曲線データにアクセスするためのクエリは次のとおりです。 SELECT wmpro ep  mjd  load id  frame idFROM  bigquery public data wise all sky data release mep wise WHERE source id mf  p ac  ORDER BY mjd ASC返されたデータをData Studioでプロットすると、次のようになります。ベンチマークとして使用できる数値を得るため、私たちは天文学者が興味を持つような現実的なクエリを試してみることにしました。そして、未加工のテーブルがロードされた状態での初期テスト実行後、次に示すつの重要な最適化を施しました。テーブルを分割しました。天体を三角測量するレベル HTM空間インデックスキーの整数値でデータをクラスタ化しました。ジオメトリ型のPOINTを使用して天体の位置を事前に計算しました。ST WITHINの代わりにST CONTAINSを使用して、空間のリージョンを制限し、データセットのサイズを小さくしました。最終的なクエリは次のとおりです。このクエリは、予想される天文データクエリのタイプに最も近いクエリのなかで代表的なものです。 CREATE TEMP FUNCTION ArcSecondDistance p GEOGRAPHY  p GEOGRAPHY  d FLOAT  AS  ST DISTANCE p p   lt  d    SELECT source id mf  pointFROM  bigquery public data wise all sky data release mep wise WHERE ArcSecondDistance point  ST GEOGPOINT        AND ST CONTAINS  ST GEOGFROMTEXT  Polygon                      point 上記つの最適化の組み合わせにより、 TBのテーブルのクエリに要する時間の中央値は秒から秒に下がりました。これによって、単一の天文データソースから関連情報を迅速に取得するよう最適化されたデータベースプラットフォームに匹敵する性能が、BigQueryからも得られるようになりました。さらに、フルテーブルスキャンに関しては、BigQueryは大きく優位に立つ可能性があります。何よりも素晴らしいのは、BigQuery GISと天文データセットがまだ開発初期の段階にあることです。私たちはもっと多くの天文カタログをBigQueryの一般公開データセットに追加していきます。WISEデータセットは、いくつか計画しているうちの最初のものに過ぎません。BigQuery GISの初心者は、こちらのドキュメントを読むことで、地球上の地理データを分析する方法を学べます。BigQueryを使って自然現象を記録することに興味のある方は、BigQuery GISを使用したハリケーン進路のプロットに関するこちらの優れたチュートリアルをご覧ください。ビジネスユースで地上のGISデータのアナリティクスを実行する方法については、ニューヨーク市でのシティバイクの移動データに関するチュートリアルが役に立ちます。地理空間 または天文 データを使用するにあたって何か発見がありましたら、ぜひ私たちにお知らせください。  By Ross Thomson  Cloud Solutions Architect",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/4539037808440784143/comments/default",
      "updated": "2019-06-14 02:00:11"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "署名付きURLを活用してCloud Storageに画像ファイルを直接アップロードするアーキテクチャを設計する",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。昨今のクラウドの普及によって、クラウド上にデプロイしたアプリケーションをエンドユーザーに広く提供することが一般的になってきました。それと同時に、不特定多数のエンドユーザーがクラウド上のリソースに直接アクセスすることを念頭に置くことによって、柔軟にクラウドの利点を活かしたシステムアーキテクチャを設計できるようになります。本稿では、主にエンドユーザーに対してコンテンツを配信するサービスを提供している方に向けて、クラウド上のリソースを理解し、その強みを最大限活用することによって、従来の課題をスマートに解決し、サーバの開発や運用にかかるコストを削減する方法を、具体的なアーキテクチャの設計を通してご紹介します。全体を通して、エンドユーザーが署名付きURLを使って直接Cloud Storageに画像ファイルをアップロードし、そのファイルを配信するシステムをGoogle Cloud Platform   GCP  上に構築することを目指します。本稿でご紹介するアーキテクチャのように、エンドユーザーからの画像ファイルのアップロードを受け付けるサーバを、クラウドを活用せず、大規模に開発・運用することは非常に困難な場合があります。考慮しなければならないものとして、例えばファイルのアップロードを担当するサーバのプロセスへのリクエストを一度キューに入れてフロー制御をしたり、リクエストの過負荷によるシステムダウンを防ぐ必要があります。さらに、関連するサーバの有限のリソース  RAMなど に対して、適切な制限を設定する必要もあるでしょう。加えて、非常に大きなファイルをサーバにアップロードできるようにするためには、サーバの開発と運用にさらに大きなコストが発生する可能性があります。また、アップロードを担うサーバを何百万ものユーザーが使用する場合、そのスケーラビリティやユーザーエクスペリエンスを確保するために多くの作業が必要になります。このような課題に対応するために、Cloud Storageを画像ファイルのアップロードサーバとして活用するシステムアーキテクチャを提案します。次項から実際にその設計方法を見ていきましょう。なお、本稿で使用するソースコードはすべてGitHub上で管理しています。画像アップロード機能の構築をはじめましょうこのアーキテクチャに必要なGCPコンポーネントを掘り下げる前に、まずは要件を定義しましょう。オペレーションコストを削減するために、マネージドサービスを可能な限り使用する。認証されたユーザのみがファイルをアップロードできるようにする。アップロードされたコンテンツに対して適切にバリデーションを行う。上記の要件を満たすために、次のアーキテクチャを提案します。このアーキテクチャの主な特徴として、ファイルのアップロードからその配信に至るまで、全てのコンポーネントをサーバーレスに使用可能なアプリケーションランタイム、もしくはマネージドサービスによってのみ構成している点が挙げられます。これは一つ目の要件を満たすための設計指針に基づいています。次に、各ステップについて詳しく解説します。App Engineがユーザーからのリクエストを受け取ったら、Cloud Storageの特定のバケット・特定のオブジェクトキーに対してPUTリクエストを実行可能とするための署名付きURLを生成し、ユーザーに返却する。このとき、App Engine上では任意のアプリケーションロジックに基づいて、認証済みのユーザーに対してのみ、当該署名付きURLを払い出すようにする。署名付きURLを返却されたユーザーは、それを用いてCloud Storageの特定のバケット・特定のオブジェクトキーに対してファイルをアップロードする。Cloud Storageへのファイルのアップロードを終えると、Cloud StorageのfinalizeイベントをトリガーとしてCloud Functionsを起動する。Cloud Functionsではアップロードされたファイルをバリデーションする。Cloud Functions上で、アップロードされたファイルが適切な形式、かつ適切なサイズであることを確認したら、Cloud Vision API を用いて不適切なコンテンツをフィルタリングする。手順のとの検証が完了したら、アップロード先のバケットから配信用のバケットにファイルをコピーする。コピーが完了した画像はインターネットに公開される。次に、上記の手順を実現するための実装について解説します。署名つきURLをApp Engine Standard runtime上で生成するCloud Storageには、個々のエンドユーザーが特定のリソースに対し、特定のアクションを実行できるようにするための署名付きURLと呼ばれる機能があります。署名付きURLを使用すると、ファイルを安全にアップロードするために、特定のエンドユーザーにのみ有効な一時認証情報を生成することができます。これについて、Google Cloud公式のクライアントライブラリを使用すると、簡単にこの機能を実装することができます。本稿ではこの機能を用いて、特定のエンドユーザーのために署名付きURLを生成させるAPIサーバをApp Engine Standard runtime上に作成します。実装は公式のライブラリに頼るとしても、署名付きURLを生成する実際の流れについては理解しておくことが望ましいため、処理の流れを次に示します。任意のバイト列を署名するためのサービスアカウントを用意する。新たなサービスアカウントを作成するか、App Engineのデフォルトサービスアカウントなどの既存のサービスアカウントに必要なパーミッションを付与する。署名付きURLの生成に必要なシグネチャを生成する 本稿ではContent MDとCanonicalized Extension Headersを省略する 。ファイルのアップロードを受け入れるHTTP Verbには、PUTを選択する。Content Typeの値はアップロードされるファイルのMIME typeによって異なる。この値はエンドユーザーから送信されるAPIリクエストによって決定することとする。有効期限をUnixタイムでX Goog Expiresにセットする。APIリクエストを受け取った時点から未来の時間を設定する。本稿では分と設定する。Canonicalized Resourceとしてアップロード対象のバケットとオブジェクトキーを設定する。オブジェクトキーはUUIDなどを用いて、動的に生成することで、既存のオブジェクトキーとの重複を回避する。手順で用意したサービスアカウントの秘密鍵を用いて、手順で生成した文字列を署名する。各手順の詳細については、公式ドキュメントを参照してください。本稿では上記の手順の例として、Goを用いて実装します。 package mainimport    context   encoding base   fmt   log   net http   os   time   cloud google com go storage   github com google uuid   golang org x oauth google  iam  google golang org api iam v  var      iamService is a client for calling the signBlob API  iamService  iam Service    serviceAccountName represents Service Account Name     See more details   serviceAccountName string    serviceAccountID follows the below format      projects  s serviceAccounts  s  serviceAccountID string    uploadableBucket is the destination bucket     All users will upload files directly to this bucket by using generated Signed URL  uploadableBucket string func signHandler w http ResponseWriter  r  http Request       Accepts only POST method     Otherwise  this handler returns   if r Method     POST    w Header   Set  Allow    POST   http Error w   Only POST is supported   http StatusMethodNotAllowed  return   ct    r FormValue  content type   if ct         http Error w   content type must be set   http StatusBadRequest  return      Generates an object key for use in new Cloud Storage Object     It s not duplicate with any object keys because of UUID  key    uuid New   String   if ext    r FormValue  ext    ext         key    fmt Sprintf    s   ext       Generates a signed URL for use in the PUT request to GCS     Generated URL should be expired after  mins  url  err    storage SignedURL uploadableBucket  key   amp storage SignedURLOptions  GoogleAccessID  serviceAccountName  Method   PUT   Expires  time Now   Add   time Minute   ContentType  ct     To avoid management for private key  use SignBytes instead of PrivateKey     In this example  we are using the  iam serviceAccounts signBlob  API for signing bytes     If you hope to avoid API call for signing bytes every time     you can use self hosted private key and pass it in Privatekey  SignBytes  func b   byte     byte  error    resp  err    iamService Projects ServiceAccounts SignBlob  serviceAccountID   amp iam SignBlobRequest BytesToSign  base StdEncoding EncodeToString b      Context r Context    Do   if err    nil   return nil  err   return base StdEncoding DecodeString resp Signature        if err    nil   log Printf  sign  failed to sign  err    v n   err  http Error w   failed to sign by internal server error   http StatusInternalServerError  return   w WriteHeader http StatusOK  fmt Fprintln w  url  func main     cred  err    google DefaultClient context Background    iam CloudPlatformScope  if err    nil   log Fatal err    iamService  err   iam New cred  if err    nil   log Fatal err    uploadableBucket   os Getenv  UPLOADABLE BUCKET   serviceAccountName   os Getenv  SERVICE ACCOUNT   serviceAccountID   fmt Sprintf   projects  s serviceAccounts  s   os Getenv  GOOGLE CLOUD PROJECT    serviceAccountName    http HandleFunc   sign   signHandler  log Fatal http ListenAndServe fmt Sprintf    s   os Getenv  PORT     nil   コード内で言及しているように、署名付きURLを生成するための方法は大きく分けて二つあります。一つはサービスアカウントに紐付いた秘密鍵を使って署名を行う方法です。この方法は自分自身の手で秘密鍵を管理する必要があります。これはGoogle Compute EngineやGoogle Kubernetes Engineなどのフルマネージドでないランタイム上でAPIサーバを開発・提供するケースに適しています。二つ目の方法は、Cloud Identity and Access Managementが提供するService Account APIの中の一つであるserviceAccounts signBlobを用いる方法です。これを用いることによって、アプリケーションランタイムで秘密鍵を管理せずに署名をすることができます。本稿では、秘密鍵の管理を避けるために、この方法を採用しています。補足として、Cloud Storageで署名付きURLを生成するためにサービスアカウント付与すべきパーミッションを次に列挙します。storage buckets getStorage objects createStorage objects delete二つ目の方法に示したserviceAccounts signBlob APIを使用する場合には、 nbsp roles iam serviceAccountTokenCreatorロールを付与します。このロールにまとめられたパーミッション群は公式ドキュメントで確認することができます。署名付きURLを使ってファイルをアップロードするここまで説明した機能によって、ユーザーはApp Engine上で生成された署名付きURLを使ってCloud Storageにファイルを直接アップロードできるようになりました。ここでは、署名付きURLを介してPut Objectと呼ばれるAPIを呼び出すことで、実際にファイルをアップロードします。次に示すのは、モバイルアプリケーションやWebブラウザなどで実行される処理を想定したGoによるサンプルコードです。前出のApp Engine上で動かすことを想定したソースコードと対応するものになります。 package mainimport    bytes   fmt   io ioutil   log   net http   net url   strings  const signerUrl     lt APPENGINE URL gt  func getSignedURL target string  values url Values   string  error    resp  err    http PostForm target  values  if err    nil   return     err   defer resp Body Close   b  err    ioutil ReadAll resp Body  if err    nil   return     err   return strings TrimSpace string b    nil func main        Get signed url from the API server hosted on App Engine  u  err    getSignedURL signerUrl  url Values  content type     image png     if err    nil   log Fatal err    fmt Printf  Signed URL here   q n   u  b  err    ioutil ReadFile   path to sample png   if err    nil   log Fatal err       Generates  http Request to request with PUT method to the Signed URL  req  err    http NewRequest  PUT   u  bytes NewReader b   if err    nil   log Fatal err    req Header Add  Content Type    image png   client    new http Client  resp  err    client Do req  if err    nil   log Fatal err    fmt Println resp  Cloud StorageのBucket LockとObject Lifecycleを署名付きURLと組み合わせる前出のアーキテクチャ図からもわかるように本稿では、ファイルのアップロード先を担うバケット Uploadable Bucket と、ファイルの配信を担うバケット Distribution Bucket の二つのCloud Storageバケットを定義しています。しかしながら、これらのバケットをデフォルトのまま取り扱うと、大きく分けて二つの問題が発生します。一つ目に、アップロードされた全てのオブジェクト ファイル は、それにかかる検証を終えた後にDistribution Bucketにコピーされますが、何もしなければそれらのファイルはUploadable Bucketに存在し続けることになります。しかしそれらのオブジェクトが、それ以降の処理でいずれのコンポーネントからも参照されることはありません。したがって、コピーが完了したオブジェクトをUploadable Bucketに残し続けておくことには意味がありません。言い換えれば不要な費用が発生し続けるということになります。二つ目に、エンドユーザーは署名付きURLの期限内であれば、何度も特定のバケット・オブジェクトキーに対してファイルをアップロードできてしまうということです。言い換えれば、アップロードされたファイルをサーバ側のコンポーネントが参照する度に、その中身が書き換わっている可能性があるということになります。これにより、後述のCloud Functionsで動かすアプリケーションにおいて、リトライと共に冪等性を担保することが困難になります。用途によってはそのような仕様を持っていても構わないのですが、本稿ではよりシンプルな仕様を維持するために、これを問題と捉えて対策します。さて、これらの問題を解消するために、Object Lifecycle ManagementとRetention Policyという二つのCloud Storage固有の機能を活用します。最初に、ライフサイクルを定義することによって、不要なオブジェクトがUploadable Bucketに残り続けないようにします。これを実現するためには、バケットや署名付きURLの有効期限などに合わせてCloud Storage上に存在するオブジェクトのライフサイクルを定義します。本稿では次のように、オブジェクトがUploadable Bucketにアップロードされてから日が経過したら、それを不要と判断して削除するライフサイクルを定義します。    rule        action     type    Delete     condition     age         次に、ユーザーが有効期限内であったとしても何度もファイルをアップロードできないようにするため、保持ポリシーを定義します。バケットロックと保持ポリシーを使用して、あるバケットにおけるオブジェクトの保持期間を設定し、その期間中はオブジェクトの上書きや削除ができないようにします。この保持期間を署名付きURLの有効期限に定める値と一致させることで、署名付きURLを介してアップロードされたオブジェクトが、エンドユーザーによって上書きされることを防ぐことができます。なお、前述のライフサイクルの定義によって、オブジェクトは日が経過すると削除されるため、ライフサイクルとの競合が起こらないようにしてください。仮にライフサイクルとの競合が起きてしまうと、正常にオブジェクトを削除できないなどの問題が発生します。補足として、バージョン管理が有効となっているバケットでは、このバケットロックと保持ポリシーを有効化できない点に注意してください。上記の設定を有効化したバケットを作成するためには、次に示すコマンドを実行します。 REGION  REGION PROJECT ID  PROJECT ID UPLOADABLE BUCKET  UPLOADABLE BUCKET DISTRIBUTION BUCKET  DISTRIBUTION BUCKET LIFECYCLE POLICY FILE   path to lifecycle json   Creates the uploadable bucketgsutil mb  p  PROJECT ID  l  REGION   retention s gs    UPLOADABLE BUCKET  Creates the bucket for distributiongsutil mb  p  PROJECT ID  l  REGION gs    DISTRIBUTION BUCKET  Set lifecyle for the uploadable bucketgsutil lifecycle set  LIFECYCLE POLICY FILE gs    UPLOADABLE BUCKETファイルを検証し、コピーするこれまで、署名付きURLを生成してファイルを直接Cloud Storageにアップロードし、アップロードされたファイルを適切に管理する方法について解説してきました。しかし、アップロードされたファイルを実際に多くのエンドユーザーに配信する前に、アプリケーションの仕様に合わせてそのファイルの妥当性を検証する必要があります。ここでは、認証されたユーザーによってアップロードされたファイルの有効性・妥当性を検証する方法と共に、検証が済んだオブジェクトをDistribution Bucketにコピーする方法について解説します。まず、本稿ではこれを行うために検証・コピーを行う処理をCloud Functions上に実装します。Cloud Functionsを実行するためには、特定のCloud Storageのイベントをトリガーとして、予めデプロイしておいたCloud Functionsを呼び出します。対象となるCloud Storageのイベントには、オブジェクトの書き込み完了を意味するgoogle storage object finalizeイベントを用います。次に、どのようにバリデーションやそれに付随する処理を行うべきかを考える必要があります。これを順序立てて整理しましょう。アップロードされたオブジェクトと同一のオブジェクトキーを持つオブジェクトがDistribution Bucketに存在しないことを確認する。オブジェクトキーが存在する場合は、以降の処理を停止する。これはCloud Functionsがat least onceの実行を保証することを考慮し、同様の処理を不要に実行させないためのものである。Uploadable Bucket内のオブジェクトのメタデータから、Content Typeとファイルのサイズを取得する。サイズが規定サイズを超過するのものでないことを確認する。規定サイズはアプリケーションの仕様に基づいて決定する。超過している場合は以降の処理を停止する。本稿ではアップロードされたファイルをCloud Functionsのメモリ上に一度展開するため、Cloud FunctionsのRAM上限値を考慮して規定サイズを決定するとよい。アップロードされたオブジェクトを実際に取得し、手順で取得したContent Typeに基づいて、そのオブジェクトが適切なファイルフォーマットに準拠していることを確認する。署名付きURLの生成時に指定するContent Typeはあくまでリクエストに指定すべきContent Typeを規定するものであり、アップロードされたオブジェクトがそこに指定されたフォーマットに準拠している保証はない。オブジェクトが指定されたフォーマットに準拠していないと見做された場合は、以降の処理を停止する。Cloud VisionのSafe Search Annotationを用いてアップロードされた画像ファイル オブジェクト が不適切な表現を含むものでないことを検証する。暴力描写・アダルト表現・人種差別などの可能性があるものを検閲する。本稿ではlikelihoodがPOSSIBLE以上のものを該当すると判定し、以降の処理を停止する。全ての検証をパスしたら、アップロードされたオブジェクトをDistribution Bucketにコピーする。上記の手順を実装するために、本稿ではCloud Functions Go ランタイムを使用します。次にソースコードを示します。 package functionimport    context   errors   fmt   image   image gif   image jpeg   image png   log   cloud google com go storage  vision  cloud google com go vision apiv   golang org x xerrors  pb  google golang org genproto googleapis cloud vision v  type GCSEvent struct   Bucket string  json  bucket   Name string  json  name   var retryableError   xerrors New  upload  retryable error  func validate ctx context Context  obj  storage ObjectHandle  error   attrs  err    obj Attrs ctx  if err    nil   return xerrors Errorf  upload  failed to get object attributes  q    w   obj ObjectName    retryableError       You can enlarge maximum size up to MB by modifying this line   if attrs Size  gt       return fmt Errorf  upload  image file is too large  got    d   attrs Size       Validates obj and returns true if it conforms supported image formats  if err    validateMIMEType ctx  attrs  obj   err    nil   return err      Validates obj by calling Vision API  return validateByVisionAPI ctx  obj  func validateMIMEType ctx context Context  attrs  storage ObjectAttrs  obj  storage ObjectHandle  error   r  err    obj NewReader ctx  if err    nil   return xerrors Errorf  upload  failed to open new file  q    w   obj ObjectName    retryableError    defer r Close   if    err    func ct string   image Image  error    switch ct   case  image png   return png Decode r  case  image jpeg    image jpg   return jpeg Decode r  case  image gif   return gif Decode r  default  return nil  fmt Errorf  upload  unsupported MIME type  got    q   ct      attrs ContentType   err    nil   return err   return nil    validateByVisionAPI uses Safe Search Detection provided by Cloud Vision API    See more details  func validateByVisionAPI ctx context Context  obj  storage ObjectHandle  error   client  err    vision NewImageAnnotatorClient ctx  if err    nil   return xerrors Errorf   upload  failed to create a ImageAnnotator client  error    v    w   err  retryableError      ssa  err    client DetectSafeSearch  ctx  vision NewImageFromURI fmt Sprintf  gs    s  s   obj BucketName    obj ObjectName      nil    if err    nil   return xerrors Errorf   upload  failed to detect safe search  error    v    w   err  retryableError         Returns an unretryable error if there is any possibility of inappropriate image     Likelihood has been defined in the following      L L if ssa Adult  gt   pb Likelihood POSSIBLE    ssa Medical  gt   pb Likelihood POSSIBLE    ssa Violence  gt   pb Likelihood POSSIBLE    ssa Racy  gt   pb Likelihood POSSIBLE   return errors New  upload  exceeds the prescribed likelihood     return nil    distributionBucket is the distribution bucket    It s used for distributing all of passed files    TODO  This value MUST be updated before deploying this function const distributionBucket    DISTRIBUTION BUCKET    UplaodImage validates the object and copy it into the distribution bucket func UploadImage ctx context Context  e GCSEvent  error   client  err    storage NewClient ctx  if err    nil   return fmt Errorf  upload  failed to construct a client  error    v   err    defer client Close   dst    client Bucket distributionBucket  Object e Name     err   dst Attrs ctx     Avoid proceeding if the object has been copied to destination  if err    nil   log Printf  upload   s has already been copied to destination n   e Name  return nil      Return retryable error as there is a possibility that object does not temporarily exist  if err    storage ErrObjectNotExist   return err   src    client Bucket e Bucket  Object e Name  if err    validate ctx  src   err    nil   if xerrors Is err  retryableError    return err   log Println err  return nil      Returns an error if the copy operation failed     Will retry the same processing later  if    err    dst CopierFrom src  Run ctx   err    nil   return err   return nil 上記のソースコードにおけるUploadImageという関数をCloud Functionsにデプロイするために、次のコマンドを実行します。 gcloud functions deploy UploadImage   runtime go   trigger resource  UPLOADABLE BUCKET    trigger event google storage object finalize   retry  retryオプションを付けて実行することで、Cloud FunctionsやCloud Storageの一時的なエラーに備えることができます。一方でリトライを有効化する際には、それを考慮してコーディングをする必要がある点に注意してください。また前出のソースコードについても、Cloud Functionsがat least onceの実行を保証することを考えれば、Distribution Bucketへのコピーが一回以上行われる可能性を否定できないなど、冪等な実装になっているとは言えませんが、この場合は副作用はありません。ファイルをアップロードするここまでに設計し、実装してきたシステムに対して、実際にファイルをアップロードしてみましょう。テストは非常に簡単です。前述の署名付きURLをApp Engineにリクエストし、そのURLに対してファイルをアップロードするために示したソースコードを実行します。Cloud Functionsの実行が完了したら、Uploadable Bucket内のオブジェクトがDistribution Bucketにコピーされていることを確認します。Distribution Bucketへのアクセスを全体に対して公開していれば、コピーが完了した時点でそのオブジェクトはインターネットに公開されていることを意味します。なお、あなたが設計するシステムの要件によっては、Cloud Functionsの処理は非同期に実行されるため、配信の開始をPush通知などの方法でエンドユーザーに伝える必要があるかもしれません。もしくは、エンドユーザーからのリクエストを同期的に処理するシステムのデータベースの更新が必要となるかもしれません。マイクロサービス・アーキテクチャを採用しているならば、Cloud Functionsから元のアプリケーションにHTTPリクエストを送信しても良いでしょう。本稿で提案したシステムをカスタマイズして、あなたの目的に合わせたファイルアップロード機能を開発することができるはずです。Google Cloud Platformには、ここで紹介したコンポーネントや機能以外にも様々な選択肢があります。例えば、ファイルをアップロードする際の条件をより詳細に規定したい場合や、POST Objectを使いたい場合には、署名付きポリシードキュメントを署名付きURLの代わりに使用することができます。また、モバイルアプリやWebサービスの開発にFirebaseを使用している場合には、ここで紹介した署名付きURLの代わりにCloud Storage for Firebaseを使用することができます。本稿の内容があなたのアーキテクチャ設計の一助となれば幸いです。Google Cloudのエキスパートと繋がり、本稿で提案したアーキテクチャについてより深く学びたい方はGoogle Cloudのコンサルティングサービスを確認してください。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/9163583695948940171/comments/default",
      "updated": "2019-06-13 01:30:00"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "年月日 金  第回INEVITABLE ja nightを開催します",
      "description": "Google Cloudに代表されるクラウド技術の進化が引き起こすその先の世界を、機械学習、VR   AR、IoTなどの領域で活躍されているスタートアップの方々と一緒に議論するイベント「INEVITABLE ja night」。第回目となる今回は、「デベロッパーカンファレンスから読み解くテクノロジーの不可避な流れ」がテーマです。対談では、国内外のデベロッパーカンファレンスに数多く参加されている及川卓也さんをお迎えし、次々に登場する新しいテクノロジーがビジネスをどのように変えていくのか、その潮流について語っていただきます。また、Googleからは、この月、月に米国で開催されたCloud Next  やGoogle I O で発表されたテクノロジーやビジネス事例を中心にご紹介します。講演会後には恒例の交流会も行います。参加者様同士の交流はもちろん、日頃の業務の課題や悩みについても、ご相談 共有いただける良い機会となります。本テーマにご関心のある方々のご参加をお待ちしています。開催概要イベント名  INEVITABLE ja night   “インターネットの次にくるもの 第回デベロッパーカンファレンスから読み解くテクノロジーの不可避な流れ日程  年月日 金      開場 より 会場 グーグル合同会社定員  名ハッシュタグ   inevitablejaプログラムINEVITABLE対談スピーカー 及川卓也氏  Tably株式会社代表取締役Technology Enabler聞き手 小島英揮氏、Still Day One合同会社代表社員パラレルマーケター・エバンジェリストGoogleテクノロジーアップデート佐藤一憲グーグル合同会社デベロッパーアドボケイト福田潔グーグル・クラウド・ジャパン合同会社カスタマーエンジニア松田白朗 Hak グーグル合同会社デベロッパーアドボケイト申し込みサイト多数のご参加をお待ちしております。   INEVITABLE TVのご案内   INEVITABLE TVでは、イベントでは取り上げることができなかったこと、語り尽くせなかったことを中心に、「インターネットの次に来るもの」に関連する話題を深く掘り下げていきます。こちらもぜひご覧ください。PWA  Progressive Web Apps への不可避な流れ ゲスト 宇都宮佑亮 CXO視点で見る、エクスペリエンスとテクノロジーの関係 ゲスト 深津貴之氏 その他IoT活用最前線、ソラコムに聞く事例選 ゲスト 玉川憲氏 街×テクノロジーの不可避な流れ ゲスト 林直孝氏 エンターテック最前線 ゲスト 鈴木貴歩氏 VUIの普及と進化の方向性 ゲスト 岩佐琢磨氏 TPUが拓くAI活用の近未来 ゲスト 佐藤一憲 ARのこれまでの歩みと今後の展望 ゲスト 松田白朗 ",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/4999924482844740446/comments/default",
      "updated": "2019-06-10 00:41:05"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "「GCP認定資格チャレンジ デベロッパー編」キャンペーンをスタート",
      "description": "クラウドテクノロジーがITの主流となりつつある昨今、多くの企業にとって、クラウドのスキルを持つ技術者の確保が大きな課題となっています。一方で、オンプレミスなど従来からのテクニカルスキルを持つ技術者にとっては、クラウドテクノロジーを学ぶことが今後のキャリアに重要な価値をもたらします。こうした状況をふまえ、Google Cloudでは今年月に発表したように、クラウドスキルの向上支援を本年の重要項目として取り上げています。クラウドの成功に特に必要とされる人材の確保に直結するよう、認定資格のラインアップを増やすとともに、資格取得を支援するトレーニングの提供も推進しています。このたび、Google Cloud Japanでは、今年新たに提供を開始した「Google Cloud認定Professional Cloud Developer」の認定資格を、日本語にて提供開始いたしました。また、本認定資格をより多くの方に取得いただけるよう、本日より「GCP認定資格チャレンジ デベロッパー編」キャンペーンを開始いたします。本キャンペーンでは、認定資格取得に必要な知識を習得するための学習コースとして、Courseraが提供するオンデマンドトレーニングと、オンラインでGCPのハンズオンを行えるセルフペースラボを、それぞれか月間無料で提供いたします。キャンペーン期間は、月日 木 から月日 金 となりますので、お早めにお申込みください。キャンペーンの詳細およびご登録は、こちらよりご確認いただけます。私たちはGoogle Cloud Platformをより多くの皆さまにお使いいただけるよう、引き続き学習プログラムの拡張を全力で推進してまいります。皆さまもぜひご活用ください。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/885857524902193731/comments/default",
      "updated": "2019-06-06 06:00:06"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート オンプレミスからGCPへのデータの移行",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。月日の放送では、オンプレミスのデータをBigQueryに移行する際のポイントを解説します。オンプレミスにあるデータを効率的にBigQueryに持っていくためには、技術面、運用面、組織面などから最適なアーキテクチャを探っていく必要があります。番組では、オンプレミスにあるデータをBigQueryに持ってくるために必要な技術要素をつに取り上げ、よくあるケースとそれを実現するアーキテクチャについて解説します。アーキテクチャを選択するオンプレミスのデータをGoogle Cloud Storageに効率よく移行するには、ネットワーク、ツール、パイプライン管理のつの観点から、アーキテクチャを探っていくことが重要です。ネットワークオンプレミス環境とGCPを接続するネットワーク構成について紹介します。大別するとパブリック接続 インターネット経由 とプライベート接続があり、プライベート接続は、VPN、Partner Interconnect、Dedicated Interconnect に分かれます。インターネット経由でGoogle Cloud Storageを使う場合は、オンプレミス環境と nbsp GCP間はすべてHTTPSの暗号化通信となるため、十分なセキュリティを確保することができます。一方、セキュリティポリシー等により、VPNまたはInterconnectでの接続が必要となる場合は、VPC内のプロキシ経由で接続する事も可能です。さらに、DNSとルーティングの設定を追加することで通信経路全体をプライベートにすることができます。ツールデータ転送に使うツールです。オンプレ側で使うものと、GCP側で使うツールに大別されます。オンプレ側で使う代表的なものとして、オープンソースのログ転送ツールFluend があります。ストリーミングでログデータをGCP側に持っていく際に使います。GCP側では、Apache Beamのマネージドな実行エンジンであるCloud Dataflow、ノンコーディングでETL処理とパイプライン管理が可能なCloud Data Fusionがよく利用されます。オンプレ側でもGCP側でも使われるものが、Google Cloud Storageのコマンドラインツールであるgsutil、オープンソースのバッチ転送ツールであるembulk、GCPクライアントライブラリです。全体のデータパイプラインの管理GCPのサービスとして、Cloud ComposerとCloud Data Fusionがパイプライン管理をサポートします。Cloud ComposerはAmache Airflowを基に構築された、フルマネージドのワークフローオーケストレーションサービスです。クラウドとオンプレミスデータセンターにまたがるパイプラインの作成、スケジューリング、モニタリングを実現します。Cloud Composerパイプラインは、Pythonを使用して有向非巡回グラフ DAG として構成でき、ユーザーの経験を問わずに簡単にワークフローを作成したり、スケジュールを設定したりできます。タスクの実行を司るOperatorも充実していることも特徴です。なお、Airflow以外のパイプライン管理ツールとしては、DigdagやLuigiというものもあります。ユースケースとサンプルアーキテクチャよくあるケースを例に、どのようなアーキテクチャが考えられるかを紹介します。ここでは、次のつのユースケースを取り上げます。初めてBigQueryを使って自社データを分析するという、まずは、何ができるかを試してみたいというケースです。 に加えて、ネットワークをプライベート接続としたいケース。オンプレミス環境上のDBを定期的にアクセス、データを転送して、分析するケース。日々の売上データを分析するといった場合です。企業の重要情報を扱うため、DBへの接続方式に制限があったり、セキュリティポリシー上、プライベート接続のみという場合もあります。番組では、それぞれのケースにあった、GCPのサービスの選択と組み合わせ方、またそれらを利用する上での注意点を詳しく解説しています。年月日放送 nbsp オンプレミスにあるデータをGCPで分析する前に知っておきたいアーキテクチャ番組で説明した資料はこちらで公開しています。 オンプレミスにあるデータをGCPで分析する前に知っておきたいアーキテクチャ from Google Cloud Platform   JapanCloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/6822809054886642397/comments/default",
      "updated": "2019-06-06 12:33:02"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Google Cloud Next   in Tokyoセッション登録を開始",
      "description": "月日 水  月日 木 の日間にわたって開催するGoogle Cloud Next   in Tokyoまでか月をきりました。本日より、イベントサイトに各セッションのスピーカーや詳細スケジュールを公開し、セッション登録をしていただけるようになりました。月日 水 および月日 木 の午前中に開催する基調講演では、『かつてないクラウドを体験しよう』をテーマに、年もの間Googleで活動するウルスヘルツル テクニカルインフラストラクチャ部門シニアバイスプレジデント をはじめ、テクノロジーカンパニーとしてクラウドの活用を牽引する株式会社ディー・エヌ・エー代表取締役会長南場智子氏など多数のゲストスピーカーにご登壇いただく予定です。午後のブレイクアウトセッションでは、機械学習とAI、データ分析、DevOps・SRE、セキュリティやハイブリッドクラウドなど多岐にわたるカテゴリのセッションをご用意しています。ハンズオンラボやBootcampなど、約のセッションに、ぜひご参加ください。セッションページはこちらご自身の興味関心のあるテーマやレベルでフィルタリングして、選択が可能です。また、今年は担当部門に合わせたセッションパッケージもご用意しています。ぜひご活用ください。ビジネス部門ご担当者様向けおすすめセッションはこちら開発部門ご担当者様向けおすすめセッションはこちら情報システム部門ご担当者様向けおすすめセッションはこちらデータ部門ご担当者様向けおすすめセッションはこちら人気セッションは早期の満席が予想されますので、まずはイベント参加登録にお申込みいただき、ご希望のセッションへのご登録をお願いたします。イベントお申し込み amp セッション登録イベント概要イベント名  Google Cloud Next   in Tokyoウェブサイト 日程  年月日 火 ・月日 水 ・月日 木 時間 月日 火 Bootcampコース 有料      予定 DevDay     予定 月日 水 ・月日 木 開場   予定 基調講演     予定 セッション    予定 会場  ザ・プリンスパークタワー東京および東京プリンスホテル注意事項※本カンファレンスは、Google Cloudの製品・サービス導入を検討されているエンドユーザー企業、団体、教育機関、政府自治体向けのイベントです。※十分な座席数をご用意しておりますが、定員を超えた場合、エンドユーザー企業様優先の抽選とさせていただきます。※イベントの当日登録は承ることが出来ません。前もってご登録くださいますよう、お願いいたします。※ご不明な点は、FAQをご確認ください。お問い合わせ先Google Cloud Next Tokyo運営事務局gc nexttokyo info google com",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/705926904245333128/comments/default",
      "updated": "2019-06-05 02:30:41"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "ブラウザ翻訳システムをCloud Translation APIで構築翻訳の精度やセキュリティ、専門用語に対応できる柔軟性で採用を決定",
      "description": "世界中の顧客に、「夢」や「感動」に溢れるサービスで、安全かつ高品質な空の旅を提供する全日本空輸株式会社 以下、ANA 。同社CS推進部では、より一層の顧客満足度の向上を目的に、「ご意見・ご要望デスク」を運営しています。今回、グローバル対応の一環として開発したブラウザ翻訳システムのプラットフォームにGCPを採用。開発の中心となったANAのキーマン名に話を伺いました。利用しているGoogle Cloud PlatformサービスGoogle App Engine、Cloud Datastore、Google Cloud Storage、Cloud Translation API、Cloud Natural Language API写真左からCEマネジメント室CS推進部CS教育推進チーム魚住晃之氏CEマネジメント室CE戦略部顧客データ分析チーム博士 情報学 マネジャー長尾若氏CEマネジメント室CS推進部業務チーム姫井淳子氏全日本空輸株式会社年に、安全運航を第一に航空輸送事業を開始。年には、国際線就航周年を迎えています。現在、定期航空運送事業、不定期航空運送事業、航空機使用事業、その他附帯事業を展開。「安心と信頼を基礎に、世界をつなぐ心の翼で夢にあふれる未来に貢献します」という企業理念に基づいて、お客様満足と価値創造で世界のリーディングエアライングループを目指す取り組みを推進。航空運送事業を中核とする世界トップクラスのエアライングループとして成長を遂げ、年間旅客数は万人を突破しています。GCPの採用で「ご意見・ご要望デスク」の業務効率化を推進一貫した高品質なサービスの提供や、グローバルカスタマーのニーズを踏まえたサービス改善を継続的におこなっていることが高く評価されているANA。英国ロンドンを本拠地とする航空業界の格付け会社であるSKYTRAX社の「ワールド・エアライン・スター・レーティング」において、世界最高評価の「スター」を獲得しています。サービス品質向上の取り組みの一環としてANAでは、「ご意見・ご要望デスク」を運営しています。ご意見・ご要望デスクに寄せられるお客様の声は、国内はもちろん、海外からも届きます。たとえば、海外の空港に関する改善要望があった場合、その国の空港に情報共有する必要があります。このとき、まずは日本語でレポートを作成しますが、米国の空港の場合、英語でも共有が必要です。長尾さんは、「翻訳ができるスタッフに集中して負荷がかかることや、翻訳会社に依頼する場合、時間とコストがかかるため、情報をタイムリーに現地スタッフに共有できないことが課題でした」と話します。こうした背景から、ブラウザ翻訳システム「WACA Words Advanced Communication Approach 」を構築することを決定。開発プラットフォームとして、Google Cloud Platform GCP が採用されています。GCPの採用に至った評価ポイントは、翻訳の精度とセキュリティ、航空業界特有の専門用語にも対応できる柔軟性でした。更なる業務改革に向けCloud Speech to TextなどGCPの充実したサービスにも期待WACAの開発は、年月より検討を開始し、月にGCPの採用を決定。システム開発、運用、改善のサイクルを何度か繰り返し、月より本格的な運用を開始しています。システム構成としては、Cloud DatastoreとCloud Storage上の単語辞書に登録された専門用語に基づいて、Cloud Translation APIを介してブラウザ上で翻訳ができる仕組みを、Google App Engine GAE 上に構築しています。翻訳精度を上げるために、最初にCloud Natural Language APIを通して、登録した辞書を利用し形態素解析をおこない、それからCloud Translation APIを通し翻訳をしています。また、Chromeの拡張機能を利用することで右クリックにより翻訳ができる操作性を実現しています。魚住さんは、「Chrome上で翻訳したいテキストを選択し、右クリックをすると翻訳メニューが開くので、現在、登録されているか国語から、翻訳したい言語を選べば翻訳ができます。翻訳が自動化されたので、手作業による翻訳の負荷が削減されました」と話します。一般的なウェブサイトの翻訳ツールでは、翻訳したい文書をコピーして、翻訳窓にペーストし、翻訳ボタンをクリックするというステップが必要でした。WACAでは、右クリックのみのステップで翻訳できるので、作業工数の削減にもつながります。また、WACAは社内システムなので、情報が外部に漏れず、セキュリティを意識することなく利用できます。翻訳精度の向上で、点だけ課題となったのは、航空会社には略語などの専門用語が多く、それに対応することでした。たとえば、それらの専門用語を翻訳すると不自然な表現になり、文章として成立しません。GCPでは、専門用語を登録するための単語辞書の仕組みの実現など、カスタマイズにも柔軟に対応できることが高く評価されています。辞書登録について姫井さんは、次のように話します。「辞書登録は、ご意見・ご要望デスクのスタッフから登録したい用語を募り、一括してcsvでCloud Datastoreに登録しています。辞書登録をすることで、より精度の高い翻訳が可能になりました。辞書登録をすることで、翻訳精度が格段に上がり問い合わせ対応業務の効率化につながっています。翻訳精度の向上とともに、ご意見・ご要望デスクのスタッフにも、WACAはかなり浸透してきました。」今後は、人工知能 AI や画像認識などの活用による翻訳精度のさらなる向上や、Dialogflowによる問い合わせに迅速かつ簡単に対応できるチャットボットの構築など、より一層の業務効率化を実現できる仕組みを検討していく予定です。全日本空輸株式会社の導入事例PDFはこちらをご覧ください。その他の導入事例はこちらをご覧ください。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/1857411930612350165/comments/default",
      "updated": "2019-06-02 14:00:36"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "KubernetesアプリケーションをGCP Marketplaceで提供することの意義",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。コンテナを介したアプリケーションのモダナイゼーションは、デジタルトランスフォーメーションで特に注目を集めているポイントのつです。Forrester Researchが最近実施した調査 Googleが委託 によると、企業におけるアプリケーション開発リーダーの  は、カスタムアプリケーション構築の生産性とリリースサイクルを上げるために、利用しているアプリケーションのコンテナ化を進めています。商用ソフトウェアとオープンソースのどちらを使用する場合でも、企業アプリケーションのモダナイゼーションを促進するための優れた方法は、Kubernetesアプリケーション どのKubernetes環境でも実行できてデプロイしやすい、エンタープライズ対応のコンテナ化アプリケーション を使用することです。私たちはGoogle Cloud Next  において、そうしたKubernetesアプリケーションをGCP Marketplaceで一般提供することを発表しました。この投稿では、その詳細についてご紹介します。Kubernetesアプリケーション お客様の声に応えた設計コンテナとKubernetesを用いたアプリケーション開発は、混乱を招くことがあります。実際、私たちのもとには、「Kubernetesアプリケーションは、もっと簡単に使用できて生産性も向上するようなものにならないのか」という問い合わせが数多く寄せられています。そこで私たちは、あらゆる開発者のエクスペリエンスをもっと高めるべく、アプリケーションとそのコンポーネントの表示や管理を容易にする「アプリケーションリソース」をGKEに導入しました。このメカニズムは、Helm Chartsなどの一般的なパッケージング形式をサポートし、デプロイが容易で、かつ今日のGCP Marketplaceで配布可能なエンタープライズ対応の製品を開発できるようにします。Kubernetesアプリケーションは単なるコンテナイメージではありません。アプリケーションリソースの標準化により、Kubernetesアプリケーションは、個々のコアやワークロードリソースとしてだけでなく、つのまとまったユニットとして扱えるようになりました。さらには、このデプロイテンプレートを認識するデプロイツールもサポートしています。現在では、Kubernetesをベースとするアプリケーションのデバッグ、モニタリング、デプロイにおける複雑さはかなり緩和されています。アプリケーション環境のモダナイゼーションアプリケーション開発がつの環境だけに限定されることはありません。Kubernetesアプリケーションも開発チームと同じくらい動的に構築されています。さほど複雑ではないプロジェクトの場合はGKE Standardを、大企業レベルのコンテナオーケストレーションや金融データ対応のSLAが必要なプロジェクトの場合はGKE Advancedを選ぶことができます。大企業ではクラウドへの移行前にオンプレミスでアプリケーションをコンテナ化することが一般的になっており、Anthos上で実行されるKubernetesアプリケーションは、このシナリオをサポートします。Anthosにより、環境の違いを越えて動作する新しいハイブリッドアプリケーションの構築や管理が可能になりました。GCP Marketplaceには、AnthosをサポートするKubernetesアプリケーションの第一陣が登場し、その数は増えつつあります。クラウドネイティブなアプリケーションをAnthosベースのKubernetesアプリケーションで構築する方法については、Next での私たちのプレゼンテーションをご覧ください。また、IstioとStackdriverをサポートする、完全なAnthosエクスペリエンスのKubernetesアプリケーションも増えてきています。オープンソースと商用ソフトウェアの強力なエコシステム企業アプリケーションの開発には堅牢なエコシステムが必要不可欠です。Kubernetesアプリケーションの種類は、セキュリティ、データベース、開発ツール、モニタリングシステムなど多岐にわたります。また、PostgreSQL、RabbitMQ、Airflowといった人気の高いオープンソースソフトウェアのGoogleパッケージ版もあります。さらに、私たちはパブリッククラウドとしては初めて、Aerospike、Aqua Security、Galactic Fog、Kasten、ManagedKube、Portworx、Robin ioの商用Kubernetesアプリケーションを提供します。「クラウドネイティブな最新アプリケーションのデリバリを、使いやすいCI CDソリューションによって自動化したいお客様との間に接点ができることは、私たちにとってうれしいことです。JenkinsベースのCI CD KubernetesアプリケーションであるCloudBees Coreを使用すれば、GCP Marketplaceからオンプレミスにもクラウドにもデプロイできます。」ーRob Davies氏、CloudBeesの技術担当VPセキュリティ強化の手段、企業が購入しやすい場としてのGCP MarketplaceKubernetesアプリケーションの購入を希望する企業を支援するため、私たちは、パートナーとお客様との間の特別な料金設定や、年間サブスクリプション 現時点ではベータ 、GCP MarketplaceのソリューションとGCPサービスの一括請求 GCP Marketplaceで販売している他のプロダクトと同様のもの をサポートします。前述したForresterの調査によると、アプリケーション開発リーダーの  は、コンテナ化アプリケーションをベンダーから直接入手するよりもマーケットプレイスから購入するほうを選ぶ、と回答しています。クラウドソフトウェア開発ツールのサポートのためにマーケットプレイスを利用するメリットについては、彼らの  がセキュリティの強化を挙げています。GCP Marketplaceで提供されているソリューションは、Kubernetesアプリケーションを含めて、すべて提供前に私たちがセキュリティ上の脆弱性をチェックしています。また、回答者の  が購入前の試験運用を望んでいることから、Kubernetesアプリケーションについても、私たちは仮想マシンやSaaSプロダクトと同様に無料で試用できるようにします。今すぐ始めましょうスタート地点がオンプレミスであれ、クラウドであれ、アプリケーションのモダナイゼーションを支援できることを、私たちはうれしく思います。Kubernetesアプリケーションは、オープンソースおよび商用製品をコンテナ環境に組み込むためのシンプルで統一された方法を提供します。今すぐ使ってみてください。また、お気に入りのパートナー企業にも、GCP MarketplaceでKubernetesアプリケーションを提供するよう勧めてください。加えて、私たちはGCP MarketplaceをGKE Connect 現時点ではアルファ と統合し、オンプレミスや他社クラウドで実行されているGKE以外のKubernetesクラスタとGCPプロジェクトとを接続できるようにしました。これにより、接続されたすべてのクラスタでKubernetesワークロードを表示したり、ログインして操作したりすることができます。GCPパートナーがアプリケーションを一度ビルドすれば、お客様はそれをオンプレミスであれクラウドであれ、好きな環境で実行できます。GKE Connectのアルファプログラムに参加を希望される方は、こちらからサインアップをお願いします。私たちとともにコンテナ化アプリケーション開発とAnthosを支えるパートナーのエコシステムにぜひご参加ください。Kubernetesアプリケーションは、あなたのソリューションをGCP Marketplaceで販売するというユニークな機会を提供し、オンプレミスやGCP、その他のクラウドにおいて顧客がデプロイすることを可能にします。詳細はこちらのページをご覧ください。方法  Forresterは、この調査のためにか国社にオンライン調査票を送り、ソフトウェア開発で必要なアプリケーション、ツール、サービスをクラウドマーケットプレイスから調達することについて、そのニーズなどを評価しました。出典  A commissioned study conducted by Forrester Research on behalf of Google  November  調査対象者 グローバル企業のITおよび開発部門で開発やデリバリの責任者を務める人  By Kavitha Radhakrishnan  Senior Product Manager",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/1824365629944070709/comments/default",
      "updated": "2019-05-30 23:11:14"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Google Cloud INSIDE FinTechを初開催 年月日 月 ",
      "description": "Google Cloudでは、年月日 月 にGoogle Cloud INSIDE FinTechを初開催いたします。初開催のテーマは『FinTech業界の勢いを支える最新技術』。あらゆる業界でのデジタルトランスフォーメーションが進むなか、とりわけ先進技術の導入が進むFinTech業界。FinTech業界をリードする方々をスピーカーに迎え、金融業界を取り巻く環境の変化や企業トレンドの解説、そして新しい技術のナレッジシェア、さらに様々な導入事例紹介を行います。セミナー終了後は、軽食付きの懇親会を予定しています。ぜひこの機会に、Google Cloudのエキスパートや、参加者、登壇者との交流を深めてください。このGoogle Cloud INSIDE FinTechでのナレッジシェアをきっかけに、新しいサービスやプロダクトが生まれるような会にしていきます。ぜひ  Google Cloud INSIDE FinTech  にご参加ください。参加登録※申し込み締切 月日 金 プログラム 開場     オープニング     「Google Cloudと機械学習が切り拓く、IT開発の新しい潮流」グーグル合同会社デベロッパーアドボケイト佐藤一憲     「リスクを抑えながらアジリティを上げる  Google Cloudで実現するシステム運用」 Google Cloudカスタマーエンジニアセブランフェラン     休憩     「金融庁から見たFinTech Landscape ーAI、ブロックチェーン、APIの観点から」金融庁フィンテック室長三輪純平氏     事例紹介株式会社Ginco高妻智一氏     「マネーフォワードにおけるCDNログ基盤」株式会社マネーフォワードCTO室エンジニア鈴木陽介氏     「マネックス証券API  金融のオープンAPIを支えるApigee」マネックス証券株式会社法貴大輔氏     懇親会 軽食付 開催概要イベント名Google Cloud INSIDE FinTech日程年月日 月 時間       受付開始 会場JPタワーホール カンファレンス〒 東京都千代田区丸の内二丁目番号KITTE   階対象FinTech業界や金融・保険・証券業界で働くインフラエンジニア、サーバーアプリケーションエンジニア、テクニカルリーダー定員名参加費無料 事前登録制 登録※企業のお客様が対象となりますので、学生の方のご参加はご遠慮いただいております。予めご了承くださいませ。※競合他社様、パートナー企業様からのお申し込みはお断りさせていただくことがございます。※報道関係者のご参加はお断りさせていただきます。※定員は名です。早期に定員に達した場合、お申し込みを締め切らせていただく場合がございます。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/4564498852168169816/comments/default",
      "updated": "2019-05-24 00:00:07"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート  GCPで始めるデータドリブンマーケティング",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。月日の放送では、データに基づくマーケティングの実践方法を解説しました。データに基づいた意思決定を行うには、必要なデータを活用できる状態にしておく必要があります。そもそもマーケティングとはなにか 広告とは何が違うのかを整理しながら、データを活用するために行うべきことを詳しく説明します。マーケティングとは製品やサービスが売れる仕組みをつくることです。この仕組みを作る上で、カスタマージャーニーを見極めて適切なメッセージを適切なタイミングで適切なユーザーに届けることが重要です。広告はこのマーケティングの一つの手段ですが、オンライン、オフラインにかかわらず広告の効果を分析し、最適な方法を探ることが課題となっています。データドリブンマーケティング基盤を作る企業内に存在するさまざまなデータを統合、管理、利用するための基盤の存在はデータドリブンマーケティングには欠かせません。社内システムとのシームレスなデータ連携、増え続けるログを格納できる十分なストレージ、分析を高速に実施するためのコンピューティングリソース、通常の集計やレポーティングだけでなく、機械学習機能も含めたデータ分析、こうした要求に対応できる基盤が今求められています。マーケティング関連のサービス月に開催されたCloud Next  サンフランシスコでは、データドリブンマーケティングに関連する新しいサービスや機能が数多く発表されました。BigQuery Connected Sheets   G Suiteのスプレッドシートから数十億件のデータを扱うことが可能です。何回かクリックするだけで、データをスプレッドシート上のダッシュボードとして可視化し、セキュアに組織内で共有が可能です。BigQuery ML   k meansクラスタリング、強調フィルタリングなど顧客のセグメンテーションやプロダクトレコメンデーションを行うための複数の新しいモデルの生成をサポートします。AutoML Tables  最先端の機械学習モデルを自動的にビルドして、構造化データに対してデプロイできます。Google Brainが提供する最先端モデルも含むさまざまな機械学習モデルを利用可能です。実際の適用例を番組内で紹介しています。年月日放送 nbsp GCPで始めるデータドリブンマーケティング番組で説明した資料はこちらで公開しています。    Cloud OnAir  GCPで始めるデータドリブンマーケティング年月日放送  from Google Cloud Platform   Japan Cloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/9208926692939894044/comments/default",
      "updated": "2019-06-06 12:39:20"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Compute EngineへのVM移行時に役立つヒント ベストプラクティス",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。クラウドへの移行で重要なのは、移行は一度限りの大規模な変更ではないということを肝に銘じておくことです。クラウドへの移行は、細かいステップをいくつも重ねて進む長い道のりにほかなりません。Google Cloud Platform GCP では、仮想マシン VM をGCPに移行する場合のガイダンスとベストプラクティスをまとめました。このブログ投稿ではその内容を簡単にご紹介します。なお、このガイドと本稿では、VMのCompute Engineへの移行に焦点を当てています。それでは、GCPのメリットから見ていくことにしましょう。Compute Engineに移行する理由ご存じのように、移行後のVMでアプリケーションを動かすには、データベース、メッセージング、アナリティクスなどのサービスと共にコンピューティングリソースが必要です。VMの実行環境として考えた場合、Compute Engine上で実行することの主なメリットは次のとおりです。コストの削減  継続利用割引が適用されるCompute Engineは、従来型のデータセンターでハードウェアやVMを管理する場合と比べて大幅に割安です。継続利用割引は、ほかのクラウドからGCPに移行するときにも大きなメリットとなります。機敏性 リソースの確保やプロビジョニングを待つことなく、ほとんど瞬間的にVMを立ち上げられるようになるので、お客様の多くは移行と同時に機敏性が向上します。新しいアプリケーションをすばやく立ち上げて試用し、必要に応じてオフにすることができます。オーバーヘッドの削減 データセンターの運営では通常、それぞれ独自の窓口、課金モデル、契約内容を持つさまざまなベンダーとの取引が必要です。クラウドに移行すれば、そのオーバーヘッドが大幅に削減されます。これにより、担当スタッフはデータセンターの運用にまつわる雑用に振り回されることがなくなり、ビジネスの成功に必要な業務に集中できます。移行先としてCompute Engineを選択したら、移行の際にどのようなことに留意すべきでしょうか。その一部を簡単に説明します。コストの計算移行に取りかかる前に、移行のコストを計算しましょう。これは、現状のデータセンターや既存のクラウド環境で実行しているもののコストを評価することを意味します。GCPのVM移行支援センターでコスト管理の方法を学び、ニーズに最もマッチするパートナーを選んでください。移行対象VMの評価移行コストの評価後は、移行するVMの検討を開始します。現代の企業はさまざまな種類のアプリケーションをVMで運用しており、それらすべてを同時にクラウドに移行することは通常はありえません。移行をうまく進めるには徹底的な評価が必要ですが、GCPであれば、そうした評価を無料で行うことができます。移行の設計移行するVMを決めたら、移行を始める前にクラウド環境を設計する必要があります。その最初のステップは、現在の環境とGCPとの違いを明らかにすることです。そして次に、GCP上の新しいクラウド環境をどのような形にすべきかを検討します。以下の各項では、移行に先立って検討すべきことを説明します。ガバナンスの確立クラウドリソースの作成、アクセス、変更、破棄に関するパーミッションを誰に与えるかを明確にする必要があります。また、リソースコストの支払い方法も決めなければなりません。それには、IAMのベストプラクティスに関するドキュメントが参考になります。ネットワークの作成移行後に使用するネットワークは、VMの移行前に存在していなければなりません。アプリケーションの稼働後にネットワークを設定するのは難しいため、パーミッションやアカウントと同様に、事前に作成しておくことが重要です。運用計画の策定クラウドでVMを稼働するようになれば、システムの常として、VMのモニタリングやロギング、運用上の管理が必要になります。移行後に慌てることがないよう、運用管理については事前の計画段階で考慮に入れておくべきです。クラウドへのVMの移行こうした準備を整えてから最初のVMの移行に取りかかります。最初の移行は、将来の移行のテンプレートになります。移行を重ねるうちにプロセスは改善されていくはずですが、最初の移行では特にあらゆることを記録することが重要です。Google Cloudの移行ツールであるVelostrataを使用すれば、VMのGCPへの移行を迅速かつ安全に、そして大規模に行うことができます。Velostrataは、ストリーミング技術を使用して移行にかかる時間を短縮すると共に、適切なインスタンスタイプを選びやすくするために推奨サイズを提示してくれます。また、組み込みテスト機能とロールバック 必要な場合 も提供します。GCPに移行するお客様はVelostrataを無料で利用できます。以上、VMをクラウドに移行する前に検討すべき項目を駆け足で見てきました。もっと詳しく知りたい方はこちらのドキュメントをご覧ください。  By Ron Pantofaro  Cloud Solutions Architect  Google Cloud Platform and Tom Nikl  Product Marketing Manager",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/2879983556997129856/comments/default",
      "updated": "2019-05-22 00:00:02"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Google Cloudで実践するWebアプリ開発開催のお知らせ",
      "description": "Google Cloudは、開発エンジニア、インフラエンジニア向けに「Google Cloudで実践するWebアプリ開発」を開催いたします。実際にアプリやサービスのフロントエンド開発に携わる先進的な企業を迎え、アプローチの方法やポイント、バックエンドとの連携などについてお話しいただきます。また、深い専門知識をもつGoogle Cloudのエンジニアが、Firebase、Cloud Functions、Google App Engine、Google Kubernetes EngineをはじめとしたGoogle Cloud Platformを利用し、どのようにWebアプリ開発、運用を行うのかについて解説します。開催概要名称 Google Cloudで実践するWebアプリ開発日時  年月日 月            会場 グーグル・クラウド・ジャパン合同会社〒 ­東京都港区六本木  六本木ヒルズ森タワースピーカー株式会社リクルートテクノロジーズ新井智士氏株式会社Ginco西川達哉氏グーグル合同会社宇都宮佑亮グーグル・クラウド・ジャパン合同会社篠原一徳プログラム   nbsp  nbsp 受付開始   オープニング    Webフロントエンド開発最新動向グーグル合同会社宇都宮佑亮    GCPのサーバーレスサービス紹介グーグル・クラウド・ジャパン合同会社篠原一徳   休憩   リクルートにおけるWebパフォーマンス改善の取り組み株式会社リクルートテクノロジーズ新井智士氏   インフラ管理不要のFirebaseとGKEで実現するモバイルアプリ開発株式会社Ginco西川達哉氏   クロージング   懇親会参加費 無料 事前登録制 主催 グーグル・クラウド・ジャパン合同会社定員 名参加申し込み上記リンクからお申し込みください。※競合他社様、パートナー企業様からのお申し込みはお断りさせていただくことがございます。※報道関係者のご参加はお断りさせていただきます。※ビジネス向けのイベントとなっております。学生の方のご参加はご遠慮ください。※お申し込み多数の場合は抽選を行います。参加いただける方には、後日、ご登録されたメールアドレスに参加のご案内をお送りします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/5994769357544147887/comments/default",
      "updated": "2019-05-20 01:36:21"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "つのユースケースで学ぶGCPのサービスアカウント",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。Google Cloud Platform GCP 上でアプリケーションを構築している方であれば、サービスアカウントのことはよくご存じでしょう。これはアプリケーションか仮想マシン VM に属する特殊なGoogleアカウントで、IDとして、もしくはリソースとして扱うことができます。サービスアカウントでは、管理方法やリソースへのアクセス権限の付与方法がユースケースによって異なります。この投稿ではサービスアカウントの一般的なユースケースをいくつか取り上げますので、サービスアカウント管理の適切な運用モデルを決定するうえで参考にしていただきたいと思います。ユースケース   GCPリソースにアクセスするウェブアプリケーションユーザーがCloud Identity Aware Proxy IAP から認証を受けてウェブアプリケーションにアクセスしているとします。GCPリソースを利用するウェブアプリケーションにアクセスできるユーザーの場合は、GCPリソースに対する直接的なアクセス権限を持つ必要はありません。こうしたウェブアプリケーションは、Cloud DatastoreといったGCPサービスへのアクセス権限を、サービスアカウントを使って獲得します。この場合、サービスアカウントはウェブアプリケーションと一対一で対応します。つまり、サービスアカウントはウェブアプリケーションのIDなのです。最初に、ウェブアプリケーションをホスティングするGCPプロジェクトでサービスアカウントを作成します。次に、GCPリソースへのアクセスに必要な権限を、そのサービスアカウントに付与します。そして最後に、サービスアカウントの認証情報を使用するようにウェブアプリケーションを設定します。ユースケース  複数のコストセンターに対するBigQuery使用料の請求このシナリオでは、部門のユーザーはカスタムビルドのアプリケーションを使用して、BigQueryの共有データセットにクエリを送ります。このとき、クエリによって発生する料金は、ユーザーが所属するコストセンターに請求する必要があるため、VM上で実行されるアプリケーションに対しては、BigQueryデータセットへのクエリ発行権限を有するサービスアカウントが与えられます。プロジェクトで使用されるリソースが請求書に反映されるようにするため、各部門の一連のプロジェクトにはラベルを付与します。また各部門は、部門に割り当てられたプロジェクトからアプリケーションを実行する必要があります。これにより、BigQueryに対するクエリ料金が、その部門に適切に請求されるようになります。クエリを発行する各部門のプロジェクトにこのシナリオを適用するには、BigQueryデータセットへのクエリ発行に必要なIAM権限を、アプリケーションのサービスアカウントに割り当てます。このシナリオによる権限設定の詳細はこちらをご覧ください。ユースケース  運用、管理作業で使われるサービスアカウントの管理GCP環境の管理を担当するシステム管理者やオペレーターであれば、環境のプロビジョニングや監査などの共通操作をGCP環境全体にわたって一元管理したいと考えるでしょう。この場合、さまざまなタスクを実行できるようにするため、適切な権限を持つサービスアカウントを複数作る必要があります。これらのサービスアカウントは、上位の特権を持ち、階層内の適切なレベルで権限を付与されているはずです。そして、他のサービスアカウントと同様に、権限のないユーザーに情報が漏れることのないよう、ベストプラクティスに従う必要があります。たとえば、こういった運用のためにサービスアカウントを作成したプロジェクトでは、サービスアカウントが誤って削除されないようにするため、プロジェクトリーエンを追加すべきです。サービスアカウントの奥の深さ上記のユースケースからおわかりいただけるように、つのモデルではすべてのケースに対応しきれないので、ユースケースに応じて運用モデルを選ぶ必要があります。今回はつのユースケースを紹介しましたが、サービスアカウントを論理的にどこに配置すべきかを検討するうえで参考になれば幸いです。サービスアカウントの詳細については、次のチュートリアルのどれかをお試しいただき、選択したGCPコンピューティングサービスのもとでサービスアカウントの認証情報をどのように使用するべきかを学んでください。サービスアカウントを使用したCloud Platformへの認証サービスアカウントの認証情報を使用したアプリケーションの認証AppEngineにおけるサービスアカウント  By Grace Mollison  Cloud Solutions Architect",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/2923309041375320247/comments/default",
      "updated": "2019-05-16 00:38:59"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Google Cloud Platformトレーニングイベント  Cloud OnBoard  大阪スペシャルバージョン月日 火 開催",
      "description": "Cloud OnBoardは、GCP認定トレーナーによるGoogle Cloud Platform  GCP トレーニングイベントです。年の開催以来、毎回多くのお客様にお越しいただき、トレーニングを実施してまいりました。このたび『Cloud OnBoard  大阪』では、大阪初のスペシャル企画として、つの講座を同時開催いたします。まずはGCPに興味がある・理解したいと考えているお客様のはじめの一歩となる入門編トレーニング、そして、機械学習、BigDataの集中トレーニング講座、更にGCP製品の賢い組み合わせ方法をお話するアーキテクトデザイン講座も開催いたします。各トレーニングでは、スペシャルトレーナーが各製品の利点、特徴、ユースケースをお話し、さらにGCPエキスパートが皆さまの疑問にお答えいたします。セミナー終了後は、Qwiklabs演習をセルフラーニング形式で実施予定です。時間は時間程度で、ご希望の方はどなたでもご参加いただけます。ノートPCをご持参の上、ぜひご参加ください。ITマネージャ、システムエンジニア、ソリューションデベロッパー、ソリューションアーキテクト、オペレーター、ビジネスリーダーまたはGoogle Cloud Platformに初めて触れる方、更に知見を広めたい方のために構成されたトレーニングです。ぜひ、同僚の方々とお誘い合わせの上、ご参加ください。Cloud OnBoard  大阪スペシャルバージョン講座 Google Cloud Platform入門編トレーニング「初めてのGoogle Cloud Platform」Big Data講座「GCPを活用した効率的なビッグデータの利活用方法」アーキテクトデザイン講座「アーキテクチャ原則とパターンで学ぶ、実践的なGCP使いこなし講座」機械学習講座「エンドツーエンドで学ぶGCPを活用した機械学習モデルの構築」日時  年月日 火        受付開始  会場 ハービスホール〒 大阪市北区梅田  ハービスOSAKA BF参加費 無料 事前登録制 お申し込みはこちら",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/5247691945989813994/comments/default",
      "updated": "2019-05-16 05:02:00"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "大阪GCPリージョンの正式運用を開始します",
      "description": "Posted by Google Cloud CEOトマスキュリアンGoogle Cloudは、本日より、大阪GCPリージョンの正式運用を開始したことをお知らせいたします。大阪GCPリージョンは、年に運用を開始した東京GCPリージョンに続く、日本国内でか所目のクラウド拠点です。同リージョンの提供開始にともない、アジア太平洋地域ではか所、グローバルでは拠点が利用できるようになります。大阪GCPリージョンは、急速に拡大する日本の顧客層をサポートするだけでなく、お客様における事業継続性を高めるほか、可用性の向上に寄与します。Google Cloudは、セキュアで信頼性の高いグローバルインフラストラクチャ、大規模データを管理するデジタルトランスフォーメーションプラットフォーム、新しいデジタル機能をもたらす業種特化型ソリューションの提供を通じ、ビジネスの発展を支援することを、そのミッションに掲げています。Googleでは過去年間、グローバルインフラストラクチャに対し億ドルを超える投資を行ってきました。こうした継続的な投資と、グローバルなパートナーエコシステム、Go To Market部門の拡充等を通じ、Google Cloudは世界的なリーディングカンパニーとの戦略的パートナーシップを構築してきました。日本では、年の東京GCPリージョン公開以降、小売、ゲーム、金融サービス、製造を中心とする様々な業界においてGoogle Cloudのテクノロジーソリューションの採用が進んでいます。今回、大阪GCPリージョンの公開にあわせアサヒグループホールディングスや、京セラコミュニケーションシステム等の日本のリーディングカンパニーが、それぞれの業種におけるデジタルトランスフォーメーションのためにGoogle Cloudを選択されました。「GCP大阪リージョンの正式運用開始心待ちにしておりました。弊社は以前よりBigQueryを始めとしたGCPのサービスを活用して参りました。大阪リージョンの開設により、さらなるシステムの可用性向上、ビジネスの継続性実現に取り組んでいきたいと考えています。」ーアサヒグループホールディング株式会社執行役員IT部門 nbsp ゼネラルマネジャー知久龍人氏「京セラコミュニケーションシステム株式会社はGCP大阪リージョンの正式運用開始を心より歓迎いたします。弊社のIoTトータルソリューションにおいてApp EngineやBigQueryを始めとしたGCPのサービスを以前から活用してきました。大阪リージョンの開設に伴い、リージョンを利用したシステムの可用性向上、ビジネス継続性の実現に取り組み、今後さらにサービス品質を高めて参ります。」ー京セラコミュニケーションシステム株式会社代表取締役社長黒瀬善仁氏お客様の成功を後押しするために、パートナー各社との協力は不可欠です。この度、NTTスマートコネクトおよびソフトバンクと協力できることを、私たちは嬉しく思っています。「大阪GCPリージョンの正式運用開始を歓迎します。東京に続く日本でヶ所目となるリージョンの運用開始により、当社が提供するGCPとの閉域接続サービスを活用することで、日本国内でのディザスタリカバリにも対応したより堅牢なGCP環境の提供が可能となります。」ーソフトバンク株式会社常務執行役員法人事業統括副統括担当佐藤貞弘氏「この度、GCP大阪リージョンの提供開始を心より歓迎いたします。今回のGCP大阪リージョンの開設により、大阪市内を中心に提供している弊社ハウジング・クラウドサービスとGCP大阪リージョンを閉域で接続できるサービス『クラウドクロスコネクト』で組み合わせ、低遅延でセキュアなハイブリッドクラウド環境の提供を実現します。」ーエヌ・ティ・ティ・スマートコネクト株式会社代表取締役社長白波瀬章氏Google Cloudでは、年上半期に運用を開始するソウル、ソルトレイクシティ、ジャカルタをはじめとし、引き続き、新しいリージョンを順次開設して参ります。大阪GCPリージョンの詳細は以下をご参照ください。つのゾーンと、Compute Engine、App Engine、Google Kubernetes Engine、Cloud Bigtable、Cloud Spanner、BigQueryなど一連の標準プロダクトを提供するGoogle Cloud Platformリージョンです。先ごろリリースしたAnthosと組み合わせることで、オンプレミスと任意のパブリッククラウドを自由に選択し、どんなインフラストラクチャのワークロードも管理できる柔軟性が得られます。ディザスタリカバリでのITおよびビジネス要件に不可欠なセキュアな分散インフラストラクチャにより、従来よりも強固な事業継続計画を策定できます。日本国内の強力なパートナーエコシステムにより、充実したサポートとサービスを提供します。大阪GCPリージョンについてはこちらのページを、新規のサービスやリージョンの提供についてはロケーションページをご覧ください。また、今回GartnerのMagic QuadrantでGoogle Cloudが、日本のクラウドインフラストラクチャ分野におけるリーダーに選出されたことをお知らせします。この栄誉は、業界最高品質のクラウドサービスをお客様に提供する努力が認められたものであると考えています。私たちは、今後とも、日本をはじめ世界のお客様に価値あるサービスをお届けすべく、一同尽力して参ります。  By Thomas Kurian  CEO  Google Cloud",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/3969311540166725458/comments/default",
      "updated": "2019-05-16 05:36:42"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート  GCPで実現するバックアップ・ディザスタリカバリのベストプラクティス",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。月日の放送は、システムバックアップやディザスタリカバリ DR など、GCPの機能を利用した可用性の高いシステムの構築について解説します。Compute EngineのバックアップCompute Engineの永続ディスクをバックアップする方法には大きく分けて、スナップショット、イメージ、Cloud Storageにコピーするの種類があります。イメージ 永続ディスクをまるごとバックアップする方法です。インスタンスやインスタンステンプレートの作成時に用います。常にフルバックアップを行うので、保存サイズは一般的に大きくなります。スナップショット こちらも永続ディスクをまるごとバックアップする方法です。フルバックアップの後は増分のみを保存します。データは圧縮されるため、保存時のサイズも小さくなります。永続ディスク上のデータをバックアップする場合はこちらを利用します。また、スナップショットは別リージョンに保存可能です データ転送料は不要 。さらに、スナップショットを定期的に実行するスケジュール機能も提供されています 現在、ベータ提供 。Cloud Storageにコピー ファイル単位でバックアップを行うための方法です。Cloud StorageはRestful APIでオブジェクトを操作するオブジェクトストレージサービスで、高い耐久性を備えています。オブジェクトのバージョニング 世代管理 に対応し、ライフサイクル管理機能も備えています。Cloud SQLのバックアップフルマネージドなデータベースサービスである、Cloud SQLのバックアップについて解説します。種類のリストアの方法を取り上げて、それぞれのメリット、デメリットを整理します。ディザスタリリカバリをクラウドで実現自然災害などでサービスやシステムが深刻な被害を受けたときに、損害を軽減したり、機能を維持あるいは回復・復旧する方法や措置です。一般に、ディザスタリカバリを実現するには、本番のデータをリアルタイムもしくは定期的に同期する仕組み本番環境とDR環境を高速で接続するネットワーク本番環境と同じ機能が提供できるサーバーやネットワーク機器 公開サービスの場合 インターネット接続電力や空調、スペースなどのファシリティなどが求められます。番組では、こうした要求に対して、GCPがどのようなサービスを提供しているのか、またそれらをどのように使えば良いかのポイントを解説します。さらに、一連の復旧作業をテンプレート化する方法として、Cloud Deployment Manager を紹介します。Cloud Deployment Managerでは、アプリケーションに必要なすべてのリソースを宣言的に記述することができます。PythonテンプレートやJinjaテンプレートを使い、構成内容をパラメータ化し、負荷分散や自動スケーリングインスタンスグループなど、共通のデプロイ計画を何度も使用することができます。オンプレミス環境のDR対策は、ファシリティ データセンター の冗長化も含めた対策が求められるため、クラウドに移行済みの環境に比べて解決すべき課題が広範囲におよびます。そこで、データのバックアップなど部分的にクラウド化することで、安価かつ堅牢なバックアップ環境を構築することが可能です。オンプレミスからGCPへのデータバックアップをサポートするサードパーティ製品を活用すれば、データ移行もより容易になります。年月日放送 nbsp GCPで実現するバックアップ・ディザスタリカバリーのベストプラクティス番組で説明した資料はこちらで公開しています。    Cloud OnAir  Google Cloudで実現するバックアップディザスタリカバリのベストプラクティス年月日放送  from Google Cloud Platform   Japan Cloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/1241202708503780628/comments/default",
      "updated": "2019-05-07 11:34:32"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Google Cloud Platform  サポート体制拡充に関するお知らせ",
      "description": "年にGoogle Cloud Platformサポートを開始して以来、グローバルで多くのお客様にGoogle Cloud Platformサポートをご利用いただいてきました。多くのお客様にGoogle Cloud Platformをご利用いただく中で、お客様が利用される言語でサポートさせていただくことの大切さを実感してまいりました。また、お客様のご要望にお応えするため、私たちはグローバルでサポート体制の強化に努めてきました。そして、この度、以下の言語を新たにサポート、拡張することになりました。中国語 標準語 による営業時間 中国標準時 でのサポート韓国語による営業時間 韓国時間 でのサポート日本語による時間日サポート Platinum   Enterpriseのお客様向け  nbsp この変更により、Gold  ProductionとPlatinum  Enterpriseのお客様は、中国語と韓国語によるサポートをご利用いただくことができるようになりました。韓国語でのサポートへのお問い合わせはこちらのぺージから、中国語 簡体字 でのサポートへのお問い合わせはこちらのページから、中国語 繁体字 でのサポートへのお問い合わせはこちらのページからご利用いただけます。中国語、韓国語でのサポートは、現地時間の平日午前時から午後時までの営業時間でのご案内となります。また、サポートのサービスレベルがPlatinum、もしくはサポートロールがEnterpriseのお客様は、日本語による時間日サポートをご利用いただくことができるようになりました。緊急対応が必要な時には、時間日お問い合わせいただくことができます。詳細はTechnical Support Services Guidelines  英語 にてご確認いただけます。Google Cloud Platformをご利用いただいている皆様からの様々なフィードバックをお待ちしております。どうぞこれからもGoogle Cloud Platformサポートをご利用ください。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/7175971167542207683/comments/default",
      "updated": "2019-05-09 04:37:44"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "GKE Advanced 信頼性、単純性、スケーラビリティを高めたエンタープライズ向けのGKE新エディション",
      "description": "※この投稿は米国時間年月日にGoogle Cloud blogに投稿されたものの抄訳です。編集部注 本稿は、Google Kubernetes Engineのユニークな機能を紹介する連載の第回目です。回目以降はさらに高度な機能を取り上げます。ご期待ください。年のオープンソース化以来、Kubernetesは大きな発展を遂げてきました。さまざまなユースケースに対応するべく、堅牢なインストール、管理、構成のツールなどがコミュニティによって整備されました。しかし、多くの企業はKubernetesを自社で実行することの負担に耐えかねて、私たちGoogle CloudのマネージドサービスであるGoogle Kubernetes Engine GKE を採用しています。企業は、基盤のインフラストラクチャに不満があったのではなく、ビジネスへの注力を可能にする基盤を求めていたのです。このほど、私たちはGKE Advancedを発表しました。GKE Advancedは、世界規模の堅牢なインフラストラクチャを管理するにあたって私たちが学んできたことを基に、エンタープライズグレードの制御、自動化、柔軟性の機能を追加したGKEです。従来のGKEはGKE Standardに改称します。つのGKEの違いを簡潔に説明すると、次のようになります。GKE Advancedは、インフラストラクチャ管理の高度な自動化、セキュリティ強化のための統合型ソフトウェアサプライチェーンツール、金銭補償付きのSLAによる信頼性強化、サーバーレスワークロードの実行サポートを提供します。これらの新機能とツールにより、変化の激しい環境でのワークロードとクラスタの管理は単純化され、スケーリングはハンズフリーになります。Kubernetesの高いポータビリティとサードパーティエコシステムはそのままに、機能強化を図っています。GKE Standardは、現在一般提供されているすべての機能を備えており、さほど複雑ではないプロジェクト向けのマネージドサービスを提供します。GCP Marketplaceで入手できるツールも含め、Googleおよびサードパーティ製品から成るGCPのリッチなエコシステムを引き続き利用できます。GKE Advancedに組み込まれる機能について詳しく見ていきましょう。拡張されたSLAGKE Advancedは、SLAにおいてリージョンクラスタの  の可用性が金銭補償付きで保証されており、ミッションクリティカルなワークロードを安心して実行できます。シンプルになった自動スケーリング可用性と信頼性を高めるために手作業でKubernetesクラスタをスケーリングしようとすると、作業が複雑になり時間を要することがあります。GKE Advancedにはスケーリングを容易にするつの新機能が備わっています。垂直ポッド自動スケーリング VPA は、リソースの利用状況を監視して、リクエストされたCPUとRAMを調整し、ワークロードを安定させます。ノード自動プロビジョニングは、Cluster Autoscalingの拡張版としてクラスタリソースを最適化します。防御用のレイヤを追加DevOpsやシステムの管理者は、必要なサードパーティソフトウェアをKubernetesクラスタで実行しつつ、それを隔離してセキュリティを確保したいと思うことがよくあります。GKE Advancedには、gVisorをベースとする軽量のコンテナランタイム、GKE Sandboxが組み込まれています。これは、ポッドレイヤに防御用の第レイヤを追加し、コードおよび設定の変更や新しい制御方法の学習を必要とせずにコンテナ化アプリケーションをハードニングします。ソフトウェアサプライチェーンのセキュリティソフトウェア開発ライフサイクルにおける悪意ある、もしくは意図せぬ変更により、システムダウンやデータ侵害が発生することがあります。Binary Authorizationを使用すれば、ビルドやテスト中のコンテナイメージが、信頼のおける機関によって署名されます。ビルド リリースプロセスでは検証済みのイメージだけが統合され、コンテナ環境をより厳格に管理できるようになります。サーバーレスコンピューティングコードの実行基盤となるインフラストラクチャのことを気にせずに、アプリケーションをスピーディに開発してローンチしたいとお考えでしょう。Cloud Run on GKEは、Knativeベースの自動スケーリング ゼロインスタンスも可能 、ネットワーキングとルーティング、ロギング、モニタリングにより、ステートレスなサービスのデプロイおよび実行のための一貫したデベロッパーエクスペリエンスを提供します。インフラストラクチャ使用状況の把握つのGKEクラスタをマルチテナントの形で共有すると、どのテナントがリソースのどの部分を使用しているかがわかりにくくなります。GKE usage meteringは、クラスタのリソース使用状況を表示できるようにします。リソースをKubernetes名前空間とラベルで分類し、顧客や部門などのエンティティに帰属させた形で示すことができます。GKE Advancedは、高度な自動スケーリングとセキュリティ機能、サーバーレスワークロードのサポート、拡張された使用状況レポートをすべてSLAによる金銭補償付きで追加し、最も条件の厳しい本番アプリケーションをマネージドKubernetesサービス上に構築するために必要なツールと信頼性を提供します。GKE Advancedは米国の第四半期に無料トライアルの形でリリースされる予定です。GKE Advancedについて聞きたいことはありませんか 詳細はGoogleのお客様窓口にお問い合わせください。また、Cloud OnAir「Your Kubernetes  Your Way Through GKE」も、ぜひご視聴ください。  By Jerzy Foryciarz  Senior Product Manager",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/3987783443475691475/comments/default",
      "updated": "2019-04-26 04:45:00"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート  Next  サンフランシスコ最新情報G Suite特集",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。月日の放送は、「Next  サンフランシスコ最新情報G Suite特集」と題して、Google Cloud Next  で発表されたG Suite関連の情報をお伝えしました。Google Cloudが提供するつのソリューションのうち、「Transform Work」と「Security  amp  Trust」を中心に製品アップデートをまとめました。より速くチーム内の仕事のスピードを加速させるためのさまざまなアップデートが発表されました。そのいくつかを紹介します。Assistant in Calendar ベータ提供    G SuiteとGoogleアシスタントのカレンダー連携です。最新のスケジュールをGoogleアシスタントに確認することができます。Hangouts Meetのアップデート  Meet Live Captionsという自動字幕 一般提供 が追加されました。また、ライブストリーミングの一般公開 近日提供 や参加者上限数を人に引き上げることも予定しています 近日提供 。Cloud Searchのサードパーティ接続 一部のお客様に正式提供    Salesforce、Atlassian Confluence、SAPなどサードパーティのデータソースを検索することが可能となります。以上のコネクターによって、以上のエンタープライズソースからインデックスを作成することができます。よりスマートにチームの全員にデータへのアクセス手段を提供し、より正しい判断へと導くための発見を促します。connected sheets  近日ベータ提供    Googleスプレッドシートから、BigQueryデータを直接操作できる機能です。Googleドキュメント、スライド、スプレッドシートにおけるOffice編集機能 一般提供   nbsp    nbsp Microsoft Officeファイルを変換することなく直接編集することができます。G Suiteアドオン 近日ベータ提供  使い慣れた業務アプリケーションへのアクセスを、複数のアプリケーションやタブを切り替える代わりに、G Suiteのサイドパネルから行えます。よりコラボレーティブにチーム全員が協力し合うというカルチャーづくりを支援します。Hangouts Chat into Gmail  ベータ提供   チーム内のコミュニケーションをGmail画面の左下区域で簡単に行えるようになります。Googleドライブでのビジター共有 ベータ提供    Googleアカウントを持たない人でも、持っている人と同じように、Googleドライブに格納されたファイルを簡単に共有することが可能です。Currents ベータ提供   エンタープライズ向けのGoogle がCurrentsという新しい名前に変わりました。Currentsでは、社内で話題に上っている議論に目を通したり、興味のあるテーマを追いかけたりすることが簡単にできます。セキュリティに関するアップデート前号でも触れたセキュリティに関するアップデートの追加情報です。コンテキストベースのアクセス制御  BeyondCorp   ユーザーの認証とアクセスの要求のコンテキストによるきめ細かい粒度のアプリおよびインフラストラクチャへのアクセス制御の定義および強制を支援します。Android搭載携帯端末のビルトインセキュリティキー  お手持ちのAndroidスマートフォンをFIDOセキュリティキーとして使うことで、フィッシングやアカウントハイジャックからの保護を強化します。Gmail Advanced Protections  ベータ提供   フィッシング攻撃に対する新しい警告とドメインスプーフィング なりすまし から保護します。G Suite Security Collaboration ベータ提供   セキュリティセンターの調査ツールの調査データを保存し、アラートセンターの中でアラートの管理やステータスの更新等の一元管理が可能となります。Chrome Browser Cloud Management   ChromeブラウザのポリシーをWindows、Mac、Linux、Chrome OSを横断して管理するだけでなく、ブラウザのデプロイに関するインサイトと可視性を提供します。年月日放送 nbsp Next  サンフランシスコ最新情報G Suite特集番組で説明した資料はこちらで公開しています。    Cloud OnAir  Next  サンフランシスコ最新情報GCP特集年月日放送  from Google Cloud Platform   Japan Cloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/6702602290839404763/comments/default",
      "updated": "2019-04-26 02:36:07"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Google Cloud Next  で行ったの発表",
      "description": "※この投稿は、米国時間年月日に、Google Cloud Blogに投稿されたものの抄訳です。今年のGoogle Cloud Next  は、皆さま、お楽しみいただけましたか。Google Cloudのコミュニティが結集し、多くの新しい技術について学び、また、お客様やパートナーの皆さまが、どのようにクラウドを用いてアイデアを実現し、ビジネスを推進しているのかを深く知る有意義な機会となりました。先週Next  で発表した、インフラストラクチャからアプリケーション開発、データ管理、スマートアナリティクス、AI、生産性向上、パートナーシップ関連に至るまでの数多くの新製品や製品アップデート、ソリューションなどの発表をまとめてご紹介します。インフラストラクチャ韓国ソウルおよび米国ユタ州ソルトレイクシティに新たにか所のGCPリージョンを開設することを発表しました。これにより、グローバルネットワークをさらに強化し、世界でビジネス拡大を続けるお客様をサポートします。ハイブリッドクラウドAnthos  nbsp  旧Cloud Services Platform はGoogle Kubernetes Engine  nbsp  GKE およびGKE On Premで一般提供を開始しました。お客様はオンプレミスまたはクラウド上のアプリケーションをデプロイ、実行、管理することができます。また、近いうちにAWSやAzureなどのサードパーティクラウドでも利用可能となる予定です。Anthosはハードウェア、ソフトウェア、システムインテグレーションなど、以上のパートナーと連携しており、お客様における立上げや稼働を速やかにサポートすることができます。Anthos Migrate は、Velostrata の移行技術を採用しており、オンプレミスや他のクラウドの仮想マシン VM からGKEのコンテナに最小限の労力で直接かつ自動的に移行できます。Anthos Config Management により、“Single Source of Truth  信頼できる唯一の情報源 をもとに、ロールベースのアクセス制御とリソースのクォータを設定、強制して名前空間を作るマルチクラスタポリシーを作成できます。サーバーレスCloud Runは、フルマネージドのサーバーレス実行環境として、コンテナ化されたアプリケーションにサーバーレスの機敏性を提供します。Cloud Run on GKEは、GKEクラスタに、サーバーレスのデベロッパーエクスペリエンスとワークロードのポータビリティをもたらします。オープンAPIとランタイム環境であるKnativeは、場所を問わず既存のKubernetesクラスタに、サーバーレスなデベロッパーエクスペリエンスとワークロードのポータビリティをもたらします。また、Cloud FunctionsとApp Engineのプラットフォームにも新たな投資をしており、新しい第世代ランタイム、新しいオープンソースのFunctions Framework、およびGCPリソースへのプライベート接続を含む追加のコア機能などを含んでいます。DevOps Site Reliability Engineering  SRE 新たなCloud Codeは、IntelliJやVisual Studio Codeなど、お好みのローカルな統合開発環境IDE IDE を拡張し、Kubernetes上でのクラウドネイティブなアプリケーションの開発・導入が容易になります。 API管理Apigeeハイブリッド ベータ提供 は、Apigee API管理プラットフォームの新たなデプロイオプションです。データセンターやお好みのパブリッククラウドなど、どこでもランタイムをホストできます。Apigeeセキュリティレポーティング  ベータ提供 は、APIのセキュリティステータスを可視化します。これにより、Apigee API管理プラットフォームから直接、Cloud Functions  IAMによる保護 、Cloud Data Loss Prevention テンプレートサポート 、Cloud ML Engine、BigQueryなど、さまざまなGoogle Cloudサービスをご利用いただけます。拡張機能の全リストはこちらからご覧ください。データ管理データベース近い将来Google Cloudで利用可能に フルマネージドデータベースであるCloud SQL for SQL Server  アルファ提供 により、既存のSQL Serverワークロードの移行をサポートします。また、Compute Engine上のSQL Serverを利用すればGoogleから購入したライセンスを利用できます。CloudSQL for PostgreSQLでバージョンのサポートを追加しました。また、パーティショニングの向上、ストアドプロシージャ、より多くの並列処理などの便利な新機能を追加しました。Cloud Bigtableでマルチリージョンレプリケーションを一般提供します。これにより、アプリケーションのニーズに応じて、リージョンをまたいで、もしくは全世界でBigtableに格納されたデータを利用可能になりました。ストレージCloud Storageのさらに低料金なアーカイブクラスは他のCloud Storageクラスと同じ一貫性のあるAPIで数ミリ秒のレイテンシで保管されたコンテンツにアクセスが可能です。高性能なストレージのニーズに応えるマネージドファイルストレージであるCloud Filestoreの一般提供を開始しました。アクティブ アクティブの同リージョン内複数ゾーンにまたがるディスクレプリケーションを備えた nbsp Regional Persistent Disksの一般提供を来週開始します。Google Cloud Storage用のバケットポリシーのみ ベータ提供 はCloud IAMのポリシーをバケット単位で、Cloud Storageのバケットへのアクセスを一貫性を保ちながら統合されたアクセス制御で提供します。同じアプリケーションコードを利用して保存した複数のオブジェクトに対してさらに高いセキュリティレベルでのアクセスを可能にするGoogle Cloud Storage用のV署名のベータ版の提供を開始しました。HMAC Hash based Message Authentication Code キーのほかに、Google RSAキーもV署名リクエストをサポートしています。Transfer Service用のCloud IAMロールを公開しました。これにより、IT管理者とセキュリティ管理者はCloud IAMの権限を利用して、転送ジョブの作成、読み出し、削除ができます。ネットワーキングTraffic Directorはサイドカーサービスプロキシにインテリジェントなトラフィック制御と設定を可能にし、複数のGoogle Cloudリージョンにアプリケーションインスタンスをデプロイ可能にすることでお客様のサービスにグローバルな対障害性を提供します。High Availability VPN  近日ベータ提供予定 により、オンプレミスのデプロイメントとGCPのVPCを業界最先端のSLA   で接続可能になります。 Gbps Cloud Interconnectにより、お客様のハイブリッドやマルチクラウド展開の相互接続が可能になりました。Cloud StorageやBigQueryといったGoogleサービスや、サードパーティーのSaaSサービスをCloud InterconnectやVPNを通じてセキュアに利用できるオンプレミスからクラウドへのプライベートGoogleアクセスの一般提供を開始しました。Network Service Tiers により、お客様がGoogle Cloudのネットワークをパフォーマンスもしくは価格に応じて、PremiumもしくはStandard Tierでワークロードごとに設定できるようになりました。セキュリティ amp 認証システムセキュリティAccess Approval ベータ提供 は、GCPにあるデータやコンフィグレーションへのアクセスを、その実際の発生に先立って明示的に承認することのできる、業界初の機能です。Data Loss Prevention  DLP ユーザーインターフェース ベータ提供 は、わずか数クリックでDLPスキャンを実行でき、コーディングが不要なだけでなく、ハードウェアや仮想マシンの管理も必要ありません。Virtual Private Cloud  VPC  Service Controls 一般提供 は従来のVPCの枠を超えて、Cloud Storageバケット、Bigtableインスタンス、BigQueryデータセットなど個別のGCPリソースについてセキュリティ境界を設定できるため、データの漏出リスクを軽減することができます。GCPのための包括的なセキュリティマネジメントおよびデータリスクプラットフォームであるCloud Security Command Center の一般提供を開始しました。Cloud Security Command CenterのEvent Threat Detectionは、Google独自のインテリジェンスモデルを使い、マルウェア、クリプトマイニング、外部へのDDoS攻撃など、被害をもたらす脅威を迅速に検出します。ベータプログラムへのお申し込みはこちらから。Cloud Security Command CenterのSecurity Health Analyticsは、お客様のGCPインフラストラクチャを自動的にスキャンし、パブリックストレージバケット、オープンなファイアウォールポート、古くなった暗号化キー、無効になったセキュリティログなど、コンフィグレーション上のさまざまな問題の発見に役立ちます。アルファプログラムへのお申し込みはこちらから。Cloud Security Scannerは、クロスサイトスクリプティング XSS 、クリアテキストのパスワードの使用、古くなったライブラリなど、お客様のGCPアプリケーション内の脆弱性を検出し、その結果をCloud Cloud Security Command Centerに表示します。App Engineは一般提供、GKEとCompute Engineはベータ提供です。Capsule、Cavirin、Chef、McAfee、Redlock、Stackrox、Tenable io、およびTwistlockを含むセキュリティパートナーとの連携により、検出結果を統合するとともに、応答を迅速化します。詳細についてはGCP Marketplaceをご覧ください。Cloud Security Command CenterのStackdriver Incident Response and Management 近日ベータ提供 は、お客様の脅威への応答と修正をサポートします。Container Registry脆弱性スキャニング 一般提供 はUbuntu、Debian、およびAlpine Linuxのパッケージの脆弱性を検出し、お客様のコンテナ展開に先立って脆弱性を発見することができます。Binary Authorization 一般提供 はお客様のCI   CDシステムと一体化してデプロイ時のセキュリティを制御し、要件を満たさないイメージのデプロイを阻止します。オープンソースのgVisorプロジェクトをベースとするGKE Sandbox ベータ提供 は、マルチテナントのワークロードを対象とした隔離機能を追加し、コンテナエスケープの防止とワークロードのセキュリティ改善を支援します。Managed SSL Certificates for GKE ベータ提供 は、お客様のGKE ingress証明書について、完全なライフサイクルマネジメント プロビジョニング、デプロイ、更新、および削除 を行います。Shielded VMs 一般提供 はお客様のCompute Engine VMインスタンスの整合性を検証し、情報漏洩を防止します。Policy Intelligence  アルファ提供 はMLを活用し、ポリシーの理解と管理を支援するとともに、リスクを軽減します。Phishing Protection ベータ提供 では、安全でないURLをGoogle Safe Browsingに迅速に報告し、Cloud Security Command Centerでそのステータスを確認することができます。reCAPTCHA Enterprise ベータ提供 は、お客様のウェブサイトをスクレイピング、クレデンシャルスタッフィング、自動化されたアカウント作成などの不正行為から防御します。大きな被害をもたらす自動化されたソフトウェアによる攻撃の阻止を支援します。認証システムとアクセス管理BeyondCorp Allianceの開始などを含むContext awareアクセスの機能拡張により、ユーザーの認証とアクセスの要求のコンテキストによるきめ細かい粒度のアプリおよびインフラストラクチャへのアクセス制御の定義および強制を支援します。フィッシング攻撃に対する最高レベルの防御である、Android搭載携帯端末のビルトインセキュリティキーの提供を開始しました。数千の追加アプリへのシングルサインオンや人事管理システム HRMS との統合などを含む、Cloud Identityの強化を発表しました。Identity Platform の一般提供により、お客様の開発したアプリやサービスへ認証管理システムとして追加することが可能になります。スマートなアナリティクスデータアナリティクスData Fusion ベータ提供 はフルマネージドのクラウドネイティブなデータインテグレーションサービスです。BigQueryなどに対し、様々なデータソースからデータを統合し、取り込むことが可能です。BigQuery Data Transfer Serviceが以上のSaaSアプリに対応しました。コードを一行も書かずにデータウェアハウスとの統合を行えます。Cloud Dataflow SQL  パブリックアルファ提供 は使い慣れたSQLでバッチおよびストリーミングのパイプラインを構築可能にします。Dataflow Flexible Resource Scheduling FlexRS  ベータ提供 はコスト削減の柔軟さを兼ね備えたバッチ処理ジョブを可能にします。Cloud Dataproc autoscaling ベータ提供 はGoogle Cloud Platform上でHadoopとSparkクラスタのプロビジョニングとデコミッショニングに伴うお客様の負担を解消し、Googleの他のデータアナリティクスプラットフォームと同じサーバーレスの利便性を提供します。Dataproc Presto job type ベータ提供 は、Cloud StorageやHiveメタストアなど分断化されたデータソースに対し、よりシンプルなアドホックPrestoクエリを実行できるよう支援します。両方のクエリとスクリプトが、ネイティブDataproc APIの一部として実行できるようになりました。Dataproc Kerberos TLC ベータ提供 はKerberosに対するすべてのAPIのサポートを通じ、Dataproc上でHadoopのセキュアモードを可能にします。この新たな統合により、クロスレルム間の信頼性、RPCとSSLの暗号化、およびKDCアドミニストレータのコンフィグレーション機能を提供します。BigQuery BI Engine ベータ提供 は、インメモリの分析サービスで、膨大かつ複雑なデータをほぼ瞬時に可視化、分析、操作を可能にします。パートナーツールによるビジュアル分析もオプションとして用意されています。Connected sheetsはBigQueryのパワフルさをシンプルなスプレッドシートのインターフェースと統合した新しい種類のスプレッドシートです。何回かクリックするだけで、データをスプレッドシート上のダッシュボードとして可視化し、セキュアに組織内で共有が可能です。BigQuery ML  一般提供 は、SQLクエリにより呼出可能な新しいモデルを搭載しました。BigQuery  k means clustering ML ベータ提供 は、BigQuery内Standard SQLを通じ、お客様が指定する軸や属性に基づいてデータポイントのグルーピングを設定することができます。BigQuery  import TensorFlow models アルファ提供 では、お客様のTensorFlowモデルをインポートし、それをBigQueryから直接呼び出すことにより、分類子や予測モデルをBigQueryから直接作成できます。BigQuery  TensorFlow DNN classifierは、多数の特徴やシグナルに基づくデータ分類を支援します。BigQueryのStandard SQLインターフェイスから、お客様が選択したDNNモデルのトレーニングおよび展開を実施でききます。BigQuery  TensorFlow DNN regressorを使用することで、TensorFlow内で回帰分析を設計し、次いでBigQuery内で呼び出すことによりデータのトレンドラインを生成することができます。Cloud Data Catalog ベータ提供 はフルマネージドのメタデータ探索とメタデータ管理のプラットフォームです。大規模な組織内で保持しているデータ資産を即時に探索、管理、保護を可能にし、データ資産の理解を支援します。Cloud Composer 一般提供 は、マネージされたApache Airflowサービスにより、複数のクラウドにわたるワークロードのオーケストレーションを支援します。AIと機械学習AI Platform  ベータ提供 はデータの準備、機械学習モデルの構築、実行、そして管理を一つのインターフェースで統合し、MLプロジェクトを管理可能にします。AutoML Natural Language によるカスタムエンティティ抽出とセンチメント分析 ベータ提供 により、入力テキストからカスタムフィールドを特定および単離し、また、非構造化データにおいて各業界に特化したセンチメント分析モデルのトレーニングや実施が可能になります。AutoML Tables ベータ提供 により、構造化データを基に予測的なインサイトを導きだすことが可能になります。モデリング用のデータはBigQuery、Cloud Storage、およびその他のソースから取り込むことができます。AutoML Vision object detection ベータ提供 は、画像内の複数オブジェクトを検出するとともに、オブジェクトの場所を特定するためのウンディングボックスに関する情報を提供します。AutoML Vision Edge ベータ提供 はエッジでの高速、高精度なモデルのデプロイと、ローカルデータに基づいたリアルタイムのアクションを可能にします。AutoML Video Intelligence ベータ提供 により、アップロードした動画とカスタムタグ情報を元に、動画中のカスタムタグ情報を検知するモデルをトレーニングできます。このモデルを活用することで、動画へのカスタムタグ付与作業の自動化、特定のカスタムタグを持つ動画の検索が容易になります。Document Understanding AI ベータ提供 は、スキャンまたはデジタルドキュメントを、自動的に分類、抽出及びデジタル化するための、スケーラブルで、サーバーレスなプラットフォームを提供します。Vision Product Search 一般提供 はモバイルアプリケーションにビジュアル検索機能を組み込むことができます。ユーザーは、自身の撮影した商品画像から、それに類似した複数の商品画像を小売店の製品カタログから取得することが可能です。Cloud Vision APIの機能拡張 ベータ提供 により、バッチ予測の実行が可能になりました。またドキュメント上のテキスト検出機能により、PDFのオンライン抽出や、スキャン ラスタライズ されたテキストや、レンダリングされたテキストが混じっているファイルも検出できるようになりました。Cloud Natural Language APIの機能拡張 ベータ提供 がロシア語と日本語にも対応、さらに領収書や請求書のためのエンティティ抽出機能が組み込まれました。新しいTranslation API Vでは、翻訳時に優先して使用するボキャブラリや用語を定義でき、ブランドごとの用語を翻訳ワークフローに容易に組み込めるようになりました。Video Intelligence APIの機能拡張 ベータ提供 によりコンテンツクリエイターは、ビデオ映像内でタグ付けされた項目を検索することができます。このAPIはOCR 一般提供 、オブジェクトトラッキング 一般提供 、および新たにストリーミングビデオへの注釈機能 ベータ提供 にも対応しました。 Recommendations AI ベータ提供 は、顧客エンゲージメントとビジネス成長を加速するため、小売企業が顧客毎にパーソナライズした対のレコメンデーション体験を提供することをサポートします。Contact Center AI ベータ提供 は、コンタクトセンターにおけるモダンで直観的なコンタクトセンターにおけるカスタマーケア体験をサポートします。GCP上のWindowsワークロードMicrosoftのワークロードについては、Google Cloudからオンデマンドでライセンスを購入できます。加えて、お手持ちのライセンスをGCP内で使うことも可能になりました。ストリーミングマイグレーションツールであるVelostrata が近日利用できるようになり、単一テナントノードが必要なMicrosoftのワークロードのみをタグ付けし、自動的に既存のライセンスを適用できるようになります。Microsoft Active Directory AD のマネージドサービスが近日利用可能となります。お客様のAD依存のワークロードを管理するために、高可用性を備え、Google CloudサービスとしてハードニングされたMicrosoft ADはADサーバーのメンテナンスとセキュリティの設定を自動化し、お客様のオンプレミスのADドメインをクラウドに拡張します。フルマネージドなリレーショナルデータベースサービスであるCloud SQLがMicrosoft SQL Serverをサポートします。また、AnthosにおけるMicrosoft環境のハイブリッド環境へのデプロイメントもサポートする予定です。生産性とコラボレーションG SuiteG SuiteとGoogleアシスタントの連携 ベータ提供 ができるようになりました。次のミーティングの時間と場所を把握でき、スケジュール変更もすぐに確認できます。G Suiteアドオン 近日ベータ提供 により、使い慣れた業務アプリケーションへのアクセスを、複数のアプリケーションやタブを切り替えことなくG Suiteのサイドパネルから行えるようになります。Cloud Searchのサードパーティ接続 一部のお客様に一般提供 により、サードパーティのデータソースも含めた企業内のデジタル資産や人材を検索、そして発見することが可能になります。Drive metadata ベータ提供 G Suite管理者 代理の担当者も含む は、検索でコンテンツを見つけやすくするためにメタデータカテゴリや分類を作成できます。Hangouts Meetのアップデートには、自動字幕 一般提供 、ライブストリーミングを「一般公開  Public  」にする機能 近日提供 、参加者上限数  名 の引き上げ 近日提供 が含まれます。Google Voice for G Suite 一般提供 は、企業が場所やデバイスを問わずに使える電話番号を提供します。Google AIを用いて、ボイスメールの書き起こしや、スパムコールのブロックを行うこともできます。Hangouts Chat into Gmail ベータ提供 を使用することで、チーム内のコミュニケーションをGmail画面の左下区域で簡単に行えるようになります。また、その場所には参加者、ルーム、ボットが表示されます。Googleドキュメント、スプレッドシート、スライドにおけるOffice編集機能 一般提供 を利用することで、Officeファイルのフォーマットを変換することなく、G Suiteから直接編集することができるようになります。Googleドライブでのビジター共有 ベータ提供 は、PINコードを持つ社外の人を招待し、G Suite内のファイルをG Suiteアカウントを持たないユーザーに共有することができます。Currents エンタープライズ向けGoogle の新しい名称  ベータ提供 は、従業員が役職や地理的距離の壁を越えて他の従業員と知識を共有し、実りある議論を展開できる場となりました。アクセスの透明性 エンタープライズ用G Suiteのお客様向けに一般提供 は、Google Cloudの従業員がサポート目的でアクセスするデータの詳細な可視性を実現します。データリージョンを強化し、バックアップデータはの対応と対象アプリケーションが追加されました。フィッシングと不正なソフトウェアに対する高度な保護機能 ベータ提供 を利用することで、管理者は異質な添付や、Google Groups内にあるドメインになりすましたインバウンドメールに対して、安全性を確保します。G Suite向けのセキュリティセンターとアラートセンターのアップデートにより、管理者が脅威に対処できる統合的な改善策を実施できるようになりました。ChromeGoogle管理コンソール内で利用できるChrome Browser Cloud Management を使用すると、Windows、Mac、Linux環境のブラウザーをか所で管理できます。使用中のブラウザーを表示し、各ポリシーの設定と適用を、つの場所から設定できます。Chrome Browser Cloud Managementは、すべてのエンタープライズ向けにオープン化されており、他のGoogle製品を利用していないお客様でも利用可能です。お客様最新情報 「 Customer Voices book」では、つの主要産業にわたる社のGoogle Cloudのお客様からの声を紹介しています。Australia Postは、Google Cloudを利用しオンラインと対面での接客の両方において、いかにお客様にサービスを提供するかを詳しく紹介しています。Baker Hughesは、Google Cloudを使用して、複雑な産業規模の問題を解決する、高度なアナリティクス構築の事例を示しています。Colgate Pamoliveは、同社がどのようにG Suiteを活用しているかを紹介するとともに、GCPを使用したデータ分析ならびにSAPワークロードのGoogle Cloudへの移行によってどうビジネス変革を実現しているかを紹介しています。Kohl sは、今後年間でアプリケーションの大部分をクラウドに移行する計画を紹介しています。Fortuneランキングで位のMcKessonは、共通のプラットフォームとリソースにより、顧客や医療業界にさらなる価値を提供するという目標を示しています。Procter  amp  Gambleはデータの保存、分析、アクティブ化にGoogle Cloudを活用する事例を紹介しています。Unileverは、より迅速なインサイトの創出や顧客ニーズをより深く理解するために、翻訳、ビジュアル分析、自然言語処理 NLP などのGoogle Cloud AIツールを活用しました。UPSは、Google Cloudの分析ソリューションを活用して毎日億件以上のデータポイントを収集、解析していると述べています。Viacomは、ペタバイト以上のコンテンツの自動タグ付け、検出、インテリジェンスにGoogle Cloudを採用した理由を説明しています。Whirlpoolは、G Suiteを活用して従業員のコラボレーションを根底から変革しています。Wixは、自社のCorvid開発基盤により、デベロッパーがコラボレーションを通して、高度なウェブアプリケーションを、より迅速かつスマートに構築できるよう支援しています。Nextでは、デベロッパーがCorvidとDialogflowを使ってわずか秒足らずでチャットボットを構築するデモを行いました。パートナーシップCisco、Dell EMC、HPE、Lenovoのパートナー各社はそれぞれのハイパーコンバージドインフラストラクチャ上でお客様向けにAnthosを提供します。各社のソリューションスタックでAnthosを検証することによって、Googleとの共通の顧客企業はそれぞれのストレージ、メモリ、パフォーマンスへのニーズに応じてハードウェアを選択できるようになります。Intel は開発者、OEM、システムインテグレーター向けのプロダクションデザインを公開し、Intelの検証済みハードウェアとチャネルマーケティングプログラムを提供して、企業へのAnthosの導入を加速します。VMwareとGoogle Cloudは、Anthos向けのSD WANおよびService Meshのインテグレーションで連携することを発表します。Google CloudとConfluent、MongoDB、Elastic、Neoj、Redis Labs、InfluxData、DataStaxはOSSにおける戦略的なパートナーシップを締結しました。各社が緊密に連携してOSSを中心とした技術をGCPに統合し、管理、請求、サポートのすべてを通じたシームレスなユーザー体験を提供します。Accentureは顧客エクスペリエンストランスフォーメーションの新たなエンタープライズ向けソリューションにおける、戦略コラボレーションの拡大を発表しました。Deloitteはヘルスケア、金融、小売業界向けの変革ソリューションを発表しました。Atos and CloudBeesはGCP上で稼働するあらゆるDevOpsソリューションをお客様に提供するパートナーシップを発表しました。SalesforceはコンタクトセンターのAIをSalesforceのサービスクラウドやDialogflowのエンタープライズ版に取り入れ、Salesforce Einsteinプラットフォームに組み込みます。G SuiteとDropboxの新たな統合により、Dropboxから直接、Googleドキュメント、スプレッドシート、スライドなどのG Suiteファイルを作成、保存、共有できます。Docusign はG Suiteとの統合を拡張するつの新たなイノベーションを導入しました。AIおよび機械学習に関し、Avaya、Genesys、Mitel、NVIDIA、Taulia、UiPathを始めとする多数のパートナー発表を行いました。Google Cloudのつの新しい専門領域でのパートナーがスペシャライゼーションを取得し、今後も多数の企業が取得予定であることを発表しました。Google Cloud 認定マネージドサービスプロバイダー MSP は拡大しています。Next  では、共通のお客様がGoogle Cloudジャーニーの加速を支援できるパートナーを見つけやすくするための、認定パートナー向けMSP Initiativeバッジの導入を発表しました。年Partner Awardsの輝かしい受賞者を発表しました。全リストはこちらでご確認いただけます。最後に、件目の発表です。Google Cloud Next  は、年月日から日の日程で、再びサンフランシスコのMosconeで開催します。皆さまにお会いできるのを楽しみにしています。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/2571403774142495637/comments/default",
      "updated": "2019-04-19 00:03:25"
    },
    {
      "name": "Google Cloud Platform Japan 公式ブログ",
      "category": "GCP",
      "title": "Cloud OnAir番組レポート  Next  サンフランシスコ最新情報GCP特集",
      "description": "Cloud OnAirは、Google Cloudの製品をわかりやすく解説し、最新の情報などをいち早く皆様にお伝えするOnline番組です。年月から月にかけての放送は、サンフランシスコで行われたGoogle Cloud Next  の最新情報を皮切りに、Google Cloud製品をご利用いただいているお客様からより具体的な活用の内容についてお話いただく予定です。さて、月日の放送は、「Next  サンフランシスコ最新情報GCP特集」と題して、Google Cloud Next  で発表された新製品、サービスに関する情報をいち早くお届けする内容でした。日間にわたって行われたイベントでは、以上もの新製品や新機能が発表されました。本番組では、GCPに焦点をあて、つのカテゴリーに分けて、話題となった機能やサービスを解説します。Infrastructure  ITインフラをモダナイズし、最適化するCloud Services Platformの流れを組むAnthosはNext  では最大の関心事でしたが、これ以外にもITインフラに関わるアップデートが数多くありました。ソウル 韓国 とソルトレイクシティ アメリカ のつの新しいリージョンを加えて、世界リージョンとなります。ネットワークの面では、High Availability VPN、 GbpsのCloud Interconnect、Network Service Tiersの一般提供が始まり、Traffic Directorで柔軟なトラフィック制御が可能となります。Cloud SQLがMicrosoft SQL Serverをサポート、Microsoft Active Directory AD のマネージドサービスといったWindowsユーザー向けのサービスやオープンソースエコシステムとの統合も大きなアップデートです。App Innovation カスタマーエクスペリエンスの変革のために新規アプリを開発するアプリケーションの開発生産性やセキュリティに対しても継続的な投資を行っています。その一つ、Cloud CodeはVS CodeおよびIntelliJを拡張し、クラウドネイティブなKubernetesアプリケーション開発環境を提供します。また、新しいサーバーレスコンピューティングプラットフォームであるCloud Run を発表しました。Cloud FunctionsとApp Engineのプラットフォームにも新たな投資をしており、これには新しい第世代ランタイム、新しいオープンソースのFunctions Frameworkなどが含まれます。Insights from Data スマートアナリティクスによってデータから洞察を得るビッグデータ分析をさらにシンプルなものとするために、データのサイロ化の解消、より高度な分析を手軽にする、そしてデータガバナンスの観点からも多くのアップデートがありました。Data Fusion ベータ提供 はコーディング無しでフルマネージドのクラウドネイティブなデータインテグレーションを行います。以上のSaaSアプリからBigQueryへのデータの転送も可能となりました。Connected SheetsによってBigQuery分析をスプレッドシートから行えます。BigQuery ML 近く一般提供予定 は顧客のセグメンテーションやレコメンデーションを行います。BigQuery BI Engine ベータ提供 はインメモリの分析サービスで、膨大かつ複雑なデータをほぼ瞬時に可視化します。Cloud Data Catalog ベータ提供 はフルマネージドのメタデータ探索とメタデータ管理のプラットフォームです。AIと機械学習に関連するアップデートとしては、Contact Center AI ベータ提供 、Document Understanding AI ベータ提供 、Retail oriented solutionsといったソリューションやAutoMLのアップデートも発表されました。さらに、AI Platform ベータ提供 は、MLプロジェクトを管理可能とします。Security  amp  Trust 高度なセキュリティが組み込まれた信頼できるプラットフォームGoogle Cloud PlatformからG Suiteまで、以上のセキュリティ関連の発表がありました。特にGCP関連では、Cloud Security Command Center 一般提供 、Access Approval ベータ提供 、新たなEvent Threat Detection ベータ提供 、Policy Intelligence アルファ提供 、Virtual Private Cloud  VPC  Service Controls 一般提供 、Shielded VMs 一般提供 があります。セキュリティは、Google Cloudが提供するすべての製品、サービスの要であり、今回、大幅な拡充が図られました。年月日放送 nbsp Next  サンフランシスコ最新情報GCP特集番組で説明した資料はこちらで公開しています。    Cloud OnAir  Next  サンフランシスコ最新情報GCP特集年月日放送  from Google Cloud Platform   Japan Cloud OnAirでは、各回Google Cloudのエンジニアがトピックを設け、Google Cloudの最新情報を解説しています。過去の番組、説明資料、さらには視聴者からの質問と回答はこちらよりご覧いただけます。最新の情報を得るためにもまずはご登録をお願いします。",
      "link": "http://cloudplatform-jp.googleblog.com/feeds/914148855664954289/comments/default",
      "updated": "2019-04-18 23:49:06"
    }
  ],
  [
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "A small device that makes a world of a difference",
      "description": "Editor s note  Modoo is a China based startup that has created the smallest fetal monitoring patch in the worldーand took home the Judge s Choice award at the Google for Startups Asia Demo Day in Bangkok last week  Their founder shares his story of the company  If I had to sum up my approach as an entrepreneur  it s that I simply want to use technology to help people  I d spent years developing technology for fun  or for leisure  And then the unthinkable happenedーa very good friend of mine lost her baby just two days before her due date  She was young and healthy  and they had no reason to think that anything could go wrong  It was heartbreaking  Seeing her go through that experience made me want to learn more about what was available for expectant mothers  I didn t find much  I remember thinking  “Wow  I m an engineer developing cool gadgets  but there s no technology to help expectant mothers through probably the most anxiety ridden stage of their lives   I wanted to do more  I founded Modoo in   We created the smallest fetal monitoring patch in the world to help mothers to be monitor their unborn baby s movement and heartbeat anytime  It connects with an app that provides information and health advice  and through AI  we can detect complications early  to prevent fetal distress  I was shocked when they called out Modoo as the winner of Google for Startups Demo Day in Bangkok last week  Demo Day brings together talented entrepreneurs giving them the opportunity to shine  connect  and realize visions to solve big problems  I d spent a few days with the  other teams  and saw how they had achieved amazing progress and made a huge impact on society  I remember thinking it would be tough for the judges to make a decision From left to right  Jeffrey Paine  Golden Gate Ventures   Shannon Kalayanamitr  Gobi Partners   Jilliang Ma  Founder  Modoo   Justin Nguyen  Monk s Hill Ventures  and Michael Kim  Google for Startups But then I thought about our journey and the impact we ve made  We ve already served  mothers and given them much needed peace of mind  And with the help of early detection of fetal distress through our technology  we ve helped save the lives of  babies  The mission to save lives and make life better is what drives my team and me  What s next  We want to take our product to more parts of the globe  and are looking at ways to help mothers monitor their health postpartum  as well as ways to monitor the health of babies and young children  Google for Startups Demo Day reaffirmed my passion  and gave me a platform for more people to learn about the work that we re doing  It was also inspiring to meet other entrepreneurs from all over the region  who are similarly passionate and mission driven  Our ideas will change the world  ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/vvgiC_an83w/",
      "updated": "2019-07-05 04:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Table Stakes Europe  a program to help local journalism thrive",
      "description": "Editor s note  As part of the Google News Initiative  we work with news publishing partners across the world on efforts to help the industry thrive in the digital age  The following post comes from one of our partners Vincent Peyregne  CEO of WAN IFRA Trust  democracy and civic engagement often take root within communities and neighborhoods  Local news plays a critical role in this process  and high quality and financially sustainable local journalism is indispensable for local communities to thrive Yet  unlike global news brands  local and regional newspapers don t haveーand can t realistically growーaudiences beyond the geographies in which they operate  which makes it challenging to keep up with the changing nature of digital journalism  WAN IFRA and the Google News Initiative are joining forces to launch Table Stakes Europe  a program to help local and regional newspapers find new ways to build local audiences  prosper in a digital world and perform their crucial role in society  Started in  in the U S   our vision with Table Stakes is to show how local changes make a global impactTable Stakes Europe will build upon the proven Table Stakes approach  plus coaching methodologies that have helped dozens of local news organizations in the U S  improve their audience and digital capabilities and results  The Program is designed to help small and medium local and regional newspapers in Europe transform their business  increase consumer based revenue  and build digital capabilities The program will begin in October   and run for  to  months  We expect at least  small and medium local and regional news enterprises to participate from a variety of countries and backgrounds  Small and medium local and regional newspaper organizations can apply to the program by September st WAN IFRA and the Google News Initiative are excited about bringing this opportunity to local news organizations in Europe and are looking forward to sharing the lessons and best practices with the industry at large throughout and at the end of the program ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/Lgo09AR6bhs/",
      "updated": "2019-07-04 11:36:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "ICYMI  G Suite in   so far",
      "description": "It s been a busy year for G Suite  Gmail celebrated its th birthday  and we launched a slew of updates at Google Cloud Next    For a recap on what s happened in G Suite this year thus far  read on Communication is key Time flies  Earlier this year  we celebrated Gmail s th birthday  To commemorate the occasion  we introduced new features in your favorite email to help you write emails faster  with the help of machine learning   and also made it possible to schedule when your emails go through to colleagues  Gmail also got more dynamic so that you can take action straight from within your inbox  like resolving Google Docs comments  Lastly  we gave Gmail s mobile interface a good sprucing upーhello gorgeous Besides updating Gmail  we also brought businesses a secure  and intelligent  way to communicate no matter location or device  Google Voice for G Suite  Built in the cloud  Voice for G Suite is smart enough to transcribe voicemails for you and block pesky spam calls  Say goodbye to lengthy conference bridge numbers Take home tip  While we re on the subject  if you accidentally click “send  on an email that you didn t mean to  you can recall it by clicking “undo  at the bottom of your inbox  It appears after you ve sent an email and stays on your screen for up to  seconds before disappearing  You can choose the length that it appears in your settings Putting the team in “teamwork   People talk all the time about how collaboration is key  but nearly everyone defines collaboration differently   More on that in this post   Here s our take  we think software can only be called collaborative if your tools are easy to use and if they help people stay productive  It s called “teamwork   after all In G Suite  we re focused on making our apps intuitive and intelligent so that people can accomplish things quicker  That s why we recently added things like intelligent grammar suggestions in Docs  But we think all work tools should integrate together easily  no matter if they re Google apps or apps outside of G Suite  That s why we introduced ways to comment on Microsoft files and beefed up integrations with Dropbox Take home tip  Keyboard shortcuts are a life saver  If you re working in a Google Sheet  and want to add a comment quickly  type Ctrl   Alt   M We re always making updates like these to make G Suite more useful for you  Keep track of the latest on our website  and stay tuned for more recaps like this in the months to come ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/4XZR9Po3Ppg/",
      "updated": "2019-07-03 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "From the kitchen to the factory  Three surprising places you ll find Chrome Enterprise",
      "description": "We talk a lot about how cloud native devices like Chromebooksーwhich automatically update and store your work in the cloudーcan help you stay productive at work  What you might not know is that businesses are using these devices in unexpected ways to keep their organizations on track  whether it s cooking your favorite dish  making the factory floor more efficient  or bringing devices to ambulances for on the go emergency care  Here are three surprising examples of enterprises using Chromebooks and Chrome Enterprise to help employees be better trained  better informed  and better connected Panda Restaurant Group  cooking your favorite dishes perfectly  every timeWhen Panda Express customers order their favorite mealsーsuch as the Original Orange Chicken  Broccoli Beef  and KungPao Chicken Breastーthey expect the dish they know and love to be the same each time  That s why in almost  locations  Panda Restaurant Group associates train employees on these recipes with the help of Chromebooks  “Chromebooks make it easier for new associates to complete e modules and onboarding   says Clark Yang  a training leader at a Panda Express in Los Angeles Royal Technologies  bringing the factory floor to the cloud  some assembly requiredTo give manufacturing managers on the factory floor an easier way to check parts quality  manufacturing company Royal Technologies placed Acer Chromebase CAI devices on shop floors  As parts come off the assembly line  managers compare them to images on the Chromebase screen  then pack the parts for shipping  They can even generate quality check reports by filling out a Google Form on the Chromebase and sharing results with customers via Google Sheets  “We re not only keeping parts quality high   says French Williams  IT Manager at Royal Technologies  “we re also communicating more closely with our customers  Middlesex Hospital  equipping paramedics with tools to help patients  fastWhen you re treating a person who needs critical care  time is precious  Middlesex Hospital paramedics understand this intimatelyーthey receive more than  emergency calls each year  The hospital uses Chromebooks to respond to these calls quickly  so that treatment can start within seconds after arriving on the scene  “Even though they re built to withstand rough handling  they don t weigh down our emergency packs   says Jim Santacroce  Manager of Emergency Medical Services at Middlesex Hospital  of their Chromebooks  “Their battery life keeps pace with our long shifts  It takes no time at all to learn how to use Chrome OS on Chromebooksーand when you open them  they boot up almost immediately  We don t need to watch the minutes tick away while we wait to open a patient recordーespecially in the high pressure emergency environment  These are just a few examples of how businesses from different industries are using Chrome Enterprise to increase productivity  inspire collaboration  and better serve their customers  To find more stories like these  visit Chrome Enterprise on the Google Cloud blog ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/BLA3b8NoWSI/",
      "updated": "2019-07-03 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Hit the sauce  What barbecue lovers are searching for across the U S ",
      "description": "Hot dog  It s BBQ season  If the increasing number of dads in aprons and bucket hats doesn t give it away  an uptick in Google searches for “sauce  certainly has  We just couldn t keep this information bottled up  so let s ketchup on the uniquely searched sauces that reveal each state s taste for BBQ  Here s what the U S  is relishing this summer Keeping tradition aliveBarbecue is America s summertime fuelーit s what gives us the energy to swim all day and stay awake for fireworks at night  And when it comes down to it  basic BBQ is delicious on its own without the bells and whistles  More than  percent of states agreed by opting for traditional barbecue sauce recipesーand  of those states were willing to go the extra mile by preparing homemade concoctions  searching for “homemade bbq sauce  the most  As my mother would say  “if it ain t broke  don t fix it  Fixin  to eatSpeaking of tradition  everyone knows that southerners take their barbecue seriouslyーand have a serious sense of pride  Nearly  percent of U S  states searched for “white  barbecue sauce recipes  also known as Alabama white sauce  You know what they say  mayo for dayos  But six states  and DC  preferred what s known as Carolina sauce  including the Carolinas  naturally   where they re just searching for “vinegar sauce  Shaking it upSome states are bored with standard barbecue sauces and are looking to mix things up  Take Colorado and Minnesota  who are hankering for korean style sauce  Folks in Florida are craving tropical  guava flavor  Our New Jersey friends are in the market for more meat  how much bacon is too much bacon    And in Illinois  some families are looking to spike  oops  I mean spice up their barbecue by adding bourbon  But the award for the most original barbecue sauce flavor goes to Hawaii  where …get this …they ve been searching for “Hawaiian  bbq sauce Slim pickingsWhile many are looking for a new flavor this year  a good chunk of the U S  will be passing on the potato salad and opting for healthy options around the grill with searches for diet friendly recipes  Here are the highlights  Iowa and Texas are looking for sugar free alternatives  while folks in Arizona  Michigan  Missouri  Oregon  Washington and Virginia are on the Hunt  s  for keto friendly sauces  New Hampshire and Nebraska are cutting back by finding “low carb  options  while Massachusetts is taking it one step further by searching for “calorie free   Uh  good luck there  MA So for anyone who s standing over the grill this week  wondering what to slather on  there is no right answer  just freedom to choose from a vast array of options ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/A-tueB-SGq0/",
      "updated": "2019-07-03 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Cloud Covered  What was new in Google Cloud in June",
      "description": "Summer s here  but that s not the only big thing that arrived this month  We welcomed new partners and have new data technology related updates to share  Read on for noteworthy Cloud updates in June Google will acquire Looker We announced our intent to acquire Looker  a company that offers business intelligence  data application  and embedded analytics software  These kinds of tools are important for modern businesses  so they can use all the data they have available to make decisions  and see visualizations of data with graphics and charts  Looker will help extend the analytics tools we already have at Google Cloud  It will let users at businesses define which metrics to use across different sources of data  so everyone can see the same information  Plus  Looker will bring strong analytics technology that can be embedded into other applications that a business is using A cloud data warehouse partner came on board Yes  data analytics is a big topic at Google Cloud these days  If you think about your own data useーphotos and music  for exampleーyou can start to get an idea of how much data exists for businesses  They want to analyze that data to understand more about what s important to their businessーbut the sheer volume of data makes it challenging  Cloud data warehouses help to store  manage  and analyze all that data to find useful information  This month  we announced a partnership with data warehouse provider Snowflake  which will give people another way to bring in a wider variety of data sources to Google Cloud  then use and analyze all that data School s out  but the learning never stops We announced a new Google Cloud certification challenge in June  Study  take an exam  get certified within  weeks  and you ll get a   Google Store voucher  These certifications are becoming more important as employers recognize how important cloud skills are to their businesses  and there are different levels of certifications based on experience level  Plus  we announced some new Qwiklabs questsーself paced labs designed to educate cloud developers  The quests help developers understand what metrics can be observed using Kubernetes technology and how to use it securely to deploy containers in real life Cloud native is an important new concept “Cloud native  means that systems that provide business users with the tools they need  like email  applications and more  have been designed from scratch to best take advantage of the unique capabilities of the cloud  This is different from moving existing technology systems to the cloud without making a lot of changes to the way they re set up  sometimes known as “lift and shift   We shared how building cloud native systems is different from traditional approaches  and how businesses should plan with those differences in mind G Suite added even more security controls With the help of machine learning  Gmail already blocks up to  percentof spam  phishing and malware from ever reaching your inbox  This month  we added even more features in Gmail to further protect businesses  including a security sandbox  which lets IT teams analyze email attachments that might be malicious  and sophisticated machine learning to protect emails from malware and phishing  Check out this post to learn more about other G Suite happenings Blockchain and cloud can work together Blockchain is a technology usually associated with cryptocurrency  but it can also be used to make applications more secure  This post about building applications that use both cloud and blockchain explains how to get the most out of blockchain s capabilities and the efficiency of cloud  Using this combination of cloud and blockchain can help make transactions private  and enable more accurate predictions That s a wrap for June  To find more stories  visit the Google Cloud blog ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/EwSC0Kj1PXE/",
      "updated": "2019-07-03 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "How Google volunteers gave me the confidence to get hired",
      "description": "Editor s note  Every June  Google org hosts a month long campaign called GoogleServe to empower Googlers to volunteer in their communities around the world  This year  more than  Googlers participated in GoogleServe  and Googlers have volunteered over  hours since the campaign first began in   Today we hear from Brian Evans  a past attendee at one of our GoogleServe  amp  Goodwill resume and interview prep events  In June   I was just out of prison  I was looking for a job  but I had a criminal record hanging over my head  I did my time in the eyes of the state and had served my sentence  but others don t always see it that way  Employers often have a bias against people like me  I needed work  but wasn t getting it   I found Goodwill NOW  a program that works with people who have criminal histories to help them find full time employment  People with the program suggested I sign up for a GoogleServe event  where Googlers were going to help with resume building and interview practice  I remember walking into the event  I was really nervous  I didn t think it was a group I could fit in withーa bunch of highly educated people who had tech jobs with Google  But my perspective changed soon after I arrived  They started the event with an “ice breaker  where we were invited to simply get to know each other  I asked things like where they were from  what they do in their free time  what they care about  It made them human and relatable  After the ice breaker it didn t feel as intimidating  It just felt like a bunch of people in a room  with more commonalities than we all knew at the start  Then we got to work  We sat down with Googlers and learned about what they were looking for in a resume  Many of us didn t have a college education and feel that it really sets us back  but the volunteers were able to help us break down our experience and highlight what things stood out to them  Your resume is your first impression  so it really helped to have someone coach us on how to make it stronger and highlight what they found interesting about us  Next  we did mock interviews  We pretended as if we were going in to interview at Google  We learned about the handshake  how to make eye contact and how to make a good first impression  We learned about the kinds of questions we should be prepared to answer and what kinds of questions we should be prepared to ask  Before the clinic I didn t know how important it was to ask the interviewer questions  too  After the clinic  I felt inspired  So I signed up for school and studied peer mentorship and criminal justice  Goodwill was impressed by my education pursuit and they invited me to apply to be a resource room coordinator  Before getting the job I had to interview with the VP of Goodwill  Never in my life have I been put in front of someone with so much power  Then I remembered what a Google volunteer said  “Just find a way to relate to your interviewer  find something in common   They hired me  Since then  I ve been promoted twice in one year and I m now the lead peer mentor  This year  I helped to organize the same GoogleServe event that helped me when I was looking for a job  We can all succeed if we have ambition  I didn t have schooling and lots of Googlers do  but the most inspiring thing a Google volunteer told me is that I had a chance  I believed them and put one foot in front of the other  made it through school and went into the interview with confidence  And here I am ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/pfZ_qmt4P4Q/",
      "updated": "2019-07-02 21:16:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Three mindsets to navigate ambiguity as the world changes",
      "description": "Consider medieval maps  Back then  the world didn t know what existed beyond the horizon  Would you drop off the edge of the earth  Did unknown sea creatures lurk in these uncharted lands  When faced with the unknown  most people resort to fear  mapmakers depicted fearsome sea creatures on the outskirts of the world  But it s only when you steer the proverbial ship past the edge of what is known that you uncover all that could be  Today  advances in technology  like self driving cars and computers we can converse with  catapult us to the edge of the mapーthe line between the known and unknown  Innovators need to be able to solve for problems of tomorrow  and navigate all the ambiguity that comes along with that  To thrive on this edge  we have to stay curious  empathize with different perspectives and experiment with solutions   Embrace a curious mindset  Approach the unknown with curiosity rather than fear  The wildest questions can create the biggest opportunities  A phrase I embrace to shift myself into a mindset of curiosity is “What if…  Take voice powered assistants for example  Just recently  millions of people started having conversations with their devices  completely changing how they interact with technology  Initially  there was some uncertainty  People questioned things like the utility  the security and the effectiveness of voice powered technology  However  if you lean into curiosity and consider new possibilities  rather than pitfalls  then you can work through pending challenges more effectively  For example  you might ask  What if we had an assistant that could help us with everyday tasks  What if people no longer had to type to interact with technology  What if you could have a natural conversation with your computer  What if we had the capability of  assistants at our fingertips  This phrase can help spark optimism and fuel innovation Take multiple perspectives Once you ve embraced a curious mindset  it s time to start solving  Great solutions require empathy  or a walk in someone else s shoes  I ve found that the fastest track to empathy is to focus on the user  I recently taught a Stanford University class on inclusive product design  The class was made up of Stanford students  Googlers and students from the School of the Blind in Fremont  CA  Inclusive design demands that designers use the diversity of their users to challenge what s possible  so to give all of the students a better understanding of the challenges they might solve for  we took part in a blindfolded breakfast  While it was no substitute for truly understanding the challenges of impaired vision  it helped to shift the perspective for students who had never experienced impaired vision and gave them an opportunity to empathize with the people they were designing for  Similarly  having a diversity of backgrounds  ability  upbringings and more on your team helps you to collectively see multiple perspectives  Another tool that helps you look at problems from new angles is what we call the “Why How Ladder   For example  you might begin with a problem statement like “How might we help more girls pursue STEM careers    and then ask “Why is that important   This question helps you think about the bigger picture  Conversely  the question “How might we accomplish that   helps to narrow the scope of the problem  The road to success is paved with experimentationOnce you start to see the challenges as possibilities  you can get into the process of experimenting  More than a decade ago  I was teaching English to a group of six year old children in Shanghai  I found myself staring at a room of  kids who did not speak English  and I did not speak Chinese  But I knew that if I got it wrong on the first attempt  I could just try a new tactic  So I started experimenting with ways to teach the kids a whole new language  After a few fumbles  we started singing songs  reciting the alphabet and drawing pictures to share simple personal stories  In order to take action and thrive in this scenario  I had to be fearless in taking actionーeven if that action resulted in a failed attempt  Failing is all about getting feedback early on  The world is changing faster than ever before  and innovation is inherently ambiguousーwe simply don t know what the future is going to look like  But with these three mindsets you can navigate the waters of ambiguity and steer past the horizon of what is known to what is next  ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/tZTtUxupmMY/",
      "updated": "2019-07-02 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Next steps for enterprises transitioning to modern Android management solutions",
      "description": "Android Enterprise is the modern solution for managing devices that employees use for work  so that they can have flexibility while remaining productive  Android Q will be an important milestone for organizations transitioning from Device Admin based management to Android Enterprise s advanced management features  such as separation of work and personal data through the work profile  quicker enrollment  and tools like managed Google Play What s changing in Android QWhen the final release of Android Q is available  the following APIs that were marked as deprecated in Android Pie will be removed entirely  password enforcement  disable camera and disable keyguard  The exact impact will vary for devices depending upon which Android API level the Device Policy Controller  DPC  targets  Here are some details On devices targeting Android Q  both admins and users won t be able to use the features tied to the removed APIs On devices targeting Android  Pie  affected APIs will show in the device logs  although users won t see any specific notifications On devices that run Oreo or below  there will be no impact  How to prepareWe have several resources to help organizations make this transition as smooth as possible  The Android Enterprise Migration Bluebook is a guide for IT managers who want detailed steps and best practices for moving from a legacy Device Admin deployment to Android Enterprise  This walkthrough video also outlines many of these key concepts for this transition  We also recommend reaching out to your organization s EMM provider for additional guidance on migration ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/W1uW3IZwzhI/",
      "updated": "2019-07-01 17:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Breaking ground in Nevada",
      "description": "I m a fourth generation Nevadan on both sides of my family  Even though Google is headquartered in California  my work has brought me back to my home state of Nevada far more than I expected  And recently we ve been getting to know Nevadans in all corners of the state  Last year  I had the chance to kick off Grow with Google in Reno where we held in person digital skills trainings for hundreds of Nevadans  And last week  our team returned to host more workshops in East Las Vegas and Carson City  Today  I m back home in Nevada once more to break ground on our newest data center and Google Cloud region  Google is growing at a faster rate outside of the Bay Area than in itーand earlier this year  our CEO Sundar Pichai announced that Google will invest   billion to expand data centers and offices across the United States  Recently  we released plans for expansions in two new offices in Michigan  our data centers in Oklahoma and Texas  and now one in Henderson  Nevada  The new data center facility is a   million dollar investment  and will create a number of new jobs in the state  Together with our new cloud region  we re investing to better support our users and our Cloud customers in Nevada Data centers power your searches  store your photos  documents and emails  and help you find the fastest route to your destination  They play a vital role in our global operationsーand the communities they re a part of  And it s our responsibility to be a helpful presence in those communities by creating opportunities for our neighbors to succeed  In addition to the data center groundbreaking  we re kicking off a   million Google org Impact Challenge in Nevada  Nonprofits from any part of Nevada can submit their biggest and boldest ideas to create economic opportunities for their communities  A panel of judges will select the top five submissions from local nonprofits  who will each be awarded   to make their idea a reality  From there  the public will vote to select one of the five to be the “People s Choice  winner and receive an additional    Nevada holds a special place in my heart  and I m proud to bring the Google org Impact Challenge to my home state  As we break ground on our newest data center  we re not only investing in a facility in Nevada  we are investing in Nevadans in all corners of the state  ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/Bgpqyg9gGvs/",
      "updated": "2019-07-01 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Have a laugh with the Assistant this International Joke Day",
      "description": "Picture this  you re in a dim room packed full of people  Suddenly it s your turn to go on stage and spontaneously create and act out a storyーcomplete with characters  jokes and dialogue  While this would terrify most people  as an improv comedian  this is actually one of my “happy places   I m also one of the team members that infuses humor and personality into the Google Assistant  so naturally International Joke Day is my other happy place If you re looking to get in on the funーor prep for your next actーjust ask your Assistant “Hey Google  tell me a joke   Here are some of our favorite responses you ll get across the globe Australia“I once fell in love with an encyclopedia  I was completely infactuated  Brazil“Por que os professores ficam felizes nas festas de Dia das Bruxas   Porque hábastante espírito escolar  Translation  “Why are teachers happy on Halloween  Because there is a lot of school spirit Canada“Which Canadian dessert has the biggest antlers  The chocolate moose Egypt“أمي،هلالمحيطالهادئ،هادئدائمًا؟‍️هل يمكنأنتسألسؤالًاجدّيأكثر،ياكريم؟طيّب،متىماتالبحرالميّت؟ Translation  “Mom  mom  is the Pacific Ocean calm all the time  ‍️Can you ask a serious question  Okay  when did the Dead Sea die    Editor s note  The Pacific Ocean in Arabic is known as the “calm ocean  France“Si vous vous lancez dans le commerce  je vous conseille la vente de bateaux  Votre entreprise sera immédiatement àflot  Translation  “If you ever start a business  I recommend trying to sell boats  You wouldn t want to miss the boat Germany“Gehen zwei Hunde in der Wüste spazieren  Meint der eine zum anderen  „Wenn nicht bald ein Baum kommt  passiert ein Unglück    Translation  “Two dogs are walking in the desert  One dog says to the other   If we don t see a tree soon something bad is going to happen    Great Britain“How did the vacuum cleaner die  It bit the dust  India ডাক্তার  আপনারঅ্যাপেনডিসাইটিসেরসমস্যাহয়েছিলোকখনো  রোগী  হ্যাঁডাক্তারবাবু  ছোটবেলায়স্কুলেথাকতেখুবসমস্যাহয়েছিল  বানানকরতেগিয়ে Translation  “Doctor  Did you ever have a problem with Appendicitis  Patient  Yes  Doctor  When I was young  at school  I had a big problem    writing the spelling for it Indonesia“Kenapa ikan hidup di air asin  Karena air lada akan membuat ikan bersin  Translation  “Why do fish live in the salty water  Because peppery water makes fish sneeze  Japan“旅の出発地にピッタリな島があるのを知ってますか どこかって   「イースター島」ですいいスタート、だから Translation  “Did you know that there is an island that is perfect for beginning a journey  You want to know where it is  It s  Easter Island Because it has a good start  Editor s note  The sound of the words “a good start  is “Ii sutarto  in Japanese which sounds like “Isutar To   the name of Easter Island Mexico“¿Quéhace Pepito golpeando un reloj  Estámatando el tiempo  Translation  “Why is Pepito hitting a clock  He s killing time  Singapore“I really need to stop making jokes about prawns  It s making me hungry sia  Editor s note   Sia  is an expression in Singapore  to show excitement or exaggeration  Additionally  it s a play on the sound for the word for “prawns  in Chinese  which is one of the many languages spoken in the country Spain“Profe  ¿me castigaría por algo que no he hecho  No  Jaimito  ¡por supuesto que no  Québien  porque no he hecho los deberes Translation  “Teacher  would you punish me for something I haven t done  No  Jaimito  of course not  That s great because I haven t done my homework Taiwan“蘋果和梨子在比賽 看誰先講到自己名字就輸了 梨子興奮地說 「來啊」 梨子就輸了 Translation  “Apple and Pear are having a contest    whoever says their name first will lose  Eagerly  Pear says  Bring it on   and ends up the loser   Editor s note  Pear s comment sounds like  pear   the fruit  in Taiwanese U S “Why won t the shrimp share its treasure  Because it s shellfish  Vietnam“Béđi học về  khoe với bố  Bốơi  con làngười khoẻnhất lớp đấy  Tại sao con nghĩthế  Tại vìcôgiáo bảo con rằng  chỉmột mình con màkéo cảlớp tụt lại đằng sau  Translation  “A kid looked very proud when speaking to his father after school   Dad  I m the strongest one in my whole class    That s great  but how did you know that   said his father   My teacher told me today that I alone have dragged the whole class down to the bottom position this month    ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/azUgl4OnqQU/",
      "updated": "2019-07-01 13:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Get to insights quicker with Data Studio s new home page",
      "description": "It should be easy for everyone to discover and share insights from their data  As of today  Data Studio has a new home page  making finding and creating reports more efficient  Together with recent improvements to chart interactivity and sharing  and the   Data Studio features launched this year  you can go from data to insights to action faster than ever Create in a snap with a streamlined home pageData Studio s clean new look puts the focus on what s most important  so you can start digging into your data right away  We ve heard that you frequently use search to find reports  so we put the search box front and center  You can also create a Report  Data Source  or Explorer in less time with the new Create button  Finally  the new design should be familiar to users of G Suite products like Drive and Gmail  and is in line with our material design principles Reveal additional insights with interactive chartsEarlier this year  we introduced the ability to interact with charts to filter other charts on the report page  Since then  we ve brought even more interactivity to charts  If a report viewer wants to sort a chart differently  you can now allow them to do so right from the chart  without needing to edit the report  If they want to see a breakdown at a lower level of detailーfor example  by city rather than by countryーyou can now allow them to drill down right within the chart Explore your data at the speed of thought with BigQuery BI EngineWe also launched support for BigQuery BI Engine  Beta   a super fast in memory engine for interactive visual analysis  Together  Data Studio and BigQuery BI Engine enable you to interact with your data and see results in a fraction of a second Share insights with others through scheduled emailWant to send someone an offline copy or snapshot of a report  You can download a report as a PDF  Want to send someone a report on a regular basis  Automate the task with scheduled email delivery Data Studio s new home page and these new features make it easier than ever to find and create reports  discover insights  and share them with others  ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/VbQgrlb92DM/",
      "updated": "2019-07-01 11:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": " PrideForever  Seven Googlers on the fight for LGBTQ  rights",
      "description": "Earlier this month  we launched Pride Forever  celebrating the past  present and future of the LGBTQ  community by elevating stories from around the world  like the ones from the Stonewall Forever living monument  This interactive digital monument was created by the LGBT Community Center of New York City  “the Center   with support from Google  and it connects diverse voices and stories from the  years since the Stonewall riots to the modern day movement for LGBTQ  rights  Those voices include members of Google s LGBTQ  community  too  In offices all over the world  Googlers are reflecting on their own journeys and sharing their stories with the world  Here s a glimpse of what seven Googlers say pride means to them “For over a decade  I struggled to accept that I could possibly be trans  Then in   Argentina passed its gender identity law the first in the world to allow gender self determination  While far removed from my home in Indonesia  it meant that people like me might finally have a chance at transitioning and living without harmful legal and medical gatekeeping  It gave me the courage to accept myself and start standing up for my right to be   ーJean  Singapore“Two years ago  I was honored to create a Google Doodle for Gilbert Baker  creator of the rainbow flag representing diversity  unity  acceptance and pride  The first flag was made by hand  so I wanted to create a Doodle with the same handmade feeling  I learned to sew  not easy   and recreated the flag in my tiny kitchen just a few blocks from where Baker made his original eight color flag back in   As an LGBTQ  person  the flag and this Doodle were beyond personal to me  and it s part of why I joined the Google Doodle team  in hopes of having opportunities to brighten and strengthen people s days   ーNate  San Francisco“We were both engineers working in male dominated industries where being a lesbian was difficult  We were asked on a regular basis about husbands or why we weren t married  California s Prop  in   banning same sex marriage  was an eye opening moment for us  Although the prop passed  there was a large public opposition campaign standing up for the rights of the LGBTQ  community  It felt like a turning point for people across the United States showing that it was OK to support the LGBTQ  cause without substantial retribution   ーCandace and Michelle  South CarolinaWilliam  left  with his family   Many people have shaped my lifeーbut perhaps the most meaningful people in my life are my husband  whom I have been with for nearly  years  and my son  who gives me more joy  and a fair amount of frustration  than I could have ever imagined  For them  I owe thanks in large part to a valiant handful of New Yorkers whom I ve never met  Their act of defiance at the Stonewall Inn  years ago ultimately enabled me to live  love and be who I am   ーWilliam  New York“When I first came out to my parents  my dad told me I d never get a good job  and I d lose all my friends unless I  changed my mind  about being gay  That really hurtーthat being gay is still seen as different  even to well meaning people  Marriage equality in the U K  in  felt like a huge validation  The fact that this was part of an international wave  it was really a feeling of progressive acceptance   ーNick  London“The original LGBTQ  initialism was created in the late s to introduce a more inclusive name for the gay community  To me  the LGBTQ  acronym represents a diverse group of people that are unique and resilient  I am so proud to be a part of a community that is constantly evolving its boundaries for inclusion and actively championing societal equality  Even though there is still more to be done  being able to lean on one another for supportーno matter where in the LGBTQ  spectrum you fallーbinds us together and has enabled us to make impressive progress across the globe   ーAndrew  Sydney",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/nSQClIJGfds/",
      "updated": "2019-06-28 18:23:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Responsible AI  Putting our principles into action",
      "description": "Every day  we see how AI can help people from around the world and make a positive difference in our livesーfrom helping radiologists detect lung cancer  to increasing literacy rates in rural India  to conserving endangered species  These examples are just scratching the surfaceーAI could also save lives through natural disaster mitigation with our flood forecasting initiative and research on predicting earthquake aftershocks  As AI expands our reach into the once unimaginable  it also sparks conversation around topics like fairness and privacy  This is an important conversation and one that requires the engagement of societies globally  A year ago  we announced Google s AI Principles that help guide the ethical development and use of AI in our research and products  Today we re sharing updates on our work Internal educationWe ve educated and empowered our employees to understand the important issues of AI and think critically about how to put AI into practice responsibly  This past year  thousands of Googlers have completed training in machine learning fairness  We ve also piloted ethics trainings across four offices and organized an AI ethics speaker series hosted on three continents Tools and researchOver the last year  we ve focused on sharing knowledge  building technical tools and product updates  and cultivating a framework for developing responsible and ethical AI that benefits everyone  This includes releasing more than  research papers on topics in responsible AI  including machine learning fairness  explainability  privacy  and security  and developed and open sourced  new tools  For example The What If Tool is a new feature that lets users analyze an ML model without writing code  It enables users to visualize biases and the effects of various fairness constraints as well as compare performance across multiple models Google Translate reduces gender bias by providing feminine and masculine translations for some gender neutral words on the Google Translate website We expanded our work in federated learning  a new approach to machine learning that allows developers to train AI models and make products smarter without your data ever leaving your device  It s also now open sourced as TensorFlow Federated Our People   AI Guidebook is a toolkit of methods and decision making frameworks for how to build human centered AI products  It launched in May and includes contributions from  Google product teams  We continue to update the Responsible AI Practices quarterly  as we reflect on the latest technical ideas and work at Google Review processOur review process helps us meet our AI Principles  We encourage all Google employees to consider how the AI Principles affect their projects  and we re evolving our processes to ensure we re thoughtfully considering and assessing new projects  products  and deals  In each case we consider benefits and assess how we can mitigate risks  Here are two examples Cloud AI HubWith Cloud AI Hub  enterprises and other organizations can share and more readily access a variety of already trained machine learning models  Much of AI Hub s content would be published by organizations outside of Google  which would make it difficult for us to evaluate all the content along the AI Principles  As a result  we evaluated the ethical considerations around releasing the AI Hub  such as the potential for harmful dual use  abuse  or presenting misleading information  In the course of the review  the team developed a two tiered strategy for handling potentially risky and harmful content  Encouraging community members to weigh in on issues like unfair bias  To support the community  Cloud AI provides resources  like the inclusive ML guide  to help users identify trustworthy content Crafting a Terms of Service for Cloud AI Hub  specifically the sections on content and conduct restrictions These safeguards made it more likely that the AI Hub s content ecosystem would be useful and well maintained and as a result  we went ahead with launching the AI Hub Text to speech  TTS  research paperA research group within Google wrote an academic paper that addresses a major challenge in AI research  systems often need to be retrained from scratch  with huge amounts of data  to take on even slightly different tasks  This paper detailed an efficient text to speech  TTS  network  which allows a system to be trained once and then adapted to new speakers with much less time and data While smarter text to speech networks could help individuals with voice disabilities  ALS  or tracheotomies  we recognize the potential for such technologies to be used for harmful applications  like synthesizing an individual s voice for deceptive purposes Ultimately we determined that the technology described in the paper had limited potential for misuse for several reasons  including the quality of data required to make it work  Arbitrary recordings from the internet would not satisfy these requirements  In addition  there are enough differences between samples generated by the network and speakers  voices for listeners to identify what s real and what s not  As a result  we concluded that this paper aligned with our AI Principles  but this exercise reinforced our commitment to identifying and preempting the potential for misuse Engaging with external stakeholdersOngoing dialogue with the broader community is essential to developing socially responsible AI  We ve engaged with policymakers and the tech community  participated in more than  workshops  research conferences and summits  and directly engaged with more than  stakeholders across the world As advances in AI continue  we ll continue to share our perspectives and engage with academia  industry  and policymakers to promote the responsible development of AI  We support smart regulation tailored to specific sectors and use cases  and earlier this year we published this white paper to help promote pragmatic and forward looking approaches to AI governance  It outlines five areas where government should work with civil society and AI practitioners to cultivate a framework for AI We recognize there s always more to do and will continue working with leaders  policymakers  academics  and other stakeholders from across industries to tackle these important issues  Having these conversations  doing the proper legwork  and ensuring the inclusion of the widest array of perspectives  is critical to ensuring that AI joins the long list of technologies transforming life for the better  ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/jJF7xBXBb6g/",
      "updated": "2019-06-28 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Ready for takeoff  Meet the Doodle for Google national finalists",
      "description": "This January  we kicked off the th year of Doodle for Google  our annual art contest for students across the country  We challenged kids across the U S  to create a visual interpretation of this year s theme  “When I grow up  I hope…  And it s clear this year s students have a lot of hopes  whether it s becoming a cartoonist  growing your own food or simply never growing up In the beginning of June  we asked you to help us judge this year s state and territory winners  Doodles  and after a million public votes  we re ready to introduce our five national finalists  one from each grade group  Meet these talented students and learn about their hopes  how they come up with their Doodle and what age they think they will be when they are “grown up  Natalia Pepe  Grade Group K Hometown  Cheshire  ConnecticutDoodle Title  FarmersHow did you come up with the idea for your Doodle  I was inspired by my town of Cheshire  Connecticut  where there are a lot of farms and orchards  and where a lot of people have gardens to grow their own food  I thought that if there were more of this in the world  people would be healthier and it would be better for the planet  Plus  it s just really cool to see things grow What else do you like to draw and doodle for fun I like drawing little monsters and all kinds of dogs  I especially like drawing comics and illustrating fun stories What age do you think you ll be when you re officially a  grownup   I think that I will officially be a grownup when I am  years old  because this is the age when I will be out of my teens  That s not for a long time Amadys Lopez Velasquez  Grade Group  Hometown  Dorado  Puerto RicoDoodle Title  When I Grow Up  I Hope…  ¡Que Todos Seamos Niños Otra Vez How did you come up with the idea for your Doodle My family always tells me to enjoy my childhood  That adults would like to be children again  It s funny and weird but it seems like the key to happiness What else do you like to draw and doodle for fun  I like to have fun drawing animals and my pets  and then transform them by drawing as if they were human What s your favorite thing to learn about in school  and why  My favorite thing to learn is history because I like to know interesting facts about my country and other parts of the world  It is like traveling in time and being able to know the past and understand the things of the present Christelle Matildo  Grade Group  Hometown  Lancaster  TexasDoodle Title  A Hopeful FutureHow did you come up with the idea for your Doodle I came up with the idea of my Doodle from current issues and topics that stand out the most to me  What else do you like to draw and doodle for fun  I like to draw mostly dragons for fun  Sometimes I draw made up creatures because I think they look cool in my imagination  What age do you think you ll be when you re officially a  grownup   I think I ll officially be a  grownup  at the age of   I can act like or be a grownup  but my  official title  isn t there yet   Jeremy Henskens  Grade Group  Hometown  Burlington  New JerseyDoodle Title  Cartooning DoodleHow did you come up with the idea for your Doodle I want to be a cartoonist when I grow up  so I made my Doodle resemble a comic strip from a comic book  What else do you like to draw and doodle for fun Random people with big heads and odd objects What s your favorite thing to learn about in school  and why Social studies  because people did some strange things in the past  and it is cool to learn about them What age do you think you ll be when you re officially a  grownup    Arantza Peña Popo  Grades  Hometown  Lithonia  GeorgiaDoodle Title  Once you get it  give it backHow did you come up with the idea for your Doodle  I came up with the idea at the last minute  actually the day of the deadline  I looked at the photograph of my mother  the real version that inspired the drawing  and thought   Hey  why don t I reverse it   I wanted to focus more on a message of helping out my awesome mother  more than anything else What s your favorite thing to learn about in school  and why I like to learn about literature that focuses on more diverse perspectives of our society What age do you think you ll be when you re officially a  grownup   I think at  years old I ll feel like a grownup  I m  now and I still feel like a kid ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/Fv9pDTLNE_M/",
      "updated": "2019-06-28 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "It s time for a new international tax deal",
      "description": "Finance ministers from the world s largest economies recently came together and agreed on the need for the most significant reforms to the global tax system in a century  That s great news We support the movement toward a new comprehensive  international framework for how multinational companies are taxed  Corporate income tax is an important way companies contribute to the countries and communities where they do business  and we would like to see a tax environment that people find reasonable and appropriate While some have raised concerns about where Google pays taxes  Google s overall global tax rate has been over  percent for the past  years  in line with the  percent average statutory rate across the member countries of the Organization for Economic Co operation and Development  OECD   Most of these taxes are due in the United States  where our business originated  and where most of our products and services are developed  The rest we paid in the roughly fifty countries around the world where we have offices helping to sell our services We re not alone in paying most of our corporate income tax in our home country  That allocation reflects long standing rules about how corporate profits should be split among various countries  American companies pay most of their corporate taxes in the United Statesーjust as German  British  French and Japanese firms pay most of their corporate taxes in their home countries  For over a century  the international community has developed treaties to tax foreign firms in a coordinated way  This framework has always attributed more profits to the countries where products and services are produced  rather than where they are consumed  But it s time for the system to evolve  ensuring a better distribution of tax income The United States  Germany  and other countries have put forward new proposals for modernizing tax rules  with more taxes paid in countries where products and services are consumed  We hope governments can develop a consensus around a new framework for fair taxation  giving companies operating around the world clear rules that promote a sensible business investment The need for modernization isn t limited to the technology sector  Both the OECD and a group of EU experts have concluded that the wider economy is “digitizing   creating a need for broad based reform of current rules  Almost all multinational companies use data  computers  and internet connectivity to power their products and services  And many are seeking ways to integrate these technologies  creating “smart  appliances  cars  factories  homes and hospitals  But even as this multilateral process is advancing  some countries are considering going it alone  imposing new taxes on foreign companies  Without a new  comprehensive and multilateral agreement  countries might simply impose discriminatory unilateral taxes on foreign firms in various sectors  Indeed  we already see such problems in some of the specific proposals that have been put forward    That kind of race to the bottom would create new barriers to trade  slow cross border investment  and hamper economic growth  We re already seeing this in a handful of countries proposing new taxes on all kinds of goodsーfrom software to consumer productsーthat involve intellectual property  Specialized taxes on a handful of U S  technology companies would do little more than claim taxes that are currently owed in the U S   heightening trade tensions  But if governments work together  more taxes can be paid where products and services are consumed  in a coordinated and mutually acceptable way  This give and take is needed to ensure a better  more balanced global tax system We believe this approach will restore confidence in the international tax system and promote more cross border trade and investment  We strongly support the OECD s work to end the current uncertainty and develop new tax principles  We call on governments and companies to work together to accelerate this reform and forge a new  lasting  and global agreement ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/z7SwqK-TxDM/",
      "updated": "2019-06-27 19:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Transit crowdedness trends from around the world  according to Google Maps",
      "description": "Crowdedness predictions come from optional feedback directly from the people who use Google Maps  In fact  you may have received notifications asking about how crowded your subway  train  or bus ride was after navigating in transit mode  To learn more about how crowdedness levels vary around the world  we analyzed aggregated and anonymized reports of crowdedness from Google Maps users from October  to June  during peak commuting hours  am   am   and identified which lines had the highest number of crowdedness reports  Here s what we found  When it comes to the most crowded transit lines  Buenos Aires and Sao Paulo dominate the rankings each city has  lines in the top  New York s famed L train which  until recently  was on the verge of closing for repair  is the only U S  transit line to make it into the top  bostonchicagolos angelesminneapolisnewyorkpittsburghportlandseattlesan franciscoAmong many U S  cities  the most crowded public transit routes are buses  In Los Angeles  for example  bus routes     and  are among the most crowded amsterdambarcelonaberlinbudapestbuenos airesdehlihongkonglondonmelbournemexicomilanmoscowosakaparissaopaulosingaporesydneytokyotorontovancouver",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/HvJVo7gX3FY/",
      "updated": "2019-06-27 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "Grab a seat and be on time with new transit updates on Google Maps",
      "description": "On days when everything runs smoothly  taking public transit is one of the best ways to get around town  Not only is it cost effective and efficient  but it also lets you stay hands free so you can sit back  relax and maybe even read a few chapters of your favorite book  But unexpected delays or overcrowded vehicles can quickly turn your ride from enjoyable to stressful  Starting today  Google Maps is rolling out two new features to help you better plan for your transit ride and stay more comfortable along the way  Live traffic delays for busesWhen you have an important meeting  a date with a friend  or a doctor s appointment  often the first thing you ll do is check the transit schedule to make sure you can make it on time  Unfortunately  transit schedules don t always reflect real time traffic conditions that impact your ride  which can cause a lot of unnecessary stress when you end up arriving later than you thought you would  To solve for this  Google Maps is launching live traffic delays for buses in places where we don t already have real time information direct from local transit agencies  You ll now be able to see if your bus will be late  how long the delay will be  and more accurate travel times based on live traffic conditions along your route  You ll also see exactly where the delays are on the map so you know what to expect before you even hop on your bus  Live traffic delays for busesCrowdedness predictionsThere s nothing more uncomfortable than being packed like a can of sardines on a hot  sweaty train  We re introducing transit crowdedness predictions so you can see how crowded your bus  train or subway is likely to be based on past rides  Now you can make an informed decision about whether or not you want to squeeze on  or wait a few more minutes for a vehicle where you re more likely to snag a seat Crowdedness predictions in transit navigationYou ll start to see these features roll out on Google Maps in nearly  cities around the globe on both Android and iOS today  Interested in learning more about crowdedness trends in your area  Check it out in this post ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/zgmlc1SDESE/",
      "updated": "2019-06-27 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "GIF ing you a way to say LOL  haha or jajaja from Google Images",
      "description": "GIFs have become an essential part of communicating with friends and family  Whether we re texting  emailing  or posting online  we re always on the hunt for that perfect GIF  In fact  over the past five years  GIF search interest on Google Images has nearly tripled  as people search for the GIF that speaks most to them To make it easier to say “surprised   “good morning  or “buenas noches  with the perfect GIF  Google Images now has a “Share GIFs  section that lets you share GIFs directly into different apps  including Gmail  Hangouts  Android Messages and Whatsapp This section is powered by our GIF search engine  something we ve been investing in since our acquisition of Tenor last year  Shareable GIFs are made available by content creators  including our partners from streaming services  movie studios  and the YouTube community  Any content provider  GIF creator or GIF platform can submit GIFs to the new section on Google Images by either uploading GIFs to Tenor com  or connecting with Google s partnership team via this form GIFs appear in this section based on how likely they are to be shared  so that you can find a GIF that captures exactly what you want to say  This feature will be available starting today on the Google app for iOS and Android  as well as Chrome on Android  Over time  we ll bring directly shareable GIFs to more surfaces and mobile browsers  so it s as easy as possible to share your personality with a cartoon  animal  or something else entirely ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/8bUuyXw9Zm8/",
      "updated": "2019-06-27 16:00:00"
    },
    {
      "name": "Official Google Blog",
      "category": "GCP",
      "title": "How Rituals used Google Marketing Platform to become a global brand",
      "description": "One year ago we introduced Google Marketing Platform to help marketers drive better results for their business  Google Marketing Platform brings together our advertising and analytics solutions  allowing you to create  buy  measure  and optimize your marketing in one single place  This enables a better understanding of the customer journey and more effective campaigns  We ve seen companies around the world use this unified approach to deliver business growth  Rituals  a fast growing European bath and body company  is one success story that stands out  Although popular across Europe  Rituals is thinking biggerーand is using Google Marketing Platform to achieve its goal of becoming a global brand Google Marketing Platform  Rituals Case StudyRituals accelerates growth with an automated audience strategy Rituals has a loyal customer base in Europe  but wanted to reach more people in other regions  To support this growth  Rituals needed a marketing strategy that could scale quickly Previously  in order to reach potential customers  the Rituals team would manually build audience lists that they would later use in their marketing campaigns  But as Rituals entered into more new countries  the amount of manual work that went into creating these audiences became excessive  Their team turned to Google Marketing Platform and its machine learning capabilities for help Today  Rituals uses Google Marketing Platform to predict which visitors to its website are most likely to purchase and then automatically create audiences of those people  This makes it easier for Rituals to quickly reach them across its marketing campaigns  The result  A faster and smarter approach to marketing  driving an  percent increase in sales and a  percent decrease in cost per acquisition The results are incredible  It actually helped us transform our whole vision  We want to be a global brand and Google Marketing Platform facilitates that growth  Martijn van der Zee Digital Director at RitualsTo learn more about how Rituals automated its marketing strategy  read the full case study here As Google Marketing Platform enters its second year  we re excited to continue to work with brands around the world and share their success stories ",
      "link": "http://feedproxy.google.com/~r/blogspot/MKuf/~3/TOsMYd-xs60/",
      "updated": "2019-06-27 15:00:00"
    }
  ],
  [
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "座席を確保して時間どおりに到着  Googleマップの新しい乗換案内",
      "description": "すべてが順調に動いていれば、公共交通機関は街中を移動するのに最も適した交通手段です。費用対効果が高く、効率的であることはもちろん、車の運転のように操作に集中する必要はなく、腰を下ろしてリラックスして、お気に入りの本を読んだりすることもできます。ただし、遅延や混雑によって、移動時間がストレスになる場合もあるでしょう。そこで、本日よりGoogleマップにつの新機能を追加しました。公共交通機関での移動の計画を事前に立てて、移動中の車内を少しでも快適に過ごしてください。バスのリアルタイム遅延情報会議、デート、店の予約などの予定がある場合、時間に間に合うよう、まずは出発前に乗り継ぎ時間を含めた移動時間を確認するかと思います。しかし、公共交通機関の運行スケジュールに、遅延の原因になるかもしれないリアルタイムの交通状況が必ずしも反映されておらず、予定より遅く到着してしまい、不要なストレスを感じることがあるかもしれません。そうした問題を解決するため、Googleマップでは、公共交通機関側からリアルタイム情報が直接提供されていない場合でも、バスの遅延情報をリアルタイムで確認できる機能を追加しました。この機能は、バスの遅延情報を路線上の実際の交通状況にもとづいて表示し、より正確な移動時間を確認できるようになりました。また、マップ上で遅延が発生している場所を正確に知ることができるため、実際にバスに乗る前にあらかじめ交通状況を把握できます。バスのリアルタイム遅延情報混雑予測夏の暑い日に満員電車にすし詰めになるほど不快なことはありません。そこで、Googleマップでは、バス、電車、地下鉄の過去の混雑状況にもとづいて、混雑度の目安を表示する混雑予測の提供を開始します。あらかじめ混雑度を知ることで、時間に余裕がある場合には乗る電車やバスを数本見送り、混雑を避けることもできるようになります。公共交通機関の混雑予測これらの新機能は、世界の約都市でAndroidとiOS版Googleマップで本日よりご利用いただけるようになります。地域別の混雑トレンドに関しては、こちらをご覧ください。Posted by Taylah Hasaballah Product Manager  Google Maps  Anthony Bertuca Product Manager  Google Maps   lt    ",
      "link": "http://japan.googleblog.com/feeds/8967014622610259963/comments/default",
      "updated": "2019-06-28 04:51:42"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Googleが東京オリンピック・パラリンピック競技大会のオフィシャルサポーターに",
      "description": "この度、Googleはオフィシャルサポーター インターネットにおける情報及びナビゲーションサービス として東京オリンピック・パラリンピック競技大会のスポンサーシッププログラムに初めて参画することをお知らせします。この記念すべき場所が、年にGoogleが海外に初めてオフィスを開設した「東京」であることに特別の感慨を覚えています。東京オリンピックが衛星放送やテレビのカラー放送が普及する契機となったように、東京大会がオリンピック・パラリンピック史上最も先端技術を駆使した革新的な大会になるよう、Googleは様々なサービスを通じて貢献していきます。Google検索、Googleマップ、Google翻訳といったGoogleの製品を通じ、日本で事業を営む方々が様々なお客様と円滑に交流できるようサポートするだけでなく、海外から訪れる観光客の皆さまにとって、日本滞在が素晴らしいものになるように支援します。そして、競技場に足を運べない方々にも、本大会で刻まれるスポーツの新たな歴史をリアルタイムでお楽しみいただけるようにしたいと考えています。Googleは、東京がオリンピック・パラリンピックの開催に向けて準備を着々と進める中、その歴史的瞬間の一端を担う機会を得られたことを嬉しく思っています。東京オリンピック・パラリンピック競技大会を満喫するための新機能等については、今後、随時ご案内します。  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/6479552212267348375/comments/default",
      "updated": "2019-06-27 08:14:07"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "コンテンツ共有サービスに対する考え方",
      "description": "SNSや動画共有サイトといったコンテンツ共有プラットフォームにおいて、悪用を防ぎながら、オンラインでの情報発信や情報交換、検索を可能にするための大筋の合意は形成されつつあります。一方で、各国の政府や企業、社会は、違法性や問題のあるオンラインコンテンツにどう対処すべきか、依然としてその対応を模索しています。情報科学と人によるレビューの双方を活用し、Googleは長年に渡り不誠実な情報、虚偽の情報、子供の搾取につながる行為など、さまざまな不正行為を迅速に特定し、防止することに取り組んできました。Googleは違法コンテンツの報告に対し速やかに対応するとともに、Googleの各種サービスでの利用を禁止するコンテンツを定めています。Googleは、テクノロジーと人によるレビューの両輪で、不適切なコンテンツの検出やポリシーの適用、さらには安全性の継続的な向上を目指しています。今年前半には、Googleにおける虚偽の情報に対する対策について詳細を公開したほか、YouTubeではコミュニティガイドラインやその適用状況に関するレポートを定期的に更新しています。この問題は、誰か一人の努力で解けるものではなく、関係するすべての人の協力が不可欠です。オンラインコンテンツには、消費者保護からプライバシー保護までを網羅する多くの法律が適用されています。中でも、オンラインプラットフォームにおける「セーフハーバー」や「善きサマリア人の法」は、プラットフォームに対して、問題のあるコンテンツと対峙する上で必要な法的確実性を提供しながら、情報の自由な流れを守り、革新や経済的な成長を可能にしています。インターネットの歴史を振り返れば、多くの国々がセーフハーバーに該当する基準を設定しており、行動規範 たとえば、EUによる「オンライン上の違法ヘイトスピーチに対抗する行動規範」や、「虚偽の情報に関する行動規範」など を策定してきました。さらに、オンラインサービスにおけるテロリスト対策で協力する共同団体「Global Internet Forum to Counter Terrorism」など、企業間の協力も進んでいます。こうした取組みは継続的に広がっています。今月はじめには、政府と主要なデジタル企業がテロリズムや過激主義のコンテンツを削除し、一連の具体的措置を講じることを約束した「クライストチャーチ・コール」にGoogleも参加しました。私たちは以前にも、スマートレギュレーションを推進する立場から、プライバシー、人工知能、政府による監視などの分野について、Googleの知見を公開してきました。最近では、違法なオンラインコンテンツの撲滅に向けた具体的な法の枠組みについても記事を公開しています。これらに基づき、国、企業、そして社会の皆さまに向けて、コンテンツ共有プラットフォームのあり方を議論する際の論点となる、いくつかのアイディアを共有したいと思います。Clarity  明確性  コンテンツ共有プラットフォームは、ユーザーに対し基準となる行動の期待値を定めるとともに、コンテンツの削除やアカウントの一時停止または停止措置に対する明確な根拠を説明した責任あるコンテンツポリシーの策定および運用を行っています。しかしながら、証拠に基づき、民主的な説明責任および国際的な人権規範の範疇で、政府が言論の合法性に対し一定の線を引くことも重要だと考えています。明確な定義が行われなければ、一方的な規制や不透明な規制が、正当な情報へのアクセスを制限する恐れがあります。Suitability 適合性  モニタリングを指向したフレームワーク  oversight framework  は、異なるサービスのそれぞれの目的や機能を認識することが重要です。ソーシャルネットワークや動画共有プラットフォームなど、幅広いユーザーとコンテンツを共有可能にすることを目的としたサービスのルールは、検索エンジン、企業向けサービス、ファイルストレージ、通信ツール、またはその他のオンラインサービスなどといったユーザーニーズや用途が根本的に異なるサービスには適合しない場合があります。同様に、異なるコンテンツに対しては、異なる対応方法が必要になる場合もあります。Transparency  透明性  意義のある透明性は、説明責任を果たしこれを強化する上で重要です。Googleが最初の透明性レポートを公開したのは、今から年以上前です。以来、Googleは透明性に対する取組みを継続的に推進しています。意義のある透明性は、プロセスの悪用を防ぎ、ベストプラクティス、研究、革新を促します。Flexibility  柔軟性   Googleをはじめとするテクノロジー企業各社は、コンピューターサイエンスの限界に挑戦することで、問題のあるコンテンツの識別および削除の大規模な処理を実現しています。そうした技術的進歩は、静的あるいは万能薬のような法律よりも、柔軟な法的枠組みを必要としています。同様に、法的アプローチにおいては、新興企業や中小企業のさまざまなニーズや特性を認識することが肝要です。Overall quality  全体的な品質  現代のプラットフォームが包含する領域の広さや複雑性といった特徴に対しては、個別のイシューよりも、全体に影響する結果に注目するデータに裏打ちされたアプローチが重要です。また、すべての問題のあるコンテンツを排除することは不可能ですが、そのようなコンテンツを目立たなくするための取組みの進化に対し一定の評価を与えることも必要です。一連の複雑な目標に対する全体的な進捗状況を評価する取組みの有用な例として、EUによる「オンライン上の違法ヘイトスピーチに対抗する行動規範」や「虚偽の情報に関する行動規範」を挙げることができます。Cooperation  協力  国際的な協調は、一般原則と慣行に従って調整されるよう努めるべきです。子どもを搾取する可能性のあるコンテンツなどについては、国際的に広く見解が一致しますが、その他の領域においては、例えば、許容される言論の統制範囲等は各国に独自の裁量が認められています。いずれの国や地域の法律も、その他の国や地域におけるコンテンツの扱いに制限を課すことはできません。先に公表された「クライストチャーチ・コール」は、立場の異なる関係者が協力して、オンラインコンテンツの課題解決を目指すことから生まれる可能性を強く示唆しています。インターネットは情報へのアクセスを拡大し、世界中の人々に広く貢献しています。あらゆる新規の情報技術がそうであったように、社会や文化はその技術がもたらす新しい挑戦や機会を前に、新しい社会規範、制度、そして法律を発展させて来ました。私たちは、今後もこの壮大かつ重要な取組みに貢献して参ります。Posted by Kent Walker  SVP  Global Affairs  Abbi Tatton Editorial Elf Google    ",
      "link": "http://japan.googleblog.com/feeds/5340521483017893259/comments/default",
      "updated": "2019-06-27 02:05:15"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Google for Startups Campusが東京にやってきます",
      "description": "Googleは、カリフォルニア州のとあるガレージで生まれたスタートアップです。スタートアップ企業を支援することは、今でも私たちのDNAの重要な部分を占めています。創業以来、人々が大きな夢を持ち、アイディアを元に起業し、コミュニティを創出し、大きな社会問題を解決し、経済発展に貢献する様子を見てきました。Googleは、日本におけるスタートアップ支援の取り組みとして、年内にGoogle for Startups Campusを渋谷ストリームに開設します。このCampusは、多様なスタートアップがビジネスを成功に導くため、互いにつながり、学び、成長するコミュニティスペースです。Google for Startups Campusは、スタートアップの拠点として、現在、ロンドン、マドリード、サンパウロ、ソウル、テルアビブ、ワルシャワにあり、東京は世界で番目となります。Campusでは、これまでの経験と世界に広がるネットワークを活用し、スタートアップが必要とするグローバルなサポートおよびツールを提供します。Campusが、日本の起業家が社会に大きな軌跡を残し、急成長する経済に貢献するための重要な拠点になることを願っています。イメージ図Google for Startups Campusは、アーリーからグロースステージのスタートアップを対象とし、コミュニティの構築とメンターシップといった、学びと成長の場を提供します。ワークスペース、コラボレーションエリア、イベントスペースや会議室などを備え、コミュニティで開催されるイベントや初期段階のスタートアップ創業者への研修プログラムを運営するなど、有望なスタートアップが世界で活躍できるよう支援していきます。Google for Startups Residency Programでも同様に、アーリーからグロースステージのスタートアップ企業を対象に、Googleの製品やサービスに加え、ネットワークおよびベスト・プラクティスなどを提供します。プログラムに参加する各スタートアップは、Campus内にワークスペースを持ち、メンター制度を活用することができます。私たちは、明確な使命を持ち、より良い世界を創造することを目指す次世代のスタートアップを応援していきたいと考えています。参加を希望するスタートアップの募集開始は、Campus開設後を予定しています。世界での成功を視野に入れるスタートアップの皆さんの応募をお待ちしています。イメージ図東京にGoogle for Startups Campusを開設し、日本におけるスタートアップの成功と繁栄を支援できる機会を持てることを、非常に嬉しく思っています。Posted by Michael Kim  Partnerships Manager  Google for Startups  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/6563544781210164996/comments/default",
      "updated": "2019-06-19 04:34:57"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Project Strobeのアップデート  Google ChromeおよびGoogleドライブのポリシー変更について",
      "description": "サードパーティー製のアプリやウェブサイトは、多くのユーザーが様々なタスクをこなしたり、オンラインでの体験を自分用にカスタマイズしたりするためのサービスを提供しています。このエコシステムを機能させるには、ユーザーが自身のデータの安全性を確信できなければならず、また、開発者のために明確なルールが確立されていなければなりません。そこで私たちは昨年、サードパーティーの開発者によるGoogleアカウントやAndroidデバイスのデータへのアクセスを抜本的に見直す「Project Strobe」を発表しました。この一環として、ユーザーデータのより堅牢な保護のために、GmailとAndroidにおけるポリシーの改定を発表しました。結果、たとえば、SMSや通話ログへのアクセス許可について行ったポリシー変更では、このような取扱いに注意すべき情報にアクセスするAndroidアプリ数が 以上減少しました。多くのアプリは、より機密性の低いデータへのアクセス許可をユーザーに求めるように変更したり、アプリの機能性への影響を最小限に控えながらも、引き続きコアサービスをユーザーに提供しています。Project Strobeの成果に基づき、本日、Googleでは、Google Chromeの拡張機能及びGoogleドライブAPIに関して、新しいポリシーを含むアップデートを発表いたします。詳細は以下のとおりです。Chrome拡張機能の信頼性向上Chromeウェブストアには実に以上の拡張機能があり、デスクトップでChromeを利用するユーザーの半数近くがこれらの拡張機能を使ってChromeおよびウェブ上での体験をカスタマイズしています。To doリストの管理から、オンラインショッピングのお得情報探しなど、利用方法も様々です。このようにユーザーのオンライン体験を発展させ、一人ひとりに合わせてカスタマイズできるのは、活発なChromeブラウザ開発者の皆さんのおかげです。昨年月、GoogleはすべてのChrome拡張機能がデフォルト設定で信頼できることを保証するという方向性を提示しました。今後もGoogleは、Project Strobeの一環として新たに追加する以下のChromeウェブストアのポリシーをもとに、その努力を継続してまいります。拡張機能は、機能を実行するために必要なデータに対してのみ、「アクセス許可」を求めることを義務化します。サービスのある機能が動作するのに必要なデータへの「アクセス許可」方法が複数ある場合、開発者は対象のデータ量が最も少ないものを選択しなければなりません。このアプローチは、これまでも開発者の間で推奨されてきたことですが、この度Googleでは、すべての拡張機能にこの条件を義務化します。Googleは、より多くの拡張機能にプライバシーポリシーの公開を義務付けます。この対象には、個人間のコミュニケーションやユーザーによるコンテンツ提供を扱う拡張機能も含まれます。従来のポリシーでは、ユーザーの個人情報および機密情報を取り扱う拡張機能に対して、プライバシーポリシーの公開とデータの機密性保持を義務付けていました。今後は、対象のカテゴリーを拡大し、ユーザーがコンテンツを提供するサービスや個人間のコミュニケーションを扱う拡張機能にも上記を適用します。この変更に関わらず、拡張機能におけるユーザーデータの取扱い方、収集範囲、そしてデータの使用および共有方法に関する透明性の担保は、当然ながら、引き続き、遵守されなければなりません。ポリシーの改定は今年の夏を予定しています。今回は、開発者の皆さんに、ご自身の拡張機能が新しいポリシーを満たしているかを確認するために必要な期間を設けるべく、少し早めにご案内致しました。ポリシー変更に関する詳細は、FAQをご覧ください。GoogleドライブAPIの強化昨秋、このProject Strobeを発表した際にGmailのデータにアクセスするアプリに対して追加のガイドラインと制限を設け、ユーザーデータポリシーを改定しました。サードパーティーアプリによるGoogleドライブ内のデータへのアクセスを、ユーザーがよりコントロールできるようにするべく、この度、同様のポリシーをGoogleドライブにも適用します。今回のポリシー改定により、GoogleドライブAPIを利用するアプリが、Googleドライブ内のコンテンツやデータに広範囲にアクセスすることを制限します。たとえば、これには特定のファイルに対するサードパーティーアプリのアクセスを制限するといったことが含まれ、バックアップサービスのように広範囲なアクセス権を必要とする、組織内にとどまらないパブリックなアプリについて検証を行います。これらの変更は、来年初頭以降に適用開始を予定しています。詳細はこちらをご確認ください。ユーザーの皆さんのデータの安全性が確保され、そしてユーザーにとって便利で有益な機能をデベロッパーの皆さんが開発できる環境の整備が重要であると、私たちは考えています。今後も、Project Strobeを継続して行くとともに、開発者の皆さんにとって、十分な準備期間を提供できるよう取組んで参ります。  Posted by  Ben Smith  Google Fellow and Vice President of Engineering",
      "link": "http://japan.googleblog.com/feeds/5219796700636546694/comments/default",
      "updated": "2019-06-07 07:27:32"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "GoogleマップとGoogle検索上の災害情報の可視化を強化",
      "description": "Googleは本日、災害時に信頼できる情報をリアルタイムに表示するSOSアラートに、自然災害に関する、よりわかりやすくなったビジュアル情報と、新たなナビゲーション警告システムをGoogleマップに追加しました。今回のアップデートにより、更に正確に災害の位置を把握し、その災害がどの方向に向かっているのか、予測できるようになりました。変更内容、および、緊急時に連絡を絶やさず情報を受け取るためのつの方法を、以下にまとめましたのでご覧ください。台風の進路予測、地震「シェイクマップ」など発生中の災害の概略、関連ニュース、緊急通報用電話番号およびウェブサイト、現地の公的機関による公式ツイートの更新、安全確保のためのアドバイス等を含む災害情報は、これまでにもSOSアラートで提供していました。今後はさらに、台風、地震および洪水 インドのみ の情報についてもより詳細に可視化することで、現地の状況をもっと簡単に把握できます。台風が到来する数日前より、該当エリア付近のユーザーには、Googleマップ上で災害注意カードを自動的に表示します。このカードは台風の進路予測に加え、予想される軌道や特定地点をいつごろ通過するかといった情報も表示するため、事前に対応を検討できます。地震が発生した場合にも、災害カードをタップするだけで地震のシェイクマップが表示されます。シェイクマップとは震源地、マグニチュード、さらには周辺地域での揺れの激しさを色分けし、可視化した地図情報のことです。この情報で迅速に地震の範囲を把握し、もっとも被害が大きいと思われる地域を確認できます。被害を受けた地域外にいる場合に、災害に関する詳細を確認したいとときは、Googleで災害の名称や地名といった関連する単語などを検索するだけで、同様の概略を示すSOSアラート等の関連情報がカードで表示されます。Googleマップでの災害ナビゲーション警告上記に加え、ご利用のルートが災害の影響を受けていると思われる場合にも、警告を表示 また、可能な場合には被害エリアを迂回するルートを提案 する機能を近日中に提供開始する予定です。災害時の対応は一刻を争います。連絡を絶やさず、必要な情報を迅速に入手するためのGoogleマップ活用方法をつご紹介します。現在地を共有する  刻々と変化し混乱を極める状況において、友人や家族に現在地を知らせることは極めて重要です。災害カードから、現在地を共有 最短分間、または共有を停止するまで できます。道路の閉鎖状況を確認、報告する  地図上で交通情報をオンにすれば、Android上で付近で閉鎖がすでに確認されている、またはその懸念がある道路を確認でき、さらにその状況が最後にいつ更新されたのかが確認できます。運転中に閉鎖された道路に遭遇した場合は、状況を報告し、付近の他のユーザーに知らることができます。さらに道路をタップすれば、その道が閉鎖されているか否かの状況を更新することができます。災害情報を大切な人たちと直接共有する 災害カードの共有ボタンを押せば、友人や家族に現状を共有し続けることができます。共有相手は、Googleマップから、災害情報、ビジュアル情報、緊急連絡先などを確認することができます。台風進路予測、地震シェイクマップは今後数週間のうちにAndroid、iOS、デスクトップおよびモバイルウェブにて運用を開始します。ぜひご活用ください。Posted by Hannah Stulberg Product Manager  Google Maps   Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/5966976156196426173/comments/default",
      "updated": "2019-06-07 02:42:53"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Google Nest Hub新登場",
      "description": "Googleは、Googleアシスタントを搭載するスマートディスプレイ「Google Nest Hub」を月日 水 より国内で発売します。Google Nest Hubは、毎日の生活に便利な情報を一目で確認できるスマートディスプレイです。Googleアシスタントを搭載し、話しかけるだけで検索、YouTube、Googleフォト、カレンダー、マップなど、Googleの役立つ機能をディスプレイに表示して利用することができます。どんなお部屋にもぴったりGoogle Nest Hubはインテリアとしても馴染みやすく、どんなお部屋にもぴったりです。インチのディスプレイ画面は大きすぎず、部屋で写真を見るのにちょうどいいサイズです。柔らかく丸みを帯びたデザインは、キッチンカウンターや寝室のサイドテーブルなど、どんな場所においても違和感がありません。本体カラーは、Chalk、Charcoal、Aqua、Sandの色。インテリアに合わせてお好みの色をお選び頂けます。Google Nest Hubはカメラを搭載せず、カメラを置きたくない部屋での使用に適しています。端末上部には、周囲の光に合わせて画面の明るさを自動的に調整するアンビエントEQ機能のライトセンサーを搭載しています。これにより、画面が極端に眩しく光ることがなく、表示される写真も自然と部屋に溶け込みます。また、就寝時は画面の照度が暗くなり、眠りを妨げることもありません。朝のルーティンで一日の始まりをスムーズにどんなご家庭でも、朝は慌ただしいものです。目が覚めてから家を出るまで、分たりとも無駄にはできません。そんな忙しい朝には、Google Nest Hubのルーティン機能を使って、効率的に情報を入手しましょう。「OK Google おはよう」と声をかけるだけで、あなたにパーソナライズされたGoogleカレンダーの予定、Googleマップを使った通勤ルートの交通情報、リマインダーや天気情報などの、一連の便利な情報がひと目で確認できます。また、話している人の声を聞き分けるボイスマッチ機能を使えば、最大人が各々のニーズや好みに合わせたカスタムのルーティンを作成でき、家族での利用に便利です。スマートホームをシンプルにGoogle Nest Hubを使って、数多くのメーカーから販売されているスマートホーム向けデバイスを簡単に一括で管理できます。画面を上から下にスワイプすると、ご自宅でつながっている対応スマートデバイスの一覧を確認し、タッチで操作できるダッシュボード画面「ホームビュー」が表示されます。対応デバイスを操作するたびに別々のアプリを起動する必要はありません。家族のどなたでも、Google Nest Hubのホームビューから、簡単に管理できるようになります。Googleフォトで思い出の写真を楽しむ利用していない時は、Google Nest Hubをフォトフレームとしてお楽しみください。簡単な設定で表示する写真を自動的に更新することができ、いつでもベストな写真をご覧いただけます。また、Googleフォトの「リアルタイム共有アルバム」機能を使えば、表示したい人物やペットの写真を自動的にアルバムに追加し、Google Nest Hubに表示できます。手動で写真をアルバムに追加する必要はありません。重複した写真や焦点のあっていないボケた写真は非表示になります。例えば、孫の写真を自動的に表示するよう設定したリアルタイム共有アルバムを実家のご両親と共有すれば、ご両親はいつでもお孫さんの最新のベストショットをGoogle Nest Hubの画面上で見られます。また、特定の写真を表示したいときは、「OK Google 京都の写真を見せて」と声をかけるだけですぐに思い出の京都旅行の写真を表示できます。キッチンで手がふさがっていても大丈夫料理中に手がふさがっていても、Google Nest Hubがあれば素早くレシピをチェックできます。たくさんのレシピの中からお好みのものを音声で検索して、スムーズに料理を始められます。音楽や動画を楽しむのにも最適Google Nest Hubを購入すると、YouTube Premiumをヶ月無料でお楽しみいただけます。  YouTube Premiumは、YouTube MusicやYouTubeの動画を広告無しで視聴でき、オフライン再生やバッグクラウンド再生が利用できます。YouTubeの他にも、様々な音楽や動画サービスのコンテンツをお楽しみ頂けます。Google Nest Hubはフルレンジスピーカーを搭載し、鮮明でクリアなサウンドを再生します。日本での販売情報Google Nest Hubの希望小売価格は円 税込 です。月日 水 よりGoogleストアをはじめ、各地の販売店やオンラインサイトでご購入頂けるようになります。新しいGoogle Nest Hub、ぜひお試しください。Google Nest Hubを購入できる主要な店舗・オンラインサイトエディオン 株式会社エディオン Googleストア Google ケーズデンキ 株式会社ケーズホールディングス コジマ 株式会社コジマ 上新電機 上新電機株式会社 ソフトバンクショップ ソフトバンク株式会社、SB C amp S株式会社 ソフマップ 株式会社ソフマップ TSUKUMO 株式会社Project White ノジマ 株式会社ノジマ PCデポ 株式会社ピーシーデポコーポレーション ビックカメラ 株式会社ビックカメラ ベイシア電器 株式会社ベイシア電器 ベスト電器 株式会社ベスト電器 ヤマダ電機 株式会社ヤマダ電機 ヨドバシカメラ 株式会社ヨドバシカメラ 楽天ブックス 楽天株式会社 お料理をサポートするレシピAJINOMOTO PARK 味の素株式会社 E・レシピ エキサイト株式会社 オレンジページ 株式会社オレンジページ クラシル dely株式会社 DELISH KITCHEN 株式会社エブリー Nadia 株式会社オーシャンズ macaroni 株式会社トラストリッジ レタスクラブニュース 株式会社KADOKAWA 音楽や動画コンテンツを楽しむAWA AWA株式会社 うたパス・ビデオパス KDDI株式会社 Google Play Music Google Spotify スポティファイジャパン株式会社 dヒッツ 株式会社NTTドコモ  順次対応開始Hulu HJホールディングス株式会社 ラジコ 株式会社radiko YouTube   YouTube Music Google U NEXT 株式会社U NEXT  月までの対応開始を予定ニュースや情報を確認するウェザーニュース 株式会社ウェザーニューズ 日経電子版 株式会社日本経済新聞社 テレ朝ニュース 株式会社テレビ朝日 WIRED 合同会社コンデナスト・ジャパン   年月日までにGoogle Nest Hubの利用開始設定を行った日本在住の方のみご利用いただけます。YouTube Premium、YouTube Music Premium、Google Play Musicのいずれも定期購入や無料トライアルをご利用になったことがない方のみ対象となります。年月日までにご利用いただく必要があります。有効なお支払い方法をご登録いただく必要があります。無料トライアル期間が終了すると、通常の定期購入料金が自動的に課金されます。Posted by Google Nest  amp  Googleアシスタントチーム  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/6359922289032802303/comments/default",
      "updated": "2019-06-05 05:56:07"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Demo Day Asia をバンコクで開催します",
      "description": "Googleは、今年で回目となるDemo Day Asiaをバンコクで開催します。乳がん早期発見のための画像診断装置から、太陽エネルギーを使い、子供たちへの教育の機会を改善するプロジェクトまで様々な分野のスタートアップ各社より応募をいただきました。今年は、アジアを拠点に世界で活躍する社が最終候補に選ばれました。Demo Day Asia の最終候補は以下の社です。Anywhr シンガポール 、Glazziq タイ 、Kyna ベトナム 、Lily Medtech 日本 、Matelabs インド 、Modoo 中国 、Soundbrenner 香港 、Talkiplay オーストラリア 、Tello Talk パキスタン 、Wahyoo インドネシア 、Yolk 韓国 最終候補に選出された各社は、今月日にバンコクで行われる Techsauce Global SummitでGoogle for Startups Demo Dayに参加します。Googleは、日間のメンターシップ、プログラミング、ネットワーキングの体験を提供します。最後には、Golden Gate VenturesのJeffrey PaineやMonk s Hill VenturesのJustin Nguyen、GOBI PartnersのShannon Kalayanamitr等、著名な審査員に事業戦略をピッチする機会が用意されています。昨年行われた第回のDemo Day Asiaで披露されたように、いまアジアでは多くのアイディアが生まれています。Googleは、これらの才能あふれるスタートアップたちがトップレベルの投資家たちとつながる機会を創出できることを、そして彼らの社会における大きな問題を解決したいというビジョンを援助できることを大変嬉しく思います。Demo Day Asia は、もうすぐ始まります。アジア地域の次世代を担うテックチャンピオンたちにご期待ください。 nbsp  Posted by Michael Kim  Partnerships Manager  Google for Startups",
      "link": "http://japan.googleblog.com/feeds/5136392343458722634/comments/default",
      "updated": "2019-06-03 05:45:36"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "最新の研究結果 アカウントの不正利用を防止する基本的な方法とその効果",
      "description": "Googleは毎日、アカウントハイジャックを狙った大量の攻撃からユーザーを保護しています。これらの攻撃は、自動ボットによるサードパーティから漏洩したパスワードに端を発するアクセスが大半ですが、フィッシングや標的型攻撃も発生しています。今年初めに、アカウント再設定用の電話番号の追加などを含むつの簡単な手順によるセキュリティ対策方法をお知らせしましたが、今回はそうした対応策の効果についてお知らせします。Googleは、ニューヨーク大学とカリフォルニア大学サンディエゴ校の研究者と協力し、アカウントの基本的なセキュリティ対策が、アカウントの乗っ取り防止にどれほど効果的かを調査しました。一年間に渡り不特定多数を狙った攻撃と標的型攻撃を対象に、その調査結果を先日月日にサンフランシスコで開催されたThe Web Conferenceで発表しました。調査の結果、Googleアカウントに再設定用の電話番号を登録するだけで、ボットによる自動の攻撃の 、不特定多数を狙ったフィッシング攻撃の 、そして標的型攻撃の をブロックできることが明らかになりました。Googleの自動的かつ積極的なハイジャックを防止する取組みGoogleは、すべてのユーザーをアカウント乗っ取りから保護するために、自動化し、 プロアクティブな セキュリティレイヤを構築しています。たとえば、不審なサインインが検出された場合 新しい場所やデバイスからのサインイン そのサインインが実際にアカウント所有者によって行なわれたものかどうか、追加の証明を要求します。この時、アカウント所有者か否かの証明に使われるのが、信頼できる電話端末や、秘密の質問です。もしあなたがスマートフォンにサインインしていたり、再設定用の電話番号を登録していれば、デバイスベースの段階認証プロセスと同等のセキュリティ保護を提供することができます。今回の調査では、アカウント再設定用の電話番号にSMSコードを送信するように設定するだけで、ボットによる自動攻撃を 、不特定多数を狙ったフィッシング攻撃では 、標的型攻撃では をブロックできることが示されました。加えて、SMSよりもさらに安全な端末上のプロンプト 段階認証プロセスのコードでログインする代わりに、Googleからスマートフォンに送信されるプロンプトをタップする方法 を使ったセキュリティ保護では、ボットによる自動攻撃を 、不特定多数を狙ったフィッシング攻撃を 、標的型攻撃を 防ぐことができました。 nbsp 自動的な攻撃を防ぐ上で、端末ベースと知識ベースのセキュリティはいずれも効果的ですが、端末をベースとするセキュリティはフィッシングや標的型攻撃のいずれにも効果を発揮します。Googleでは、アカウント再設定用の電話番号を設定していない場合に、保護強度は落ちるものの、前回サインインした場所を聞くといった知識ベースのセキュリティ保護方法を取ります。この方法はボットに対しては効果的ですが、フィッシングに対する保護率は 程度まで低下する可能性があり、標的型攻撃に対しても同様の脆弱性が発生します。これは、フィッシングや標的型攻撃では、偽のページを表示することで、サインインに必要な情報をユーザーから騙し取ることがあげられます。こうしたセキュリティ上の利点を考慮すると、すべてのサインインにおいて追加の手順を導入すべきと思われるかもしれません。しかし、追加手順の導入は、アカウントからロックアウトされるリスクを上げることがわかっています。今回の調査では、ログインの問題が発生した際に のユーザーはスマートフォンを手元に持っておらず、 のユーザーは予備のメールアドレスを思い出すことができませんでした。もしスマートフォンにアクセスできなかったり、これらの追加手順をクリアすることができない場合は、以前にログインした端末を使うことで、アカウントにアクセスすることができます。hack for hireを調査する nbsp ボットによる攻撃やフィッシング攻撃は自動のセキュリティシステムでブロック可能ですが、標的型攻撃はより悪質になりつつあります。Googleでは、アカウント不正利用に関する継続的な対策の一貫として、あるGoogleアカウントのハッキング件に対し、ドルの賞金を提示する犯罪グループ「hack for hire」の調査を進めています。そうしたケースでは、攻撃者は家族、同僚、政府関係者、Googleを装い、ユーザーのアカウント情報を抜き出すために複数のフィッシング攻撃を仕掛けています。ターゲットが罠にかからない場合、攻撃がヶ月以上も続くようなケースも認められています。パスワードの有効性をリアルタイムでチェックするフィッシング攻撃の例。アカウントにアクセスするためのSMS認証コードを開示するように要求しています。このようなレベルのリスクに直面しているユーザーは万人に人程度であり、攻撃者はランダムな個人を標的にしていません。Googleの自動的なセキュリティ保護機能は標的型攻撃の を遅延または防ぐことができますが、リスクの高いユーザーに対してはAdvanced Protection Programへの登録を推奨しています。今回の調査でも、セキュリティキーを使用していたユーザーは、フィッシング攻撃の被害者にならずにすんだことがわかっています。アカウントのセキュリティを確認する調査結果からも読み取れる通り、Googleアカウントを保護するためにできる最も簡単な方法は、アカウント再設定用の電話番号を登録することです。ジャーナリスト、活動家、ビジネスリーダー、政治活動グループなど、リスクの高いユーザーには、高度な保護プログラムによる最高レベルのセキュリティを提供しています。Chrome拡張機能「Password Checkup」をインストールすることで、第三者によるパスワード侵害からGoogle以外のアカウントを保護することもできます。車でシートベルトを締めるのと同じように、アカウントを安全に保つためにも、ぜひアカウントを安全に保つつのヒントを確認してください。Posted by Kurt Thomas and Angelika Moscicki  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/4376660760899249114/comments/default",
      "updated": "2019-05-27 05:40:19"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "すべてのユーザーに役立つプライバシー機能を",
      "description": "いつもお使いの言語で検索結果を表示したり、自宅までの最短ルートを提案したり、データはGoogleのサービスを、あなたにとって便利にすることができます。私たちは、ユーザーの皆さんが、自身のデータを把握して管理でき、自分にあったプライバシーの設定を選択できることが重要だと考えています。だからこそ、Googleの製品には、かねてから使いやすいプライバシーの機能や設定を組み込んできました。今日は、先日のI Oで発表したGoogleの製品及びプラットフォーム全体に向けたプライバシーとセキュリティツールの拡充についてご案内します。 nbsp データの管理をもっと簡単に主要なGoogleの製品すべてから、Googleアカウントにワンタップでアクセスプライバシー設定は、見つけやすく、使いやすいことが大切です。Googleでは、あなたがGoogleに保存・共有している自分の情報をひと目で確認し、プライバシーとセキュリティの設定を管理する場所として、数年前にGoogleアカウントを導入しました。設定方法もシンプルに、オンかオフを選ぶだけで、どのアクティビティをアカウントに紐づけて保存するかを決められます。削除する場合も、個別のアクティビティを選ぶか、情報のカテゴリを選ぶだけです。Googleが提供する製品の増加にあわせて、設定をより見つけやすくするための変更も行っています。たとえば、先日からGmail、ドライブ、連絡先、Google Payでは、画面の右上にアカウントのプロフィールアイコンを表示するように変更しました。アイコンからGoogleアカウントのリンク デスクトップでは、プライバシーのリンク をタップするだけで、管理画面にすばやくアクセスすることができます。目立つ場所にプロフィール画像を表示することで、Googleアカウントにログインしているかどうかも、ひと目で分かります。今月には、検索、マップ、YouTube、Chrome、アシスタント、ニュースなどでも、同様の機能がお使いいただけるようになります。検索、マップ、アシスタントでデータを簡単に管理昨年、検索から直接、検索履歴等のデータを管理できる機能を公開しました。検索の画面を離れずに、最近の検索アクティビティを確認したり、削除したり、検索に関連したGoogleアカウント内のプライバシー設定にアクセスできる他、データ利用と検索についての説明を読むことができます。今後、マップ、アシスタント、YouTube  近日中に公開予定 でも、より容易にデータ管理ができるようしていきます。たとえば、Googleマップを使いながら、位置情報に関連するアクティビティを確認・削除し、経路検索に戻るといったことができるようになります。ウェブとアプリのアクティビティの自動削除、今後はロケーション履歴にも先週、ウェブとアプリのアクティビティおよびロケーション履歴の保存期間をヶ月もしくはヶ月から選べるように変更しました。いずれかの期間を選択すると、指定した期間より前に保存されたデータは以後継続して自動的にあなたのアカウントから削除されます。同機能は、ウェブとアプリのアクティビティで月日より、ロケーション履歴では来月からご利用いただけるようになる予定です。シークレットモードをGoogleのアプリにも年以上前に公開して以来、Chromeブラウザのシークレットモードは、閲覧履歴等のアクティビティをブラウザやデバイスに保存せずにインターネットを閲覧する自由をユーザーの皆さんに提供しています。スマートフォンがインターネットにアクセスする手段として主流となった現在、Googleの主なアプリでも、このプライベートブラウジングのための機能が求められていると考えています。シークレットモードは、すでにYouTubeアプリでお使いいただけますが、近日中に同機能をマップ及び検索アプリでもお使いいただけるようにする予定です。シークレットモードに切り替えるには、プロフィール写真をタップして「シークレットモードをオンにする」を選んでください。Googleマップでシークレットモードを有効にすると、検索した場所やルート案内などのアクティビティはGoogleアカウントに保存されません。より強力なプライバシー管理をプラットフォームに組み込む先日のI Oでは、Googleが提供するプラットフォームや製品上での横断的なプライバシーの取組みについても紹介しました。たとえば、Android Qには、設定メニュー内のプライバシーに関する項目を見つけやすいようにしたほか、位置情報もさらにわかりやすく、管理しやすく変更しています。Chromeでは、ウェブ全体を対象に、より積極的なフィンガープリントの制限とクッキーの管理強化に関する計画を発表しました。また、Googleやパブリッシングパートナーのサイトで表示される広告のパーソナリゼーションに利用するデータや、表示のプロセスに関わる企業について、透明性をさらに高める取組みも発表しました。少ないデータで、ユーザーのためにもっと多くのことを nbsp フェデレーションラーニングは、データをデバイスに留めながら、製品の利便性向上を可能にしますプライバシー保護を強化する上で、機械学習の技術進歩は非常に強い味方です。そのよい例のひとつは、「フェデレーションラーニング」です。フェデレーションラーニングは、ユーザーのデータを端末内に留めたまま、開発者がAIモデルをトレーニングし、製品を使うすべての人のために利便性を高めることができる機械学習の新しい手法です。このような新技術や新たな手法の開発・活用が、より少ないデータでも、より多くの便利な機能や製品の実現を可能にしています。たとえば、Gboard Googleキーボード は、予測変換にフェデレーションラーニングを使用しています。これまでは「zoodles」や「Targaryen」といった新しい単語は、複数回入力が繰り返された後に変換候補として表示していましたが、フェデレーションラーニングを活用することで、数千人程度のユーザーが新しい単語を使い始めると、Googleに皆さんが何をタイプしているかを一切知らせることなく、GboardのAIモデルはこの新しい単語を学習して、予測変換候補として表示できるようになります。Googleは、個別のユーザーに付随する特定の情報を記憶することなく、機械学習モデルのトレーニングを可能にするdifferential privacy protection 差分プライバシーの保護 にも注力しています。年に同技術について初期の研究結果を発表し、これまでにChrome、GmailのSmart Compose機能 英語のみ 、Googleマップでレストラン等の混雑状況を表示するために利用しています。今回公開したTensorFlow Privacyが、広く機械学習開発者にとってdifferential privacy技術を活用する助けとなることを期待しています。Google製品とプラットフォーム全体に、最も強力なセキュリティを堅牢なセキュリティが担保されていなければ、データを安全に保つことはできません。Googleはユーザーに安全な環境を提供するために、常時セキュリティシステムに投資を行っています。たとえば、毎日約億の端末を保護するセーフブラウジングや、毎日億を超えるスパムやフィッシングをブロックするGmailのスパムフィルタは、まさにその一例です。また最近、GoogleではAndroid 以上を搭載するスマートフォンをセキュリティキーのように利用できる機能を公開しました。セキュリティキーの利用は、フィッシング攻撃に対して、最も強固な段階認証を可能にします。これにより、億台以上の端末が段階認証のセキュリティキーとして利用できるようになります。すべてのユーザーにおいて、プライバシーとセキュリティは等しく尊重されるべきだと、私たちは強く信じています。将来に渡って、私たちの製品が安全であるようにつとめるだけでなく、より少ないデータでも提供できる価値を増すよう、テクノロジーに投資し、ユーザーに対しては、ご自身のデータについてわかりやすく、意味のある選択肢を提供していくことをお約束します。Posted by  Eric Miraglia  Director of Product Management  Privacy and Data Protection Office",
      "link": "http://japan.googleblog.com/feeds/7016851848225555458/comments/default",
      "updated": "2019-05-20 05:52:36"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Google Pixel a新登場、便利なスマートフォンをお求めやすい価格で",
      "description": "日常に欠かせないスマートフォンには、多くのことが求められています。思わず共有したくなるような鮮やかな写真をどこにいても撮影できる最高のカメラ、お気に入りのアプリや日常のあれこれをこなす様々な機能、長持ちするバッテリー、端末を安全かつ常に最新の状態に保つ最新のソフトウェア、そしてお手頃なお値段。本日発表する新しいGoogle Pixel aとGoogle Pixel a XL は、このような期待にお応えするスマートフォンです。Google Pixel aは手になじみやすく、シャープで鮮明な色を実現するOLEDディスプレイを採用しています。色はJust Black、Clearly White、Purple ishの種類をご用意しており、種類のサイズからお選び頂けます。Googleストアでの価格はインチモデルが円 税込 、インチモデルが円 税込 です。上位機種の機能を搭載 カメラ、Googleアシスタント、長持ちするバッテリー、セキュリティGoogle Pixel aは、上位機種で提供される機能をお届けします。受賞歴のあるカメラを搭載し、夜景モード、ポートレートモード、超解像ズームなどの機能とGoogleの nbsp  HDR 技術によって、美しい写真を撮影できます。Googleフォトを使って、ストレージ容量を気にすることなく無料で高品質の写真や動画を保存できます。さらに、付属の充電器を使って、わずか分の充電で時間、フル充電では最大時間も使用できます 。端末の下部をぎゅっと握るとGoogleアシスタントが起動し、話しかけるだけでテキストを送信したり、道順を確認したり、通知を設定したりできます。Google Pixel aには、年間のセキュリティおよびOSアップデートが適用されるほか、Googleが開発したTitan Mという独自のチップを採用しており、ユーザーの大切なデータの保護をサポートします。新機能をよりお求めやすい価格でGoogle Pixelでは、YouTube、Googleフォト、GmailなどのGoogleアプリが使いやすく、新機能も一番早くお試し頂けます。Google Pixel aを含むGoogle Pixelポートフォリオ全体で、GoogleマップのAR機能のプレビュー版をお試しいただけるようになります。また、新しくタイムラプスの機能も加わります。綺麗な夕暮れの様子などを数秒の動画で捉えて、ソーシャルメディアに投稿したり、友達にメッセージで共有したりできます。日本での販売情報Google Pixel aおよびGoogle Pixel a XLは、本日月日 水 からGoogleストアで購入の予約受付を開始し、月日 金 からご購入いただけるようになります。また、Googleストア以外にキャリアからの発売も予定しています。詳細は各社のウェブサイトをご確認ください。Google Pixelのはじめてのご利用には、付属のクイックスイッチアダプターを使って、写真、音楽、メディアをすばやく転送できます。設定などにお困りの際は、「設定」から「ヒントとサポート」をタップして、Googleのサポートをご利用ください。Google Pixel a、ぜひお試しください。  電池駆動時間は、 常に表示状態のディスプレイ をオフにした状態での通話、データ通信、待受時、モバイルアクセスポイントやその他の機能の使用などによる電池使用量を組み合わせた数値を基に概算したものです。画面表示やデータ通信を多用すると電池の消耗が速くなります。充電速度は、付属の充電器の使用を前提としています。実際に使用できる時間は使用状況によって変化する可能性があります。Posted by Google Pixel Team  Abbi Tatton Editorial Elf Google    ",
      "link": "http://japan.googleblog.com/feeds/2108139734025543949/comments/default",
      "updated": "2019-05-07 19:04:44"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Google Pixelで名探偵ピカチュウと一緒に写真を撮ろう ",
      "description": "「ポケモン」はこれまで年以上、何世代ものファンの冒険心をかきたててきました。そんなポケモンの世界が初めて実写化された映画「名探偵ピカチュウ」の公開にあわせ、「名探偵ピカチュウプレイ文字パック」を提供を開始しました。スマートフォンカメラのAR機能「Playground」で本日よりご利用いただけます。ピカチュウ、リザードン、プリン、バリヤードと一緒に、アクション感満載のシーンを撮影しましょう。カメラにある「その他」から「Playground」をタップして「名探偵ピカチュウ」をお選びください。お好きなポケモンを選択するすると、カメラを向けている方向にポケモンが現れ、写真や動画を撮ることができます。日本版では、映画「名探偵ピカチュウ」で吹替えを担当した西島秀俊さんの声でしゃべるピカチュウが現れます。名探偵ピカチュウプレイ文字パックには、映画に登場する体のポケモンが含まれています。ARCoreのモーショントラッキング、光源の推測、現実世界の認識機能といった技術によって、映画と同じように、まるでポケモンがすぐそこにいるかのようなシーンを画面に表示します。ピカチュウと一緒に自撮り写真を撮影すると、あなたの表情にあわせてキャラクターの笑顔も変化します。この機能は、機械学習によって実現しています。プリンと一緒に歌ったり、リザードンと一緒に火を吹いたり。ポケモンたちと一緒の楽しい写真や動画を撮影して Pixelと名探偵ピカチュウでぜひシェアしてください。名探偵ピカチュウプレイ文字パックは、Google Pixelでご利用いただけます。リアルなポケモンの世界へようこそ。 Legendary and Warner Bros  Entertainment Inc  All Rights Reserved  nbsp  Pokémon Posted by Playgroundチーム  Abbi Tatton Editorial Elf Google    ",
      "link": "http://japan.googleblog.com/feeds/7627701622870048387/comments/default",
      "updated": "2019-05-03 03:31:21"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Google Earth Engineが可能にする新しいGoogle Earthタイムラプス",
      "description": "本日より、年にアップデートを発表したGoogle Earthタイムラプスに、過去年の画像が加わり、年から年の年分の地球上の画像をご覧いただけるようになりました。また、モバイルやタブレットにも対応、デザインも一新し、より気軽に、わかりやすく、地球の変化にアクセスすることができるようになりました。地球の経年変化を、地域や国、世界など様々なズームレベルで見ることができるGoogle Earthタイムラプスを使えば、ドバイの人工島パームアイランドの開発の様子や、アラスカのコロンビア氷河の後退、そしてネバタ州ラスベガスの印象的な都市拡大を一目で理解することができます。日本の場所もいくつか見てみましょう。例えば、岩手県陸前高田市では、高田松原が年の東日本大震災によって被害を受け、その跡地に防潮堤が建設されているのがわかります。また、沖縄県那覇市では、年に利用開始予定の那覇空港第二滑走路が建設されているのがわかります。タイムラプスは、Google Earth Engineを使い、USGS NASAランドサットプログラム、ESAセンチネルプログラムの万以上の衛星画像 の乗、約京の画素 を組み合わせ、雲のない画像を提供しています。また、今回も、Carnegie Mellon CREATE LabのTime Machineライブラリによって、インタラクティブに探索することが可能になっています。ぜひ、世界の気になる場所を nbsp Earth Engine Websiteで探索するか、YouTubeで世界各地のツアーに出かけてみてください。また、このタイムラプスを作る鍵となったGoogle Earth Engineですが、JAXAのデータセットも先日より提供を開始し、現在までにも豪雨による土砂崩れを広範囲で検知することに使われました。Google Earth Engineがどのようなもので、どのように活用されているか、世界一美しいと言われているJAXA種子島宇宙センターでのロケット打ち上げ画像と共にご覧ください。Google Earth Engineでできること昨年月にGoogle Earthで公開されたJAXAの地球を見守る人工衛星たちでご紹介しました通り、ロケット打ち上げの目的の一つは人工衛星を軌道に乗せることです。その人工衛星から届いた衛星写真などを通じて、私たちは地球の様子を知ることができます。Google Earthでも、宇宙から見える地球上のアルファベットや、夜の地球を見ることができます。ただ、宇宙から撮影した写真には雲が写り込んでしまい、地上の様子を見えにくい場合があります。また、ある地域で森林の伐採が進んでいるように見えても、実際にはどのくらいの面積が伐採されたのかを、衛星写真のみで計測することが難しい場合もあります。こういった課題を解決するために開発したのが、Google Earth Engineです。年に公開したGoogle Earth Engineですが、今では年分の衛星画像と例えば世界の降水量といった、の地球の観測データ、そして多くの計算力に、パソコンとインターネットさえあればアクセスすることができ、アルゴリズムを使ってデータを解析することが可能です。このデータを解析する力でGoogle Earthタイムラプスが作成されましたし、世界で初めて地球規模で森林の増減を可視化することも可能になりました。気候変動などにより、様々な地球規模課題に向き合わなくてはならない今、Google Earth Engineが今の地球をよりよく知るためのツールとなり、具体的なアクションを生み出すための知見の発見に繋がることを願っています。Posted by Google Earth Outreachプログラムマネージャー、松岡朝美 Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/9108726803888793334/comments/default",
      "updated": "2019-04-26 03:13:03"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "テクノロジーの力で社会をより良いものに。日本全国万人にデジタルスキルトレーニングを提供します",
      "description": "Googleはこれまで、Women Willによる働き方改革に関するトレーニング、イノベーションジャパンによる中小企業やスタートアップを支援するデジタルマーケティングに関するトレーニング、NPO「みんなのコード」との協力による教育者に向けたコンピューターサイエンス教育に関するトレーニングなど、様々なデジタルスキルに関するトレーニングを提供してきました。在宅勤務などの働き方改革が進む中で、より効率的で生産性の高い働き方をテクノロジーを活用して実現したいというニーズ、インバウンドが拡大する中で、より海外のマーケットを意識したマーケティングを行いたいというニーズ、年に必修化するプログラミング教育を行える教育者を増やしたいというニーズなど、デジタルスキルに対する多様なニーズが高まっています。一方で、デジタルスキルの習得において、ビジネスの規模や地域によって格差が生まれています。例えば、クラウドを活用した働き方改革やデジタルマーケティングについては、従業員規模が小さいほどICTの活用ができていない 総務省通信利用動向調査報告書・平成年 状況になっており、中小企業がIT投資を行わない理由のトップは「ITを導入できる人材がいない」となっています 経済産業省中小企業・小規模事業者の生産性向上について・平成年 。また、小学校でのプログラミング教育においては、地域で「特に取り組みをしていない」教育委員会が関東ブロックでは  であるのに対して、北海道・東北・九州沖縄ブロックでは を超えており、地域差が生まれています。 教育委員会等における小学校プログラミング教育に関する取り組み状況等について・平成年度 そこでGoogleは、ビジネスの規模や地域などに関わらず、より多くの方のデジタルスキル習得をサポートをするために、デジタルスキルトレーニングの提供を開始いたします。これまで行ってきた各種トレーニングの知見を活かし、「Grow with Google」というプロジェクトの下、日本国内で年までに万人にデジタルスキルトレーニングを提供します。詳しくはこちらのサイト  をご覧下さい。Grow with Google概要 nbsp Grow with Googleでは、オンラインで受講できる「オンライントレーニング」と、セミナーやイベントなどの「対面式トレーニング」のつを用意します。いずれも無償で受講できます。プロジェクト開始時点では、下記つの主要カテゴリーで様々なトレーニングプログラムを提供してまいります。ビジネス向けプログラム例 「はじめてのデジタルマーケティング」知識ゼロの状態からデジタルマーケティングの基礎を学ぶプログラム。個人向けプログラム例 「はじめての働き方改革」個人やチームとしても取り組める「働く」をもっと良くするためのプログラム。学生・教育者向けプログラム例 「はじめてのAI」AIの基礎を理解し、どのように社会で活用されているかを学ぶプログラム。スタートアップ向けプログラム例 「Launchpad Accelerator」スタートアップのビジネスがグローバルにスケールするための支援を目的としたプログラム。デベロッパー向けプログラム例 「ML Study Jams」機械学習の専門的な知識を持たない開発者向けにML APIを利用してトレーニングを無料で提供するプログラム。Grow with Googleでは、様々なパートナーと協力させていただくことで、より多くの方にトレーニングを受けていただくことを目指します。今回、ビジネス向けには、「何から始めて良いかわからない」という方のために、「デジタルマーケティングの基礎」と「働き方改革の基礎」が時間で学べる「はじめてのデジタルマーケティング」「はじめての働き方改革」トレーニングを、三菱UFJフィナンシャル・グループの三菱UFJリサーチ amp コンサルティング株式会社様と協力して全国の中小企業に提供します。また、学生・教育者向けには、大学生を対象に、「AIの基礎的な概要」が時間で学べる「はじめてのAI」トレーニングを、株式会社マイナビ様と協力して全国の大学生に提供します。    Grow with Googleパートナーに関してご興味のある企業・団体・個人の皆さまは公式ウェブサイト LINK のお問い合わせフォームよりご連絡下さい。本プロジェクトについて、情報通信技術 IT 政策ご担当内閣府特命担当大臣平井卓也様よりコメントをいただいています。「世界ではデジタル化とグローバル化が不可逆的に進んでおり、日本もその変化に直面しています。多様なアイデアや技術が創出され、失敗しても何度でも挑戦できる環境が、社会・産業構造に変化をもたらすイノベーションにつながります。そうしたイノベーターたちが新しい時代を創っていくことは間違いなく、世界に伍するエコシステムを創れるかが、日本の将来のカギを握っています。異業種・異分野のイノベーターが連携することで、日本におけるデジタル化が一層推進され、多くのスタートアップ等が成長することで、産業の発展や技術促進、グローバル化に寄与することを期待しています。」テクノロジーの力で社会をより良いものにしていくために。Googleはこれからも尽力して参ります。  Abbi Tatton Editorial Elf Google         Posted by  Grow with Googleチーム",
      "link": "http://japan.googleblog.com/feeds/5787591226975545957/comments/default",
      "updated": "2019-04-22 05:13:56"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Gboardチームからの新しいご提案",
      "description": " Gboardチームは、いつでもどこでも思いどおりの文字入力を提供すべく、日々努力を重ねています。Gboardは、優れた変換性能にくわえて、お好きな写真を背景にできるテーマ機能や、外国語への翻訳機能などの便利な機能をそなえたキーボードアプリです。AndroidとiOSのどちらでもご利用いただけます。私たちは、快適な文字入力環境を実現するために、これまでにもさまざまなキーボードやデバイスを提案してきました。そして今回、私たちが注目したのは文字入力の柔軟性です。これまでも、従来の発想とはかけ離れたキーボードを開発してきたという自負がありますが、その手法にはまだ硬さが残っていました。そこで、より柔軟かつ自由に、杓子定規な思考からの脱却を目指して新たなデバイスを開発しました。その成果をGboardチームからの新しいご提案として、本日紹介いたします。Gboardスプーン曲げバージョンは、スプーンを自在に曲げることにより文字入力を実現するまったく新しいキーボードです。もうやみくもにスプーンを曲げる時代は終わりです。Gboardスプーン曲げバージョンを、お使いのスマートフォンやパソコンに接続すれば準備完了です。あとはスプーンを曲げるだけ。その角度に対応する文字が入力されます。Aはやさしく、Zは力強く。あなたが思いを込めてスプーンを曲げれば曲げるほど、後ろの方の文字が入力できます。Gboardスプーン曲げバージョンは、あなたの文字入力のすくいになること間違いありません。Gboardスプーン曲げバージョン紹介ページ  g co mageruこれからもより快適な入力方法の実現に向けて、キラリと光る研究開発を続けていきたいと考えています。今年もよろしくお願いいたします。Q 初心者でも使えますか A これまではスプーンを曲げられなかった方でも簡単にご利用いただけます。Q スプーンを自在に曲げるにはどうすればいいですか。A 力を入れるさじ加減を調整してください。Q 自作できますか A ご家庭のDプリンターやプリント基板加工機でご利用いただける設計図、回路図、ファームウェアなどをオープンソースとして公開しています。Q 素敵なハードウェアですね。A どちらかというと柔らかい方です。Q 今日は何の日 A 今日は月日、月曜日です。今日日、だまされないように気をつけましょう。Q 最後のオチをお願いします。A フォークバージョンも検討しています。Posted by Gboardチーム  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/631131486163670957/comments/default",
      "updated": "2019-03-31 15:01:04"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "世界記録を破る“パイ の作り方",
      "description": "毎日、円周率「π パイ 」のことを意識しているという人は、それほどいないでしょう。でも、実は通り過ぎる車のタイヤや、時計といった日常の至るところに、円周率が存在しています。円風率は円周と直径の比で、無理数と呼ばれています。便宜上、普段の計算にはを利用していますが、実際には何桁計算しても終わりがありません。そのため、数学や科学の専門家は、スーパーコンピューターのテストとして、常により多くの桁数の円周率を計算すべく挑戦を続けています。 もちろん、健全なライバル意識も少しありそうですね 月日、円周率の日を記念し、Googleの岩尾エマはるかが、円周率の計算で世界記録を塗り替える  桁を達成したことをお知らせします。この世界記録の達成に、彼女はクラウドコンピューティングを活用しました。クラウドを利用した兆桁超の円周率の計算に成功した例は、これが初めてです。それでは、アイディアから、ギネス世界記録を達成したパイが焼き上がるまでのレシピをご紹介します。ステップ インスピレーションを探す私は歳の時に円周率に夢中になりました。円周率はとてもシンプルに見えます。から始まる数字です。私が子どもだった時、自分のパソコンで円周率を計算するためのプログラムをダウンロードしました。当時、世界記録を持っていたのは金田康正氏と高橋大介氏で、日本人でした。日本で育った自分にとって、彼れらはとても身近に感じられました。その後、大学生の時に師事した先生の一人が高橋大介教授で、当時スーパーコンピューターを使って計算した、最も正確な円周率の記録保持者でした。高橋先生にこの計画を始めることを伝えたところ、いくつかのアドバイスと、技術的な戦略を与えてくれました。ステップ 材料を混ぜる円周率の計算に、y cruncherというアプリケーションを台のGoogle Cloud 仮想マシンで実行しました。円周率の計算で最も難しいのは、非常に多くのストレージとメモリが必要な点です。私の計算はテラバイトのデータを必要としました。これはアメリカ議会図書館の印刷物コレクション全体が持つデータ量に匹敵する大きさです。ステップ  ヶ月、焼く仮想マシンを使った計算は完了までに日かかりました。この間、Google Cloudのインフラストラクチャの上で、サーバーは動き続けました。もし何か障害やサービス中断が発生していたら、計算自体が狂っていたかもしれません。最終結果を確認した時には、結果が正しいことがわかってとても安心しました。その後で、少しずつ、これがチームにとって大きな意味を持つことが分かって来ました。ステップ 出来上がったら、みんなでGoogleでは、私はCloudデベロッパーアドボケイトとして、ハイパフォーマンスコンピューティングとプログラミング言語コミュニティに注力しています。開発者と直接対話し、クラウドをより活用するお手伝いをしたり、製品についての情報を共有したりする仕事をしています。そこで、今回の円周率の計算結果の全数字を、ディスクのスナップショットとして、Google Cloudで公開しました。誰でもこのスナップショットをコピーして、時間以内に計算結果をクラウド上で使うことができます。クラウドが無かったとしたら、この巨大なデータセットを手に入れるためには物理的なハードディスクを郵送する必要があったでしょう。私が子どものときは、スーパーコンピューターを使うことができませんでした。でも、今日では、例え私がGoogleで働いていなかったとしても、様々な奨学制度や支援制度を使ってコンピューターを使うことができます。世の中には、たくさんの解かれるべき数学の問題が存在します。クラウド技術がこれらの解決にどう役に立つのかが、少しずつわかり始めてきました。今後、様々なブレークスルーが生まれてくるだろうと思います。私にとって、身近に感じることができる日本人の世界記録保持者がいたことは、とても幸運なことでした。この円周率の記録を持つ、コンピュータサイエンスにおける数人の女性の仲間入りができたことが本当に嬉しいです。この分野で働こうと思う人たちに、可能性を示すことができたらと考えています。どのようにGoogle Compute Engineを使って円周率を計算したかについての技術的な詳細は、Google Cloud Platformのブログをご参照ください。この新記録を、チームと一緒に大好きなアップルパイ 甘すぎないのが好きなんです でお祝いします。Posted by Cloudデベロッパーアドボケイト 岩尾エマはるか  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/7849911091630878443/comments/default",
      "updated": "2019-03-15 04:37:30"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "人類の歴史を変えた発明物語",
      "description": "望遠鏡は年にオランダのメガネ職人ハンス・リッペルハイが発明したと伝えられています。その後、ガリレオ・ガリレイがデザインを改良し、様々な発見につながったことは皆さんもよくご存知でしょう。そこから数百年の時を超え、打上げられたハッブル宇宙望遠鏡は、地球からマイルの軌道を周回し、以上の銀河の写真ー遠いものでは億光年以上も彼方の銀河までーを鮮明な画像で撮影しています。石器はロボットアームに、蒸気機関はジェットエンジンに、ヒエログリフが絵文字へ、羊皮紙はインターネットに。新しい発明たちは、人々の空想を刺激し、私たちのこの世界を作ってきました。この度、Google Arts and Cultureで公開した「人類の歴史を変えた発明物語」は、その名の通り、歴史的な発明や稀代の発明家たちの偉業を紹介するオンライン展示です。国立科学博物館、日本科学未来館、NHKエデュケーショナルを含む、カ国の科学館や博物館等の収蔵品やそれらにまつわる物語を紹介しています。「人類の歴史を変えた発明物語」では、その先見の明から世界を変えた偉大な科学者を始め、壮大な失敗や偶然の産物、さらには人類が残した科学やテクノロジーにおける飛躍をのインタラクティブな展示でお楽しみいただくことができます。さらに、ストリートビュー、「CERNの大型ハドロン衝突型加速器」や「国際宇宙ステーション」を探検できるだけでなく、年に描かれた「アメリカ大陸が描かれた世界最初の地図」や、今回始めてオンラインで公開された「アインシュタインの手紙」など、万点を超える貴重な文化遺産を高精細画像で鑑賞いただけます。オンライン展示に加え、CERNの協力を得て開発したARアプリ“Big Bang   英語 では、宇宙誕生の物語を度のビジョンでお楽しみいただけます。また、機械学習を利用し、NASAの枚に上るアーカイブ画像を整理したNASA s Visual Universeも圧巻です。ぜひ、お試しください。「人類の歴史を変えた発明物語」は、g co onceuponatryもしくは、iOSまたはAndroidのGoogle Arts and Cultureアプリからどうぞ。Posted by Google Arts and Cultureチーム",
      "link": "http://japan.googleblog.com/feeds/7121598822230325064/comments/default",
      "updated": "2019-03-11 08:30:59"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "日本のGoogleマップが変わります  近日公開予定 ",
      "description": "Googleマップは年に日本でサービスを開始して以来、さまざまな旅をしてきました。無数の道や店舗を地図に反映したり、ビルの中に入ったり、航空写真を撮影したり、D画像を追加したり、富士山を登って頂上からの景色を撮影したり、久慈の海女さんと海に潜ったりしました。日本におけるGoogleマップの次のステップは、より柔軟かつ包括的なマップを皆さまに提供することです。そのために、ストリートビュー画像、交通機関を含む信頼のおける第三者機関から提供される情報、最新の機械学習技術、地域のユーザーの方々からのフィードバックなどを活用し、新しい地図を開発しました。前述の、地域のユーザーからのフィードバックというのは、とても大事なポイントです。Googleはテクノロジーを得意としますが、それぞれの場所を一番熟知している各地のユーザーから提供される情報は地図を作る上で欠かすことができません。新しい地図では、ユーザーの皆さまがより簡単にフィードバックを送れるようになり、いただいた情報を元にGoogleは地図を最新の情報にアップデートすることができます。情報の更新や修正が必要な場合、ぜひマップの「フィードバックの送信」ツールからお知らせください。地図が新しくなることで、「次を右折」だけではなく、「コンビニで右折」といったように、ランドマークを目印とするナビゲーションといった、わかりやすい徒歩ナビゲーション、そして今後数ヶ月の間にはより便利な乗換案内やダウンロード可能なオフラインマップなどの機能が加わります。新しいGoogleマップはまだ公開しておりませんが、今後数週間以内に提供を開始する予定です。いつもの通勤やレストランの検索にGoogleマップを使う際、新しいGoogleマップをご覧になって皆さまが戸惑うことがないよう、早めにお知らせしています。Googleは、今後も皆さまの力をお借りし、Googleマップを最新かつ正確に充実させ、地域の人々、観光客など、誰もが便利に使えるサービスを提供してまいります。Posted by Googleマップチーム   Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/3157503084729160186/comments/default",
      "updated": "2019-03-06 04:00:04"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "海中の海女さんをストリートビュー ",
      "description": "海女さんといえば、素潜りで一気に海底まで下りて、岩に張り付いた貝や海藻をとる女性漁師ですが、この独特な漁法は日本と韓国にしか存在しないそうです。映画やドラマで広まりましたが、実際どんな活動をしているのかはあまり知られていません。特に海の中で彼女たちがどんな風に漁をしているのか、見たことがある人は少ないはず。そんな海中の海女さんを見ることができるストリートビューが公開されました。岩手県の小袖海岸は、日本では最北の海女の漁場。今でも夏から秋にかけてのシーズン中には地元の海女さんたちが昔ながらの伝統漁法でウニを採っています。小袖独特の伝統の衣装がかわいいですね。腰につけているのはヤツカリという、収穫物を入れる袋ウニがこんな感じで岩に張り付いています撮影が行われたのは昨年月。シーズン終了直前の、台風が通り過ぎた翌週でした。台風の影響でまだ潮の流れも早く海水も濁っていますが、久慈市のスタッフが、その年最後のチャンスと撮影を決行します。撮影は、三脚に水中用ハウジングをつけた度カメラを海底に置いて一点一点撮影する方式です。重りをつけた三脚を海中で移動させるのはかなりの重労働。自分の体にも一気に潜るための重りがついています。海女漁は冷たい水との戦いでもあり、体が冷え切ってしまう前に漁を終えて陸に上がって来る必要があります。上がる時に藻が絡みついたりヤツカリが引っかかったりと危険もいっぱい。熟練の技が必要な職業で、ストリートビュー撮影もそんな彼女たちだからこそできたのでしょう。撮影した画像はスマホのストリートビューアプリからアップロード。グーグルマップで世界中に発信完了 日本の海の幸であるウニを美味しくいただけるのも彼女たちの体を張った素潜り漁のおかげですね。ごちそうさまです 海女さんの撮影の様子を、こちらのビデオからもどうぞ Posted by Street Viewプログラムマネージャー、長沼由希子",
      "link": "http://japan.googleblog.com/feeds/8716467348882257261/comments/default",
      "updated": "2019-02-25 03:19:08"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "あなたのデータを守るために",
      "description": "Googleでは、Google製品を利用している間だけでなく、お気に入りのウェブサイトを閲覧したり、アプリを利用したりしているような、どんな場合でも皆さんのデータが守られるように、絶えず様々な取組みを行っています。今年のサイバーセキュリティ月間にあわせて、Googleでは、サードパーティのサービス上で、データ侵害の被害から皆さんのアカウントを守るChromeエクステンション「 Password Checkup パスワードチェックアップ 」と、「クロスアカウントプロテクション Cross Account Protection 」を公開しました。これにより、Googleが提供する製品やアプリ、ウェブサイトの垣根を超えて、皆さんのデータをより安全に保護します。Password CheckupGoogleではみなさんがお使いのGoogleアカウントを守るために、セキュリティ上の脅威を検出し、速やかに対応しています。例えば、サードパーティ 第三者 のサービス上で流出した可能性のあるパスワードがGoogleアカウントでも使われているような場合、自動でアカウントをロックし、ユーザーにパスワードをリセットするように促します。これにより、アカウント乗っ取りの被害に遭うリスクを抑えることができます。さらに今回、Google以外のウェブサイトやアプリを利用している時でも、Googleアカウントと同様のアカウント保護対策を提供するために、今月、新しいChromeエクステンションPassword Checkupの提供を開始しました。Password Checkupは、Googleが把握する範囲で、ウェブ上で流出した認証情報が、あなたがあるサイトで入力したIDやパスワードと一致するかどうかを確認します。一致する情報を検出した場合には、自動的に警告画面を表示し、パスワード変更を促します。利用は簡単で、Chromeブラウザに、このエクステンションを追加するだけです。このPassword Checkupは、スタンフォード大学の暗号技術研究チームと協力し、プライバシー保護のための手法を開発しました。そのため、Googleを含め、何人もあなたのアカウント情報の詳細を把握することはできないようにデザインされています。技術の詳細についてはGoogle Security Blogの記事 英語 をご覧ください。Password Checkupは、今後数ヶ月をかけて、さらに改善を続けて行きます。ぜひ、こちらのリンクからお試しください。Cross Account Protection非常な稀なケースではありますが、第三者がGoogleアカウントに不正アクセスした場合に備え、Googleではアカウント復元のためのツールを提供しています。しかしながら、Googleアカウントを利用してサインインするような第三者提供のアプリでは、こうしたアカウント保護は利用できませんでした。今回提供を開始した nbsp Cross Account Protection nbsp は、アプリやサイトが同機能に対応している場合に、セキュリティを脅かす事象の発生情報 アカウントの乗っ取り等 をGoogleから取得することで、ユーザーアカウントをより安全に保つための仕組みです。また、ユーザーのプライバシーを守るために、共有の対象となる情報を極めて限定的に規定しました。セキュリティを脅かす事象が起きた事実のみを共有する。セキュリティを脅かす事象の基本的な情報のみ アカウントが乗っ取られた、怪しい兆候があったためGoogleがユーザーに再ログインを求めた、等 を共有する。これらの情報は、Googleアカウントでログインしたアプリのみと情報を共有する。今回、GoogleはAdobeやthe Internet Engineering Task Force  IETF の標準化コミュニティ、OpenID Foundationといったテクノロジー業界各社と共同で、同機能の開発を行いました。これにより、多数のアプリで同機能のスムーズな導入が進むことを期待しています。FirebaseまたはGoogle Cloud Identity for Customers  amp  Partnersでは、デフォルトでこの機能が導入されています。多くの開発者のみなさんが、ユーザーのセキュリティを守るために活用してくださることを願っています。GoogleではPassword Checkupや nbsp Cross Account Protection 等を通じ、Googleユーザーのみならず、インターネットを使うすべてのユーザーのセキュリティがより堅牢に守られるよう、引き続き、取り組んで参ります。Posted by Kurt Thomas  Security and Anti Abuse Research Scientist   Adam Dawes  Senior Product Manager  Developer Tools for Identity   Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/348668676221827072/comments/default",
      "updated": "2019-02-22 07:34:32"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "インターネットを安全に使うために、今すぐできるつのこと",
      "description": "月日月日は、日本政府が主導する「サイバーセキュリティ月間」です。Googleもこの取組みに参加しています。今日は、インターネットを安全にお使いいただくために、今すぐできるつのことをご紹介します。皆さんがインターネットを利用する時に、自分の情報が守られているか、セキュリティは大丈夫かを心配しなければいけないような環境は、ふさわしくないと私たちは考えています。そのために、Googleでは様々な製品でセキュリティを守るための機能を導入し、Googleアカウントに保存されたみなさんの情報を自動的に保護しています。それだけでなく、Googleではセキュリティツールや知見を他の組織とも共有し、誰にとってもインターネットがより安全な場所になるように取り組んでいます。その上で、みなさん自身が自分の情報をさらに安全に守るためにできることがいくつかあります。最近GoogleがHarris Pollと共同で実施した調査 英語 では、多くの人がこうした簡単な対策について馴染みがないという状況が明らかになりました。この機会に、インターネットを安全に使うためにできるつのことを確認しましょう。 アカウント復元用の電話番号またはメールアドレスを設定する。そして、その情報を最新に保つ。調査の回答者の多くが、アカウント復元用の第二メールアドレス   やモバイル端末   を設定していると回答しました。これは素晴らしいことです。Googleアカウントを含む多くのウェブサービスでは、アカウントの復元プロセスやアカウント上での不審なアクティビティの警告や第三者による不正ログインのブロックといった機能を提供しています。復元用の情報を追加することによって、アカウントにアクセスできなくなった際により早くアカウントを復元することができます。アカウント復元用の情報の追加は、Googleアカウントのセキュリティメニュー内の Googleによる本人確認の方法 から設定できます。 安全なパスワードを設定する調査結果では、 の回答者が複数のサービスで同じパスワードを使い回していると回答しました。パスワードの使い回しは、同じ鍵を自宅、車、会社の鍵として使い回すようなもので、セキュリティリスクを増大させます。一つでもパスワードが破られてしまえば、同じパスワードを使う全てのサービスにアクセスできるようになってしまいます。安全なパスワードをサービスごとに設定することはリスクを減少させます。それぞれのパスワードを推測されにくく、少なくとも文字以上の長さで設定しましょう。全てのパスワードを覚えておくのは難しいかもしれません。 のユーザーが「覚えるべきパスワードが多すぎて記憶しきれない」と回答しました。そうした場合は、パスワードを保存してくれるパスワードマネージャー 例えばChromeブラウザにビルトインされているもの を使うことをおすすめします。もし、それも難しければ、パスワードを紙のメモに書き留めておくのも一案です 必ず安全な場所に保管してください  。アカウントのハイジャックを狙う人たちは多くの場合インターネット上に潜んでおり、物理的にあなたのすぐそばにいることは稀だからです。 ソフトウェアを最新版に更新するインターネットで安全に過ごすために、いつも最新版のソフトウェアを全ての端末上で利用するようにしましょう。先程の調査では、 がソフトウェアをアップデートすることの重要性を理解していると回答しましたが、一方で分のがアップデートを実施していないか、しているかどうかわからないと回答しました。ソフトウェアのバージョンを確認しましょう。そして、アップデートされていないならば、ご利用のOS  オペレーションシステム ごとに以下方法にそってアップデートしてください。Windows supportMacAndroidiOSChromeなどのいくつかのソフトウェアには、自動アップデート機能があり、アップデートしたかどうかを心配する必要がありません。お使いのソフトウェアで、更新を促すメッセージが表示されたら、「あとで実行する」を選ばずに、すぐに、その場でアップデートをしてしまいましょう。  段階認証を利用する段階認証 または要素認証 を利用すると、第三者があなたのアカウントに不正ログインする可能性を格段に減少させることができます。多くの皆さんにおいては、Googleが自動的に実施するリスクに基づいたアカウント保護でも十分ですが、しっかりとアカウントを守るためにも、段階認証について知っておくことをおすすめします。調査の中では、分の    の回答者が段階認証を知らないか、使っているかどうかわからないと回答しました。段階認証を利用すると、ユーザーはログイン時にアカウントとパスワード以外にもう一つ追加のアクションを取ることが求められます。例えば、SMSメッセージで送られてきたコードや、アプリで生成されたコードなどを入力したり、物理的に端末に挿入したセキュリティキーに触れるといったアクションが求められます。たとえパスワードが盗まれたとしても、段階認証を使うことで、第三者があなたのアカウントにログインすることは困難となります。段階認証は、g co svから設定することができます。 セキュリティ診断をする。セキュリティ診断を利用することで、お使いのGoogleアカウントにあわせたおすすめのセキュリティ対策を確認し、実施することができます。診断は分程度で簡単に行えます。セキュリティ診断をすることで、Googleの製品を安全に使うことができるだけでなく、インターネットを安全に使うための様々なヒントを得ることができます。例えばモバイル端末の画面ロックの設定や、あなたのアカウントへアクセス可能な危険なサイトや、使っていないアプリを削除するといったアドバイスが得られます。さらに詳しいヒントはセーフティセンターから、この記事でご紹介した設定やツールについては全てGoogleアカウントの セキュリティ ページから確認できます。ぜひ、サイバーセキュリティ月間中に一度、このつのことを試してみてくださいね。Posted by nbsp  Guemmy Kim  Product Management Lead  Account Security",
      "link": "http://japan.googleblog.com/feeds/8428403704062632301/comments/default",
      "updated": "2019-02-20 08:35:54"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "“会話 をサポートする新しいつのアプリ",
      "description": "世界保健機関は、聴覚障害を持つ人の数が年には億人に達すると予測しています。Googleは、テクノロジーの力で障壁を取り除き、人々の生活を少し楽にするお手伝いができると考えています。この度、聴覚障害を持つ方々に向けて新しく「音声文字変換」 Live Transcribe と「音声増幅」 Sound Amplifier のAndroidアプリの提供を開始しました。音声文字変換で会話に字幕を音声文字変換は、スマートフォンのマイクを使って、話している相手の音声をリアルタイムで文字に変換します。日本語を含む以上の言語で利用できるほか、キーボード入力による会話にも対応し、発声できない、あるいは発声を望まない方との会話にもご利用頂けます。また、外付けマイクを接続して、字幕の精度を上げることも可能です。音声文字変換は、聴覚障害を持つ人々の日常的なやり取りを、より自立的にするためのサポートをします。本アプリは、聴覚障害を持つGoogle研究者のDimitri Kanevskyが日常会話において抱えていた困難に着想を得て開発されました。音声文字変換によって、Dimitriは、家族や周囲の手を借りることなく歳の双子の孫娘たちと自由にコミュニケーションをとれるようになり、愛する家族との関係を深めることができました。音声文字変換の開発で協力を仰いだギャローデット大学 世界有数の聴覚障害者を対象にした大学 からも、音声文字変換が聴覚障害者のコミュニティのニーズを満たしているというフィードバックが寄せられています。音声文字変換をご利用いただくには、「設定」にある「ユーザー補助」で「音声文字変換」を有効にしてください。画面の下部にあるナビゲーションバーにユーザー補助ボタンが現れますので、このボタンをタップしてお使いいただけるようになります。音声文字変換はGoogle Playストアで公開されているベータ版のアプリを取得してご利用頂けるほか、Google Pixel 端末向けに順次プリインストールされます。こちらのページよりご登録いただくと、アプリの正式版の公開時に通知を受け取ることができます。音声増幅で音声をクリアに賑やかなカフェや空港ラウンジなど、周囲の音が大きい場所では、音量の強調を調整することで聞き取りやすくすることができます。昨年のGoogle I Oで発表され、このたび公開した音声増幅は、このような場面で役立つアプリです。音声増幅は、有線ヘッドフォンを接続したAndroidスマートフォンを利用して、周囲の音をフィルタリング、増強、および増幅できます。大きな音を低減しながら静かな音だけを大きくすることにより、音声をよりクリアにし、聞き取りやすくします。音声やノイズの低減などは音声増幅の「設定」でカスタマイズができ、周囲の騒音を抑えるのに役立ちます。音声増幅はGoogle Playストアからアプリを入手することでAndroid  Pie以降の端末でご利用頂けるほか、Google Pixel 端末向けに順次プリインストールされます。音声文字変換と音声増幅のアプリが、聴覚障害を持つ方々のコミュニケーションの一助となれば幸いです。Posted by Androidチーム  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/1216019523825629290/comments/default",
      "updated": "2019-02-07 08:22:20"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "一般ユーザー向けのGoogle 提供終了について",
      "description": "Google の提供終了について、先日お知らせしましたが、本日はいくつかの追加情報をご案内します。Google は年月日に提供を終了致します。これに伴い、Google にアップロードしたコンテンツの保存を希望する方は、月までに、以下にご案内する方法でご対応下さい。よろしくお願い致します。また、Google に登録されている、すべてのユーザーのコンテンツを削除するには、数カ月程度かかりますので、月日以降もアップロードや登録したコンテンツが引き続き閲覧できる可能性があります。ただし、ご自身のコンテンツの保存は、月以前に作業を始める必要がありますので、ご注意下さい。Google は、一般ユーザーから企業での利用までを含む多様なユーザー層にご活用いただいています。今回の変更に対して、それぞれのユーザーが何をすればよいのかを分かりやすく説明するために、以下に整理してご案内します。一般ユーザーの場合年月日以降、Google アカウントおよびページの停止を順次開始します。削除対象となるのは、一般ユーザー向けGoogle アカウント内のコンテンツです。これには、Google アルバムアーカイブの写真や動画およびGoogle ページが含まれます。Google にアップロードしたコンテンツの保存は、月日までに行う必要があります。コンテンツをダウンロードおよび保存する方法は、Google アカウントのデータのダウンロード手順を参照してください。詳細は、ヘルプセンターのこちらの投稿、またはよくある質問をご覧ください。G Suiteユーザーおよび管理者の場合一般向けGoogle は提供終了しますが、エンタープライズ向けのGoogle は、今後も提供を継続するとともに、製品の改善を続けてまいります。G Suiteの一部としてGoogle を利用している場合は、引き続き、現在ご利用のアカウントをお使いいただけます。G Suiteのユーザーには、有償サービスを利用しているビジネスユーザー、及び無償でG Suiteを利用されている教育機関やその他の組織が含まれます。年後半を目処に、エンタープライズ向けのGoogle については、新たなご案内をしたいと考えています。ご期待下さい。所属している組織がG Suiteを利用しているかどうかわからない場合はこちらを参照して下さい。また、G Suite管理者の立場から、一般ユーザー向けGoogle 提供終了がG Suiteを利用する上で与える影響について確認したい場合は、ヘルプセンターをご覧ください。Google  APIを利用されている開発者の場合APIにおいても、すべてのGoogle  APIの提供を終了します。これには、Google  Sign inおよびGoogle  OAuthのスコープも含まれます。Googleでは、提供停止に向けた手順をすでに開始しており、年月日のAPI完全停止に向け、断続的にAPIが停止する状態が発生する場合があります。Google  APIの提供終了に関する詳細はこちら 英語 をご覧ください。Google  Sign inからGoogle Sign inへの移行に関する開発者向け情報はこちらをご覧ください。これまでGoogle をお使いいただき、活発なコミュニティを支えてくださったすべてのユーザーの皆さんに、この場を借りて、心からの感謝をお伝えします。ありがとうございました。Posted by Google チーム  Abbi Tatton Editorial Elf Google    ",
      "link": "http://japan.googleblog.com/feeds/60768309604978236/comments/default",
      "updated": "2019-02-01 00:00:02"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "Googleしごと検索で、仕事探しをもっとスムーズに ",
      "description": "アルバイト、転職、キャリアアップなど、仕事探しの理由は様々ですが、条件にあった求人情報を求めて、何時間もスクリーンとにらめっこをした経験は誰しもあるでしょう。本日より、Googleしごと検索の提供を開始します。この新機能は、ウェブ上の転職・求人情報サイトや企業の採用ページ等から求人情報を探して、検索結果に分かりやすく表示します。 渋谷バイト   本屋アルバイト や、 パンケーキシェフ募集 のように、希望する職種やエリア等を入力して検索すると、関連する求人情報が検索結果内にひと目で分かりやすく表示されます。任意の求人情報をクリック、または、タップするとより詳細な求人情報が表示されます。ログインして検索を利用している場合には、右上の「保存」をクリックもしくはタップして、気になる求人情報を保存しましょう。保存した求人情報は「保存済み」のタブで確認することができます。ウォッチしたい求人情報がある時には、企業名や店舗名等で検索し、「この検索に関するメール通知アラートをオン」にしてください。関連情報がメールで届きます。応募したい求人が見つかったら、応募ボタンから、その求人情報を掲載している大本のサイトにアクセスしましょう。実際の応募は、それぞれの募集サイトから行って下さい。しごと検索では、サイト管理者が構造化データをご用意いただくだけで、自動的に求人の情報をクロールし、検索結果に表示されるようになります。ご興味のある方は、こちらの資料をご確認下さい。しごと検索が、あなたにぴったりの仕事を見つけるお手伝いになることを期待しています。ぜひ、ご活用下さい。Posted by  Googleパートナーシップマネージャー内海友紀子",
      "link": "http://japan.googleblog.com/feeds/5597847887363588535/comments/default",
      "updated": "2019-01-23 04:03:20"
    },
    {
      "name": "Google Japan Blog",
      "category": "GCP",
      "title": "今年は共有されたサンタさんの現在地を見て、Googleと一緒にサンタを追いかけよう ",
      "description": "今年もついにクリスマスイブがやってきましたね サンタさんもサンタ夫人も、いい子にしていたみんなへのプレゼントの用意に大忙し みなさんはもう、サンタ村の楽しくてためになるゲームで遊んでみましたか 今年追加した新しいゲームでは、北極のあちこちで開催されるクリスマス会にふさわしいスタイリングを選んで、妖精を全身コーディネートしましょう。衣装、アクセサリー、髪型、髭を選んで、小さな友人をセンスよく飾ってあげてください。今年はサンタさんが現在地をみんなと共有 サンタ夫人がサンタさんと結婚してから最も困っていることは…、それは彼が今いる場所を知らせるのをいつも忘れることです。そこで、北極から飛び立ったサンタさんの現在地が、みんなにもわかるように、今年は地図作りが得意な妖精のチームを集めました クリスマスに向けて、サンタさんの今いる場所がGoogleマップで共有され、サンタの旅をマップ上で見ることができます。今年はGoogleマップ、サンタトラッカーアプリ、さらにGoogleアシスタントでサンタさんを追いかけて、訪問のタイミングを絶対に逃さないようにしましょう。サンタクロース村より、皆さまの素敵なクリスマスを祈って。Posted byサンタ村の妖精  Abbi Tatton Editorial Elf Google         ",
      "link": "http://japan.googleblog.com/feeds/7607134824974687160/comments/default",
      "updated": "2018-12-24 00:00:18"
    }
  ],
  [
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "javascriptのブックマークレットが動作しない",
      "description": "下記のような形に、クラスの値を変更するためのブックマークレットを作成したいと考えております。\n\n  result headline  \n display  block important \n \n\n  resultBox \n height px important \n \n\n\n試したのは、以下のコードです。\nブックマーク内の記述を下記に致しました。\n\njavascript  function show   \n var e   document getElementsByClassName  result headline   \n e style display    block  \n   \n\n\n  resultBoxの場合、getElementById\nを使うことまでは、分かりましたが、実際の記述の仕方は分かりませんでした。\n\nお詳しい方に、アドバイスなど頂けますと幸いです。\n何卒どうぞよろしくお願い致します。",
      "link": "https://qiita.com/kenta6@github/items/bd6b4b35b158c59eac42",
      "updated": "2019-07-08 04:33:54"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "グラフAPIを使って任意のハッシュタグを持つ投稿をインスタグラムから取得する",
      "description": "\n今回の主目的\n\nインスタグラムグラフAPI用いて\n任意のハッシュタグ付きの投稿一覧を取得する\n\n\n  一時的なアクセストークンを取得する\n\nまず、\n\n\n\n\n上記URLにアクセスして、まず短期的なアクセストークンを取得する\n【トークンを取得】\n【ユーザーアクセストークンを取得】\n【アクセス許可を選択】\n\n\n必要なアクセス許可をすべて選択する\n\n\n\n\n上記の設定を行い、【アクセストークンを取得】ボタンをクリックする。\n   すると \n\n【アクセストークンのテキストボックス内に短期的なアクセストークンがセットされる】\n\n\n\n\naccess token js\n  取得した短期的なアクセストークン\n  gt  【EAAKErsAkB           bIlOKACDUkQWQZDZD】\n\n\n\n\n  前述の項目  の短期的なアクセストークンを利用して中長期のアクセストークンを取得する\n\n手順 \n短期的なアクセストークンを用いて長期なアクセストークンを取得する\n以下APIエンドポイントをエクスプローラーに設定する\n\n\nendPointToGetLongAccessToken js\n\noauth access token client id  app id \n amp client secret  client id \n amp grant type fb exchange token\n amp fb exchange token  一時的なaccess token \n\n\n\n\nendPoint js\noauth access token client id     amp client secret ead     d amp grant type fb exchange token amp fb exchange token EAAKErsAkB           bIlOKACDUkQWQZDZD\n\n\n\n上記APIを実行する\n\n取得した長期的なアクセストークン \n\n\nJsonWrittenAboutLongAccessToken json\n \n    以下access tokenが長期的なアクセストークン\n  access token    EAAKErsAkBAJ      gZDZD  \n  token type    bearer \n \n\n\n\n\n  長期的なアクセストークンの取得に成功  \n\n\n\n取得したアクセストークンが本当に長期的なものかどうかを\n\n\n\n\n上記アクセストークンデバッガーを利用して検証する。\n\n手順 \n取得した長期的なアクセストークンを用いてインスタグラムのuser id instagram business account を取得する\nme accounts  access token EAAKErsAkBAJ      gZDZD\n\n\n \n  data    \n  \n  access token    EAAKErsAkBAM      FmoihQHZADwZDZD  \n  category    Community  \n  category list    \n  \n  id      \n  name    Community \n  \n   \n  name    所持しているFBページ名  \n  id      \n  tasks    \n  ANALYZE  \n  ADVERTISE  \n  MODERATE  \n  CREATE CONTENT  \n  MANAGE \n  \n  \n   \n  paging    \n  cursors    \n  before    MzcNDIMjAyNjQNDc  \n  after    MzcNDIMjAyNjQNDc \n  \n  \n \n\nme accounts のエンドポイントへそのままアクセスしても有効なインスタグラムuser idは取得できない\n\n\n\n\n\n手順 長期的なアクセストークンからinstagram business accountを指定してuser idを取得する\n\nもう一度、下記APIエンドポイントへリクエストする\nme accounts  access token EAAKErsAkBAJ      gZDZD\n\n\n \n  data    \n  \n  instagram business account    \n  id          \n   \n  id         \n  \n   \n  paging    \n  cursors    \n  before    MzcNDIMjAyNjQNDc  \n  after    MzcNDIMjAyNjQNDc \n  \n  \n \n\n\n\n\n  instagram business account    \n  id          \n\n\n上記、値がインスタグラムのuser idとなる\n\n\n手順 任意のハッシュタグのIDを取得する\nig hashtag search user id       amp q 任意のハッシュタグ amp access token EAAKErsAkBAJ      gZDZD\n\n\n \n  data    \n  \n  id             指定したハッシュタグID\n  \n  \n \n\n\n\n\n\n手順 指定したハッシュタグを含む投稿データを取得する\n\n     top media user id       amp access token EAAKErsAkBAJ      gZDZD\n\n上記APIエンドポイントをリクエストする\n\n\n \n  data    \n  \n  id     \n   \n  \n  id     \n   \n  \n  id     \n   \n  \n  id     \n   \n  \n  id     \n  \n  \n \n\n\n以上の様にマッチする投稿を取得できるがこれだけでは使いようがないのでリクエストを変更する\n\n\n     top media user id       amp fields media url media type comments count id like count children media url permalink  permalink caption amp access token EAAKErsAkBAJ      gZDZD\n\n\n\n\n \n  data    \n  \n  media url      nc oc AQl XfViAipKSpGbPmxaXEAeZb NmdwkDWShytHbR PGNGYffrc mUlqvuk amp  nc ht scontent xx amp oh bcbbadddfbecf amp oe DAEFB  \n  media type    IMAGE  \n  comments count    \n  id      \n  like count    \n  permalink      \n  caption     任意のハッシュタグ \n   \n  \n  media url      nc oc AQnpzzEwfsHyCIwQeCwEwjZkvUJredXXLNcrreI TaWouySnmpxkQ amp  nc ht scontent xx amp oh cdbcfadaafafcdb amp oe DCBCC  \n  media type    IMAGE  \n  comments count    \n  id      \n  like count    \n  permalink      \n  caption     任意のハッシュタグ \n   \n  \n  media url      nc oc AQnGVvnCbdgp vK VvhTVDaaqDIpSOV jCpNqeSucsMgRwTn nOrIBpU amp  nc ht scontent xx amp oh bfdcdfeceadfefec amp oe DA  \n  media type    IMAGE  \n  comments count    \n  id      \n  like count    \n  permalink      \n  caption     任意のハッシュタグ \n   \n  \n  media url      nc oc AQkZSUhjDwPkN upyXodhPuCFzAPECtOBK iLMtmeebYYDLRCfHWqHBsGTE amp  nc ht scontent xx amp oh aacbaadbabefc amp oe DBCD  \n  media type    IMAGE  \n  comments count    \n  id      \n  like count    \n  permalink      \n  caption     任意のハッシュタグ \n   \n  \n  media url      nc oc AQmnfVltSUoLXOorqkLkpAcCC j gVNQBBLDRmgZw pyPCVdGqhCrhiIADTo amp  nc ht scontent xx amp oh fdfdececfec amp oe DBCBD  \n  media type    IMAGE  \n  comments count    \n  id      \n  like count    \n  permalink      \n  caption     任意のハッシュタグ \n  \n  \n \n\n   上記の様なハッシュタグを持つ投稿の詳細情報を取得できる。   \n\n",
      "link": "https://qiita.com/1000VICKY/items/684a5702e160d0382d23",
      "updated": "2019-07-08 02:31:21"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "関数について",
      "description": "JavaScript本格入門 ISBN   で基礎からJavaScriptを勉強するシリーズです。今回はChapterから関数についてです。\n\n\n関数の定義\n\n方法は主につあります。\n\n\nfunction命令を使う\nFunctionコンストラクタを使う\n関数リテラル表現を使う\nアロー関数 ES を使う\n\n\nしかし結論として、日常的に使うのはアロー関数でよさそうです。\n\n\nfunction命令を使う\n\nfunction 関数名 引数   \n 関数本体\n  \n\n\n\n例\nfunction myFunc arg   \n return arg   arg \n  \nlet squared   myFunc   \nconsole log squared     出力 \n\n\n\n有名なfunction命令を使った関数定義ですが、静的な構造の宣言であったり意外とトリッキーな動きをします。宣言していないのに関数が使えるような動きをするのでバグを生みやすいので使わないほうがいいです。\n\n\nコード解析時に関数の登録が行われるので、先に使おうとしても使える\nlet squared   myFunc   \nconsole log squared     出力 \n\nfunction myFunc arg   \n return arg   arg \n  \n\n\n\n\nFunctionコンストラクタを使う\n\nlet 変数名   new Function  引数   関数本体   \n\n\n\n例\nlet myFunc   new Function  arg   return arg arg   \nlet squared   myFunc   \nconsole log squared  \n\n\n\n実行時に文字列をコードとして実行するのは、eval  等と同じくセキュリティ的によくないので使わないほうがいいです。\n\n\n関数リテラル表現を使う\n\nlet 変数名   function 引数   関数本体  \n\n\n\n例\nlet myFunc   function arg   \n return arg   arg \n \nlet squared   myFunc   \nconsole log squared  \n\n\n\nfunction命令を使った場合と似ていますが、無名関数を定義してからmyFuncという変数に格納しています。\n後述のアロー関数が使えない場合はこれを使うのがベターだと思います。\n\n\nfunction命令とは違い静的な関数定義とはならない\nlet squared   myFunc       ReferenceError  myFunc is not definedとなる\nconsole log squared  \n\nlet myFunc   function arg   \n return arg   arg \n \n\n\n\n\nアロー関数を使う\n\nESからはアロー関数によるユーザー定義関数が使えます。\n利用可能なら基本的にはアロー関数での定義を使う方がよさそうです。\nアロー関数には下記のような特徴があります。\n\n\nthisが固定化されること\n\n\nこれにより、関数の挙動が分かりやすく、バグを減らすことにつながりそうです。\n\nlet 変数名    引数  引数         gt    関数本体   \n\n\n\n例\nlet getTriangleArea    base  height    gt   \n return base   height    \n  \nconsole log getTriangleArea      \n\n\n\n引数がつの場合、括弧が不要だったり色々省力出来ます。\n\n\n関数はデータ型の一種\n\nJavaScriptの世界では、関数は変数のデータ型の一種です。\n\nfunction myFunc arg       の方法\n return arg   arg \n  \nconsole log typeof myFunc  \n\nlet myFunc   new Function  return arg  arg        の方法\nconsole log typeof myFunc  \n\nlet myFunc   function arg       の方法\n return arg   arg \n  \nconsole log typeof myFunc  \n\nlet myFunc        gt       の方法\n return arg   arg \n  \nconsole log typeof myFunc  \n\n\n\n実行結果\nfunction\nfunction\nfunction\nfunction\n\n\n\n関数とは言えデータ型のの種なので、別の型のリテラルを代入すると変数は別の型になります。\n\nfunction myFunc arg   \n return arg   arg \n  \nconsole log typeof myFunc     出力 function\n\nmyFunc       数値型のリテラルを代入\nconsole log typeof myFunc     出力 number\n",
      "link": "https://qiita.com/tkymeno2/items/93b3699fb0a691f8eac0",
      "updated": "2019-07-08 01:11:04"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "TypeScriptによるバックエンドとフロントエンドの両面開発",
      "description": "\nTypeScriptによるバックエンドとフロントエンドの両面開発\n\n ※全てをTypeScriptで記述したツリー型情報掲載システムを運用中です\n\n\n 言語の選択\n\n Web開発を一つの言語で行おうと思った場合、事実上の選択肢は素のJavaScriptを使うか、AltScript系を使うことになります。\n\n前者のJavaScriptは気軽にコーディングできる反面、気付かぬうちに危険なコードが混入します。JavaScriptは仕様をきっちり把握していればある程度回避できますが、実は地雷原をサンダルで散歩するに等しい言語なのです。今日は無事でも、明日は足が吹っ飛ばされるかもしれません。\n\n JavaScriptの危険性を回避するにはAltScript、中でも急速に伸びているTypeScriptを強くお勧めします。TypeScriptが一つ使えれば、フロントエンドとバックエンド両方が同じ言語で、しかもある程度の安全を担保した状態で開発出来るのです。\n\n\n TypeScriptによる両面開発の利点\n\n\n同じ文法なので効率が上がる\n例えばバックエンドをPHP、フロントエンドをJavaScriptで開発したケースです。この二つの言語、文法が似ています。似ているが故に同時開発すると、拡張for文などでミスります。PHPでfor inを使おうとしてしまったりと、一瞬で気が付くミスですが、けっこうやってしまいがちです。完全に同じ文法の言語を使えば、こういった部分でのストレスがありません。\nソースコードが使いまわせる\nフロントエンドとバックエンドで同じような処理を書く場合があります。言語が違えばいちいち書き直しですが、同じ言語ならコピペで終了です。共通ライブラリとして整備しておけば、コピペの必要すらありません。\nデータのやり取りが楽\n TypeScriptなら型が組めるわけですが、同じ言語ならこの型が共通化できます。特に威力を発揮するのは通信部分です。バックエンドとフロントエンドの通信は、Ajaxを用いた非同期通信を用います。やりとりの標準的な方法では、いったんJSON形式を通してパースし、お互いの終端でデコードして使います。TypeScriptなら、このデコード後の型が共通化できるのです。これが出来るか否かで、開発効率がまったく変わってきます。感覚的にはローカルファンクションを呼び出す感覚で通信が可能となるのです。\n\n\n\n 両面開発で気を付けるべきこと\n\n\n開発環境の設定\n\n VSCodeなどの開発環境は、TypeScriptの文法チェックをするときにtsconfig jsonを参照します。この設定値を元にチェック項目を判断するわけです。当然ですがバックエンドとフロントエンドでこの設定値が異なります。特に入出力ディレクトリは確実に異なった設定になるので、きっちり区別をつけておかないと、大量のエラーに悩まされることになります。\n\nまず最初にやらないといけないのは、プロジェクトルートのディレクトリに配置するtsconfig jsonです。\n\n\ntsconfig json\n \n  exclude    \n    \n  \n \n\n\n\nこれを置いておかないと、全てのディレクトリの tsが混合でチェックされてしまうので悲惨なことになります。これを配置したら、あとはフロントエンド用とバックエンド用のディレクトリそれぞれにtsconfig jsonを作ればOKです。コンパイル時、手動でそれぞれコンパイルするなら\n\ntsc  b configファイルのパス\n\n\nとします。\n\nまた、異なるtsconfig jsonをトップのtsconfig jsonから呼び出すこともできます。\n\n\ntsconfig json\n \n  references    \n  \n  path    フロントエンドパス \n    \n  path    バックエンドパス \n  \n   \n  exclude    \n    \n  \n \n\n\n\nとしておけば、\n\ntsc  b\n\n\nだけで、両方同時にビルドすることができます。ただし、フロントエンドはWebPackなどのモジュールバンドラを使うケースが多いのでその場合、ここに記述するのはバックエンドの参照のみになります。また、参照を受け付ける場合は、参照先の設定に\n\n\ntsconfig json\n \n  compilerOptions    \n  composite   true\n  \n \n\n\n\nを入れておく必要があります。\n\n\n出来ることの違い\n\nバックエンドはファイルの入出力やDBアクセスなど、一通り何でもできます。ただしDOM操作などは標準対応しておらず、自分で組むか、jsdomのようなパッケージをnpmからとってくる必要があります。\n\nフロントエンドはDOM操作が標準でできますが、当然ファイル操作などは出来ません。ブラウザ内で許可されているlocalstrageを使ったり、バックエンドに入出力要求する必要があります。\n\nということで使用可能なAPIはまったく異なるので、ここだけはきちんと区別して開発しなければなりません。\n\n\n 最終的な開発環境とビルド手順\n\n現在私の開発方法はフロントエンドにWebPack、バックエンドにtscという形で行っています。tscの方は参照設定で複数のプロジェクトを一コマンドでコンパイル出来るのですが、WebPackにはプロジェクトの参照設定がないので、ちょっと面倒くさいことになっています。\n\n\ntsc  バックエンドメインプロジェクト  Active Module Framework\n\nWebPack  フロントエンドメインプロジェクト\n\nWebPack   JavaScript Window Framework\n\n\n\nバックエンドはActive Module Frameworkというオレオレフレームワークを使っており、バックエンドのメインプロジェクトとは独立させた構成になっています。tscは参照設定を使うことによって、この異なるプロジェクトの同時ビルドが可能です。\n\nフロントエンドはJavaScript Window Frameworkというオレオレフレームワークを使っており、フロントエンドのメインプロジェクトとは独立させた構成になっています。WebPackには参照設定がないので、フロントエンドのビルドには、WebPackを二回立ち上げます。 wオプションを入れて監視ビルドさせるので、最初にそれぞれ一回起動する手間があるだけなのですが、やっぱりちょっと面倒くさいです。\n\n\n 利点は大きいが移行するまでの壁も大きい\n\n TypeScriptによる両面開発は数々のメリットがある反面、壁が大きいのも確かです。まずNode jsの非同期処理に慣れるまでそれなりに時間がかかります。これは素のJavaScriptでも同じですが、他の言語には無い特徴故に慣れるまでに時間を要します。さらにTypeScriptの型の扱いも、ケースごとにどう対処するのかという知識を蓄積するまでに、多少の時間をとられます。この壁を乗り越えた先にたどり着けるかどうかは、各々のやり方次第です。しかしWeb開発という面に限って言えば、とても大きな効率化を図ることが出来るのです。",
      "link": "https://qiita.com/SoraKumo/items/fe2d47fbbc102503c314",
      "updated": "2019-07-08 01:44:25"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "「素数」という響きへのあこがれ JavaScript ",
      "description": "\nはじめに\n\n\n私はバリバリの文系出身者ですが、「素数」という響きにものすごいあこがれと凄まじい数学の世界を感じます。\n「最大の素数を発見・・・」、「エラトステネスの篩 ふるい にて・・・」、「スーパーコンピュータにて、これくらいまでの素数を出すと、〇〇秒・・・」などと聞くと、あこがれとともに、関係ないな・・、と思っていました。\n\n\n※余談 次のような数学を題材にした映画が、数学がわからないながら地味に好きです。\n・イミテーション・ゲーム エニグマと天才数学者の秘密\n・奇蹟がくれた数式\n・博士が愛した数式\n\n\n素数を求めることに対する「処理速度」や「方法」の探求を抜きにすれば、プログラミング初学者にとっては、「if文」と「ループ」の勉強になると思います。そこで、JavaScriptの基礎を学んだあとに、素数を求める、というプログラムを書こうと思いました。\n\n\n\n注意点\n\nということで、いかに早く素数を求めるか、とか、きれいなロジックで、といったことを追求しているわけではなく、「素数もしくは数学に対するあこがれ」からくる「プログラミング練習」となります。\n\n\nやったこと\n\n①与えた秒数で、どれくらいの数まで素数か否かの確認でき、その中にどのくらいの素数を含まれているかを求めてみる\n②与えた数字までに、いくつの素数が含まれているか、それを求めるまでどのくらいの秒数がかかるかを求めてみる\n\n\n実際のもの・コード\n\n\n①与えられた時間で素数カウント\n\n\n\n lt  DOCTYPE html gt \n lt html lang  ja  gt \n\n lt head gt \n  lt meta charset  utf   gt \n  lt title gt 素数カウント lt  title gt \n lt  head gt \n\n lt body gt \n  lt h gt 与えられた時間で素数カウント lt  h gt \n  lt span gt 時間 秒   lt  span gt \n  lt input type  number  id  time  min    max    value    gt \n  lt p gt  lt button id  calcBtn  gt カウント lt  button gt  lt  p gt \n\n  lt p id  msg  gt  lt  p gt \n  lt p gt 確認できた数  lt span id  number  gt  lt  span gt  lt  p gt \n  lt p gt 素数の数  lt span id  outputNum  gt  lt  span gt  lt  p gt \n  lt p gt 素数  lt span id  output  gt  lt  span gt  lt  p gt \n\n  lt script gt \n   計算ボタンが押下されたとき\n var calcBtn   document getElementById  calcBtn   \n calcBtn addEventListener  click   function   \n\n   初期処理\n document getElementById  msg   innerHTML      \n document getElementById  number   innerHTML      \n document getElementById  outputNum   innerHTML      \n document getElementById  output   innerHTML      \n\n   入力値チェック\n var setTime   document getElementById  time   value \n if setTime  gt    \n document getElementById  msg   innerHTML    秒以内にしてください。息切れします  \n return \n  \n\n var i    \n var nowTime    \n var outputNum    \n var output        \n var noPrime \n\n   開始時間セット\n var startTime   new Date   \n\n while nowTime  lt  setTime   \n\n    で割り切れなければ 奇数ならば 、確認ロジックにはいる\n if  i           \n\n noPrime      \n\n for var j     j  lt  i  j    \n   割り切れた時点で素数ではない\n if  i   j       \n noPrime    on  \n break \n  \n  \n\n if noPrime        \n outputNum      素数の数をカウントアップ\n output    i           素数を格納\n  \n  \n\n i      確認数字のカウントアップ\n\n var stopTime   new Date      終了時間\n var ms   stopTime getTime     startTime getTime      経過時間をミリ秒で取得\n var nowTime   ms    \n\n  \n\n document getElementById  number   innerHTML   i \n document getElementById  outputNum   innerHTML   outputNum \n document getElementById  output   innerHTML   output \n\n    \n  lt  script gt \n lt  body gt \n lt  html gt \n\n\n\n②与えられた数字で素数カウント\n\n\n\n lt  DOCTYPE html gt \n lt html lang  ja  gt \n\n lt head gt \n  lt meta charset  utf   gt \n  lt title gt 素数カウント lt  title gt \n lt  head gt \n\n lt body gt \n  lt h gt 与えられた数字で素数カウント lt  h gt \n  lt span gt 数字  lt  span gt \n  lt input type  number  id  num  min    max    value    gt \n  lt p gt  lt button id  calcBtn  gt カウント lt  button gt  lt  p gt \n\n  lt p id  msg  gt  lt  p gt \n  lt p gt 処理にかかった時間  lt span id  procTime  gt  lt  span gt 秒 lt  p gt \n  lt p gt 素数の数  lt span id  outputNum  gt  lt  span gt  lt  p gt \n  lt p gt 素数  lt span id  output  gt  lt  span gt  lt  p gt \n\n  lt script gt \n   計算ボタンが押下されたとき\n var calcBtn   document getElementById  calcBtn   \n calcBtn addEventListener  click   function   \n\n   初期処理\n document getElementById  msg   innerHTML      \n document getElementById  outputNum   innerHTML      \n document getElementById  output   innerHTML      \n\n   入力値チェック\n var setNum   document getElementById  num   value \n if setNum  gt    \n document getElementById  msg   innerHTML    数は以下にしてください  \n return \n  \n\n var outputNum    \n var output        \n var noPrime \n\n   開始時間セット\n var startTime   new Date   \n\n   数字をループして調査\n for var i     i  lt   setNum  i     \n\n    で割り切れなければ 奇数ならば 、確認ロジックにはいる\n if  i           \n\n noPrime      \n\n for var j     j  lt  i  j    \n   割り切れた時点で素数ではない\n if  i   j       \n noPrime    on  \n break \n  \n  \n\n if noPrime        \n outputNum      素数の数をカウントアップ\n output    i           素数を格納\n  \n  \n  \n\n   終了時間をセット\n var stopTime   new Date   \n\n   経過時間をミリ秒で取得\n var ms   stopTime getTime     startTime getTime   \n var s   ms   \n\n document getElementById  procTime   innerHTML   s \n document getElementById  outputNum   innerHTML   outputNum \n document getElementById  output   innerHTML   output \n\n    \n  lt  script gt \n lt  body gt \n lt  html gt \n\n\n\nまとめ\n\n・言語の基礎を学んだ後は、もしくは、学んでいる最中には、素数を出してみるプログラムをやってみると意外と勉強になると思いますので、「数学」・「素数」という言葉の響きにあこがれている方でもそうでない方も実践してみるとよいかもしれません。そして、おもしろいと思ったら探求と開始すればよいかなと思います。",
      "link": "https://qiita.com/yoshi_yast/items/a9090d38b2778feccbbf",
      "updated": "2019-07-07 23:58:19"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "「時間バッテリー」を表示する JavaScript ",
      "description": "\nはじめに\n\n\nJavaScriptの基礎を学んだ後に番目に作成したミニアプリです。\nどの言語でもそうだと思いますが、日付や時間を扱うのはプログラミング基礎を学んだ後にやっておくとよいと思っております。 そう勝手に思っています \nそこで、「時間バッテリー」というタイトルで、現在を基準に、年末まで・月末まで・今日の終わりまでを算出してプログレスバーを表示してみようと思いました。\n\n\n\n\nやったこと\n\n\n現在日付から年末までの日数カウント、バッテリー 表示\n現在日付から今月末までの日数カウント、バッテリー 表示\n現在日付から今月末までの日数カウント、バッテリー 表示\n基準日からターゲット日までの日数カウント、バッテリー 表示\n\n\nデモサイト\n\n\n\nコード HTML CSS JavaScript \n\n\ntimebattery html\n lt  DOCTYPE html gt \n lt html lang  ja  gt \n  lt head gt \n  lt meta charset  utf   gt \n  lt meta name  viewport  content  width device width  initial   gt \n  lt title gt TimeBattery lt  title gt \n  lt link type  text css  rel  stylesheet  href    gt \n  lt link rel  stylesheet  href    timebattery css  gt \n  lt  head gt \n\n  lt body gt \n  lt h gt Time Battery lt  h gt \n  lt p id  nowtime  gt  lt  p gt \n  lt p id  year  gt  lt  p gt \n  lt div class  progress  gt  lt span id  loading  gt  lt  span gt  lt  div gt \n  lt br gt \n  lt p id  month  gt  lt  p gt \n  lt div class  progress  gt  lt span id  loading  gt  lt  span gt  lt  div gt \n  lt br gt \n  lt p id  today  gt  lt  p gt \n  lt div class  progress  gt  lt span id  loading  gt  lt  span gt  lt  div gt \n  lt br gt \n  lt p gt  カレンダー対応ブラウザ  Chrome、Opera、MS Edge lt  p gt \n  lt p gt \n基準日 lt input id  cal start  type  date  name  calendar  max      gt \nターゲット日 lt input id  cal end  type  date  name  calendar  max      gt \n  lt  p gt \n  lt p id  result  gt  lt  p gt \n  lt div class  progress  gt  lt div id  loading  gt  lt  div gt  lt  div gt \n\n  lt script type  text javascript  src    gt  lt  script gt \n  lt script type  text javascript  src    gt  lt  script gt \n  lt script type  text javascript  src    timebattery js  gt  lt  script gt \n  lt  body gt \n lt  html gt \n\n\n\n\ntimebattery css\nbody  \n text align  center \n font family  Arial  Helvetica  sans serif \n font size  px \n \n progress   progress   progress   progress  \n margin   auto \n width    \n \n loading   loading   loading   loading  \n position  absolute \n left    \n font weight  bold \n \n\n\n\n\ntimebattery js\n  function    \n  use strict  \n\n var nowDate   new Date   \n var year   nowDate getFullYear      年 桁の西暦 \n var mon   nowDate getMonth      月  \n var date   nowDate getDate      日  \n var hour   nowDate getHours      時  \n var min   nowDate getMinutes      分  \n var sec   nowDate getSeconds      秒  \n\n   現在時刻の表示\n var month   mon    \n var nowtime    現在      year    年    month    月    date    日  \n document getElementById  nowtime   innerHTML   nowtime \n\n   目的の日付をセット\n var yearEnd   new Date year         年末の日付をセット\n var monthEnd   new Date year  mon          今月末をセット\n var todayEnd   new Date year  mon  date           今日の最終時刻をセット\n\n   それぞれを数値化\n var dnumNow   nowDate getTime      現在時刻の数字をget\n var dnumYearEnd   yearEnd getTime      年末の数字をget\n var dnumMonthEnd   monthEnd getTime      今月末の数字をget\n var dnumTodayEnd   todayEnd getTime      今日の数字をget\n\n   それぞれのENDと現在との差を計算\n var diffYear   dnumYearEnd   dnumNow \n var diffMonth   dnumMonthEnd   dnumNow \n var diffToday   dnumTodayEnd   dnumNow \n\n   差と を計算 年 \n var diffDays   diffYear                    日数を割り出す\n var showDays   Math ceil  diffDays      小数点以下を切り上げる\n var yearValue   Math ceil  showDays              を計算\n   差と を計算 月 \n var diffDays   diffMonth                    日数を割り出す\n var showDays   Math ceil  diffDays      小数点以下を切り上げる\n var monthValue   Math ceil  showDays              を計算\n   差と を計算 時間 \n var todayValue   Math ceil diffToday                          を計算\n var dHour   diffToday                 時間\n diffToday   diffToday              \n var dMin   diffToday              分\n diffToday   diffToday           \n var dSec   diffToday       秒\n var showDays   Math floor dHour     時間    Math floor dMin     分    Math floor dSec     秒  \n\n   年末までの日数を表示\n var yearEnd    年末まであと    showDays    日  \n document getElementById  year   innerHTML   yearEnd \n\n   今月末までの日数を表示\n var monthEnd    今月末まであと    showDays    日  \n document getElementById  month   innerHTML   monthEnd \n\n   今日の終わりまでの時間を表示\n var todayEnd    今日の終わりまであと    showDays \n document getElementById  today   innerHTML   todayEnd \n\n   プログレスバーと テキスト表示\n progress   progress   yearValue    loading   \n progress   progress   monthValue    loading   \n progress   progress   todayValue    loading   \n\n function progress barNum  value  loadNum  \n   プログレスバーを生成\n   barNum  progressbar  \n value  value \n max  \n    \n     のテキスト表示\n var per     barNum  progressbar  value       barNum  progressbar  option    max   \n   loadNum  text Math ceil per            \n     の色つけ\n   barNum  each function   \n var selector     this  find  div   \n var value   this getAttribute  aria valuenow   \n\n if  value  gt      \n   selector  css    background    LightGreen     \n   else if  value  gt     \n   selector  css    background    LightYellow     \n   else  \n   selector  css    background    Pink     \n  \n    \n  \n\n   カレンダーにデフォルト日付を設定\n var mm        nowDate getMonth      slice    \n var dd        nowDate getDate    slice    \n var calStartYear   year    \n document getElementById  cal start   value   calStartYear         mm         dd \n var calEndYear   year    \n document getElementById  cal end   value   calEndYear         mm         dd \n\n   変数設定\n var cal startForm   document getElementById  cal start   \n var cal endForm   document getElementById  cal end   \n\n   カレンダー日が変更されたときfunction timecalcを呼び出す\n cal startForm addEventListener  change   timecalc  \n cal endForm addEventListener  change   timecalc  \n\n function timecalc   \n\n var startDate   new Date cal startForm value  \n var targetDate   new Date cal endForm value  \n\n   設定された日付が過去日付であったらメッセージ表示\n if targetDate  lt  nowDate   \n document getElementById  result   innerHTML    未来日付を設定してください  \n   else  \n   数値化\n var dnumStart   startDate getTime   \n var dnumNow   nowDate getTime   \n var dnumTarget   targetDate getTime   \n\n   それぞれのENDと現在との差を計算\n var calDiff   dnumTarget   dnumNow \n var calDiff   dnumTarget   dnumStart \n\n   差と を計算 月 \n var calDiffDays   calDiff                    日数を割り出す\n var calDiffDays   calDiff                    日数を割り出す\n\n var calShowDays   Math ceil  calDiffDays      小数点以下を切り上げる\n var calShowDays   Math ceil  calDiffDays      小数点以下を切り上げる\n\n var dateValue   Math ceil calShowDays   calShowDays          を計算\n var dateValue   calShowDays   calShowDays \n\n var dateResult    ターゲット日付まであと    calShowDays    日          dateValue    日経過   \n document getElementById  result   innerHTML   dateResult \n\n   プログレスバーとテキスト表示\n progress   progress   dateValue    loading   \n\n  \n   \n   \n\n\n\n\nまとめ\n\n\n言語の基礎を学んだ後は、日付や時間計算のプログラムをやるとよい、と個人的に思います。\n「時間バッテリー」というタイトルで、プログレスバーの表示をやってみました。\nバッテリー形式にしているので、年末に向けてエネルギーがなくなるように見えるなぁ・・と思ったりもしますが 汗 。\n",
      "link": "https://qiita.com/yoshi_yast/items/22086d20caa38bae60d0",
      "updated": "2019-07-07 23:56:24"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "「その会議いくら 」を算出する JavaScript ",
      "description": "\nはじめに\n\n\nJavaScriptの基礎を学んだ後に最初に作成したミニアプリです。\n\n\n\n動機\n\n\n仕事で比較的な大きなプログジェクトに関わっているとき、人以上が参加する「定例進捗会議」に出ることがありました。\n大きな声では言えませんが、お決まりの進捗会議ですので、本当にみんなで集まる必要あるの という疑問が、おそらく全員が持っているような状態な感じです。 最近は少なくなりましたが、いまだにあるところにはありますよね、そんな会議。。 \nそこで、「いったい、いくらこの会議にかかっているんだ 」と疑問に思いましたので、学習を兼ねて、JavaScriptで算出してみることにしました。\n探してみると、How much does this meeting pay といったサイト 英語 もあるようです。\n\n\n\nやろうと思ったこと\n\n\n「参加人数」「平均月給」「月の仕事時間」を入力パラメータにしました。 それしか思いつかなかった・・・ \n会議参加者の「平均月給」「月の仕事時間」をどうやって知るんだ、というツッコミは置いておいて、そこから、会議にかけた総合計時間とコストを計算しようと思いました。\n\n\n\n注意点\n\n\n「参加人数」「平均月給」「月の仕事時間」というパラメータしか思いつきませんでしたが、本当の会議のコストをこれだけで測ることはできないと思います。\n解決案をまとめる・ものごとが進む・Before Afterで変化がある、などといったことが会議では重要ですので。\n改善アイデアとして、「その会議の活発度」も一緒に取り込めるとよいのかな、と思ったりもします。 もうそんなアプリあるのかな・・ 例 「無音状態の少なさ」「喋っている人の数」「雰囲気 感情分析 ToneAnalyzerとか  」など\n\n\n\nやったこと\n\n\n「参加人数」「平均月給」「月の仕事時間」をパラメータに、会議にかけた総合計時間とコストを計算する。休憩なども考慮して、START STOPを何度でもできるようにする。\nデモサイト\n\n\n\n\n\nコード HTML CSS JavaScript \n\n\nconference html\n lt  DOCTYPE html gt \n lt html lang  ja  gt \n  lt head gt \n  lt meta charset  utf   gt \n  lt meta name  viewport  content  width device width  initial   gt \n  lt title gt その会議いくら  lt  title gt \n  lt link rel  stylesheet  href    conference css  gt \n  lt  head gt \n\n  lt body gt \n  lt h gt その会議いくら  lt  h gt \n  lt p gt 参加人数  lt input type  text  name  people  id  people  value    gt 人 lt  p gt \n  lt p gt 平均月給  lt input type  text  name  salary  id  salary  value    gt 円 lt  p gt \n  lt p gt 月の仕事時間  lt input type  text  name  worktime  id  worktime  value     gt 時間 lt  p gt \n  lt p gt 時  lt span id  hourly  gt  lt  span gt 円 分  lt span id  minute  gt  lt  span gt 円 秒  lt span id  second  gt  lt  span gt 円 lt  p gt \n  lt br gt \n  lt p gt かけた時間  lt span id  timeText  gt  lt  span gt 分 lt  p gt \n  lt p gt かかったコスト  lt span id  costText  gt  lt  span gt 円 lt  p gt \n  lt br gt \n  lt p gt 経過時間  lt span id  timerText  gt  lt  span gt 秒 lt  p gt \n  lt br gt \n  lt div id  start  gt 会議START lt  div gt \n  lt div id  stop  gt 会議STOP lt  div gt \n  lt div id  reset  gt リセット lt  div gt \n\n  lt script type  text javascript  src    gt  lt  script gt \n  lt script type  text javascript  src    gt  lt  script gt \n\n  lt script type  text javascript  src    conference js  gt  lt  script gt \n\n  lt  body gt \n lt  html gt \n\n\n\n\nconference css\nbody  \n text align  center \n font family  Arial  Helvetica  sans serif \n font size  px \n \n people   salary   worktime  \n width  px \n text align  right \n \n timerText  \n color  blue \n font size  px \n \n timeText   costText  \n color  red \n font size  px \n \n btn  \n display  inline block \n width  px \n padding  px \n border radius  px \n box shadow   px   acc \n color   fff \n background   bee \n cursor  pointer \n \n btn    btn  \n margin left  px \n \n btn active  \n opacity   \n \n btn inactive  \n opacity   \n \n\n\n\n\nconferenece js\n function    \n  use strict  \n\n   平均月給フォーカスアウト\n     salary   on  blur   function   \n var num     this  val   \n num   num replace    d      d d d     g        \n   this  val num  \n    \n   平均月給フォーカス\n     salary   on  focus   function   \n var num     this  val   \n num   num replace    g      \n   this  val num  \n    \n\n   デフォルト表示の取得\n var people   document getElementById  people   value \n var salary   document getElementById  salary   value \n var worktime   document getElementById  worktime   value \n   デフォルト値の計算\n var hSalary     people   salary     worktime \n var mSalary   hSalary    \n var sSalary   mSalary    \n   デフォルト値のセット\n document getElementById  hourly   innerHTML   Math round hSalary  toLocaleString   \n document getElementById  minute   innerHTML   Math round mSalary  toLocaleString   \n document getElementById  second   innerHTML   Math round sSalary  toLocaleString   \n\n   項目が変更されたときは再計算を実施\n     people   change function    \n people   document getElementById  people   value \n calcSalary   \n    \n\n     salary   change function    \n salary   document getElementById  salary   value \n calcSalary   \n    \n\n     worktime   change function    \n worktime   document getElementById  worktime   value \n calcSalary   \n    \n\n var hSalary recalc \n var mSalary recalc \n var sSalary recalc \n\n function calcSalary   \n hSalary recalc     people   salary     worktime \n mSalary recalc   hSalary recalc    \n sSalary recalc   mSalary recalc    \n document getElementById  hourly   innerHTML   Math round hSalary recalc  toLocaleString   \n document getElementById  minute   innerHTML   Math round mSalary recalc  toLocaleString   \n document getElementById  second   innerHTML   Math round sSalary recalc  toLocaleString   \n sSalary   sSalary recalc \n   \n\n   変数設定\n var startTime \n var timerId \n var elapsedTime    \n var isRunning   false \n\n var startButton   document getElementById  start   \n var stopButton   document getElementById  stop   \n var resetButton   document getElementById  reset   \n var timerText   document getElementById  timerText   \n var timeText   document getElementById  timeText   \n var costText   document getElementById  costText   \n\n   ボタンステータス管理\n function setButtonState start  stop  reset  \n startButton className   start    btn active     btn inactive  \n stopButton className   stop    btn active     btn inactive  \n resetButton className   reset    btn active     btn inactive  \n  \n\n   初期画面 startボタンのみON \n setButtonState true  false  false  \n\n   startボタンが押下されたとき\n startButton addEventListener  click   function   \n if isRunning  \n return \n  \n isRunning   true \n startTime   Date now         からの経過ミリ秒\n updateTimerText   \n setButtonState false  true  false     ボタンのステータス STOPをON\n    \n\n   stopボタンが押下されたとき\n stopButton addEventListener  click   function   \n if  isRunning  \n return \n  \n isRunning   false \n elapsedTime    Date now     startTime \n clearTimeout timerId  \n setButtonState true  false  true     ボタンのステータス Start ResetをON\n    \n\n   resetボタンが押下されたとき\n resetButton addEventListener  click   function   \n if isRunning  \n return \n  \n timerText innerHTML      \n timeText innerHTML      \n costText innerHTML      \n elapsedTime    \n setButtonState true  false  false     ボタンのステータス StartをON\n    \n\n   経過時間・時間・コストの更新\n function updateTimerText   \n timerId   setTimeout function   \n var t   Date now     startTime   elapsedTime \n timerText innerHTML    t     toFixed   \n timeText innerHTML     timerText innerHTML   people      toFixed   \n costText innerHTML   Math round sSalary   timerText innerHTML  toLocaleString   \n updateTimerText   \n      \n  \n\n     \n\n\n\n\nまとめ\n\n\n言語の基礎を学んだ後は、日付や時間計算のプログラムをやるとよい、と個人的に思います。\n今回は「会議」というテーマで時間のプログラムを学習してみましたが、他のテーマでも自分が楽しんでやれれば学習テーマはなんでもありだと思います 例 素数日を出してみるなど \n",
      "link": "https://qiita.com/yoshi_yast/items/1f48337a7caed143f07b",
      "updated": "2019-07-07 23:54:25"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": " Vue js input fileで選択した画像のプレビューを表示する",
      "description": "\nVue jsでinput type  file で選択した画像プレビューを表示\n\n\nExample\n\nサンプル\n\n lt template gt \n  lt div gt \n  lt input type  file  ref  file   change  setImage   gt \n  lt img  src  data image  gt \n  lt  div gt \n lt  template gt \n lt script gt \nexport default  \n data    \n return  \n data   \n image     \n name     \n  \n   \n   \n methods   \n setImage e   \n const files   this  refs file \n const fileImg   files files   \n if  file type startsWith  image      \n this data image   window URL createObjectURL file  \n this data name   file name \n this data type   file type \n  \n   \n\n  \n  \n lt  script gt \n\n\n\n初めに、 lt input type  file  ref  file    gt でref で適当に名前をつけてあげます。\nそして changeで画像の選択時にsetImageを発火させています。\nthis  refs fileで画像のようなdomが参照できるのでそこから画像dataをひっぱって来るようにしています。\n\n\n最後に、window URL createObjectURL file でsrcに指定するURLを生成しています。\nこんな感じでプレビューを表示させてあげれます。\n\n\nExample\n\n先ほどはthis  refs を使った方法になりますが、eventをとってきてやる方法でもいいかな \n\n lt template gt \n  lt div gt \n  lt input type  file   change  setImage  event    gt \n  lt img  src  data image  gt \n  lt  div gt \n lt  template gt \n lt script gt \nexport default  \n data    \n return  \n data   \n text     \n image     \n name     \n  \n   \n   \n methods   \n setImage event   \n const file    e target files    e dataTransfer   \n if  file type startsWith  image      \n this data image   window URL createObjectURL file  \n this data name   file name \n this data type   file type \n  \n   \n\n  \n  \n lt  script gt \n\n\n\n\n最後に\n\nもっと簡単な方法があればご教授ください。\nありがとうございました。",
      "link": "https://qiita.com/iLLviA/items/c24f385ca3334c05a682",
      "updated": "2019-07-07 22:41:49"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "NetlifyのCIでNode jsのバージョンを最新に指定",
      "description": "Nuxt jsをSSRしてNetlifyに上げて運用しています。\n\n\n\n\n\nローカル環境はNode js vを利用していますが、なかなかサンプルコードなども見当たらなかったのでメモ\n\n 途中  Netlifyドキュメント日本語翻訳が参考になりました。\n\n\n nvmrcを追加するだけ\n\nnode  v  gt   nvmrc\n\n\nという記述がありますが。\n\nプロジェクトルートに nvmrcというファイルを作成し、中にバージョンを記述すればおkです。\n\nv \n\n\nこれでNetlifyにデプロイすればimageにインストールされて使えます。\n\n\n対応バージョンは \n\nログを見る限りだとnvmで利用できるバージョンがそのまま利用できそうなので、最新を常に使うことができそうです。ありがたいですね。\n\n\n\nこんな感じで裏側でインストールされてました。なので初回は少し時間がかかる印象ですが、回目以降はこんな感じで既にインストールされた状態になっているのでビルド時間も特段気にしなくて良さそうです。\n",
      "link": "https://qiita.com/n0bisuke/items/8bddad87610b01c90003",
      "updated": "2019-07-07 17:52:13"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Next js   canaryにDynamic Routingが付いてきていて、それは嬉しい",
      "description": "\n概要\n\nNext jsに、ダイナミックルートがあれば良いなあって前々から思っていたけど、canary見ていたら増えていたわ。\n\n\ncanaryの話です。\n当然ですが、この仕様は変わるかもしれません。\n\n\n\n環境\n\n\npackage json\n \n  dependencies    \n  next       canary   \n  react        \n  react dom       \n   \n  devDependencies    \n   types node        \n   types react        \n   types react dom        \n  prettier        \n  typescript       \n  \n \n\n\n\n\nDynamicRouteの使い方\n\n基本的には、ディレクトリ名やファイル名を fugafuga  や fugafuga  jsxにするとそこに反応するようになる。\n\n \n├ーpages\n│  ├ーhoge tsx\n│  ├ーindex tsx\n│  └ーkani\n│  ├ー kaniname \n│  │  ├ーdescription\n│  │  │  ├ー kanitips  tsx\n│  │  │  ├ーhoge tsx\n│  │  │  └ーindex tsx\n│  │  └ーindex tsx\n│  ├ーhoge tsx\n│  └ーindex tsx\n\n\n\n例えばこのようなフォルダ構成だとする。フォルダ名に   と   がついているので、かなり異様。\n\n例をほど\n\n\nlocalhost  kani hoge にアクセスすると  gt pages kani hoge tsxが読み込まれる。\nlocalhost  kani piyoにアクセスすると  gt pages kani  kaniname  index tsxが読み込まれる。\n\n\nこれは大変ありがたい。\n\nlocalhost  kani piyoにアクセスされた時に、pages kani  kaniname  index tsxにおいて、useRouterフックを使うとその中のkaninameパラメタにpiyoが入っている。\n\nconst KaniName  NextPage        gt   \n const router   useRouter   \n const   kaniname     router query \n return  lt p gt  kaniname  lt  p gt  \n  \nexport default KaniName \n\n\n\n\n\n\n\n\n\nlocalhost  kani piyo description にアクセスすると  gt pages kani  kaniname  description index tsxが読み込まれる。\n\nlocalhost  kani piyo piyopiyo にアクセスすると  gt になる\n\n\n\n\n\n\nlocalhost  kani piyo description hogeにアクセスすると  gt pages kani  kaniname  description hoge tsxが読み込まれる。\n\nlocalhost  kani piyo description piyopiyoにアクセスすると  gt pages kani  kaniname  description  kanitips  tsxが読み込まれる。\n\n\nこんな感じのデータがあるとして\n\nexport interface Kani  \n name  string \n description  string \n tips    title  string  content  string     \n \nexport const kanis  Kani      \n  \n name   カニ一郎  \n description   強いカニ  \n tips   \n   title   強さの秘密   content   強いから    \n   title   生まれ   content   琵琶湖   \n  \n   \n  \n name   カニ二郎  \n description   さらに強いカニ  \n tips     title   生まれ   content   太平洋    \n   \n  \n name   カニ三郎  \n description   さすらいのカニ  \n tips    \n  \n  \n\n\nこんかな感じで動的にページを作れる。\n\n\npages kani  kaniname  description  kanitips  tsx\nconst KaniTips  NextPage        gt   \n const router   useRouter   \n const   kaniname  kanitips     router query \n const  kani    useState kanis find e   gt  e name    kaniname   \n\n return  \n  lt Layout gt \n  kani    \n  lt  gt \n  lt h gt \n  kani name の kanitips \n  lt  h gt \n  lt hr   gt \n  lt KaniFactFind kani  kani  kanitips  kanitips    gt \n  lt   gt \n      \n  lt p gt  kaniname なる奴は知らん lt  p gt \n   \n  lt  Layout gt \n   \n  \n\nexport default KaniTips \n\n\n\n\n\n\n\n\n\nuseRouteの型がanyだったので、JSON stringifyして見るだけしてみた。あとあと詳しく調べよ思うけど、そのうち治る気がする。\n\n\nrouterの中をJSON stringifyした奴\n \n pathname    kani  kaniname  description  kanitips   \n route    kani  kaniname  description  kanitips   \n query    kaniname   カニ五郎   kanitips   強さの秘密    \n asPath \n   kani  E  AB E  B E BA  E  E description  E BC B E   E  AE E A  E AF   \n events    \n \n\n\n\n今までは、ページパラメータに情報を食わせて、 やら やらがたくさん作るURLにするか、Linkにasをつけてマスクするかなかった。Linkにasすると何がまずいかと言うとリロードした時に、とかになる。\n\n lt Link href   kani kaniname piyo  as   kani piyo  gt \n  lt a gt kani piyoへと見せかけて kani kaniname piyoへ lt  a gt \n lt  Link gt \n\n\nそのためExpressなりでサーバーを立てなければならなかったが、今回のDynamic Routingが導入されれば、そのようなカスタムサーバを立てる機会が減る。 そら当然、機会が完全ににはならんけど \n\nこんな感じにgetInitialPropsしてもいいか知ら。\n\n\npages kani  kaniname  description  kanitips  tsx\nconst KaniTips  NextPage lt   kani  Kani   gt       kani      gt   \n const router   useRouter   \n const   kaniname  kanitips     router query \n return  \n  lt Layout gt \n  kani    \n  lt  gt \n  lt h gt \n  kani name の kanitips \n  lt  h gt \n  lt hr   gt \n  lt KaniFactFind kani  kani  kanitips  kanitips    gt \n  lt   gt \n      \n  lt p gt  kaniname なる奴は知らん lt  p gt \n   \n  lt  Layout gt \n   \n  \nKaniTips getInitialProps   async    query      gt   \n const   kaniname     query \n return   kani  kanis find e   gt  e name    kaniname    \n  \nexport default KaniTips \n\n\n\nとはいえ、現状はCanaryなので、仕様が変わるかも知れんし今はつかはない。\n\n\nまとめ\n\n\nNext jsの次のバージョンはDynamic Routeとかつくかも。・・・つくとそれは嬉しいかも️\nフォルダ名や、ファイル名に  がつく景色が出てくるかも\nNext jsにExpressなり、カスタムサーバーたてる機会がもっと減って、楽チンになるかも\n\n\n\n参考\n\n\nDynamic Routing\nnext js examples dynamic routing \n\n\n\nおまけ\n\nできた奴  \n\nなんかつくってもいない、description jsを読みに行こうとして、ないもんだから、とか吐いているけど、そのうち治るかな",
      "link": "https://qiita.com/NanimonoDemonai/items/b27e44ba9991411dc9c2",
      "updated": "2019-07-07 18:53:17"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Selenium for Python画像が読み込まれるまで待機する",
      "description": "\nはじめに\n\nSeleniumで処理を自動化しました。\nしばしば画像の読み込みが完了する前に、処理を始めてしまい例外が発生していました。\nそこで画像の読み込みが完了するまで待機する処理を考えてみました。\n\n\n環境\n\nPython  \nselenium  \nFirefox     ビット \ngeckodriver        \n\n\nソース\n\n\nJavaScript\n\n画像の読み込みが完了したかどうかを判定する処理は、JavaScriptで実装しました。\n以下のようになります。\nここでは、Document配下の全ての画像を対象にしています。\n適宜、修正してください。\n\n\nJavaScript\nfunction isLoadedAllImges    \n const imges   document getElementsByTagName  img   \n let completed   true \n for  const img of imges   \n if  img complete     false   \n completed   false \n break \n  \n  \n return completed \n \n\n\n\n\nPython\n\n上で定義したJavaScriptをSelenium Python 側から呼び出します。\n単純な事ですが、重要なのは、画像が読み込まれるまでSelenium側でsleepをしてやることです。\n理由は、\n\n\nJavaScriptには、単純なSleepがない\nJavaScript側がビジー状態になっていると画像の読み込みが完了しない\n\n\nからです。\nSleepをしてくれるライブラリ等あればそれを使えば良いのでしょうが、それの設定をするのも面倒なのでこのような実装にしました。\n\n\nPython\nJavaScriptIsLoadedAllImges      \nfunction isLoadedAllImges    \n const imges   document getElementsByTagName  img   \n let completed   true \n for  const img of imges   \n if  img complete     false   \n completed   false \n break \n  \n  \n return completed \n \nreturn isLoadedAllImges  \n   \n\ndef isLoadedAllImges timeOut   interval   \n start   time time  \n completed   False\n while time time     start  lt  timeOut and completed    False \n completed   driver execute script JavaScriptIsLoadedAllImges \n time sleep interval \n return completed\n\n\n\n\n最後に\n\n単純なサイトではこれでうまく行くと思います。\n実際には、アクセスしているサイトの挙動によってはうまく行かないことも出てくるかと思います。\n\n余談ですが、Selenium等での自動化にはまると楽しいですね。\nこれを仕事や収入に結び付ければ良いのでしょうが。",
      "link": "https://qiita.com/BlueSilverCat/items/4a66d136c462455570b0",
      "updated": "2019-07-07 17:25:18"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "amcharts  Demosを使ってグラフを作成",
      "description": "\n記事について\n\namcharts関連の日本語文献のあまりの少なさから、誰かのお役に立てればと思い記載しています \n\n\n環境について\n\nこの記事ではmacbook unix にインストールしたruby  p  Rails  を使っています。\n\n\n筆者について\n\nTECH  EXPERTにて月よりruby  railsを学習している未経験エンジニアです。\n記載内容に不備・不足があればご指摘いただけると幸いです。\n至らぬ点ばかりですので、改善点がありましたらどんどんご指摘下さい \n\n\namchartsとは\n\namcharts\njavascriptでチャートを描画するためのフレームワークです。棒、エリア、列、バー、パイ、XY、散布、ローソク足のようなチャートを描画することが出来ます。\n\n\n 参考文献 \n\n\n\n\nDemosとは\n\namchartsが提供しているサンプルの一覧です。\n実際にご覧いただいた方が早いと思います。下記よりご確認ください。\n\n\n\n使用方法\n\nHTML内でResources CDN を記載することで利用できます。\n商用利用の場合も、チャート内のamchartsアイコン amchartsへのリンク を削除しなければそのまま使用可能です。\n\n\n 参考文献 \n\nFree license\n Use anywhere you want as long as you don t mind a small amCharts attribution on charts \n\n\nRails環境ですが、Demoをわかりやすく、そのまま使用するために、\n今回は各viewファイル内に直接記載していきます。\n\n\nindex html erb\n lt     Resources    gt \n lt script src    gt  lt  script gt \n lt script src    gt  lt  script gt \n lt script src    gt  lt  script gt \n\n\n\nこれで準備完了です。\n\n\n導入方法\n\n今回は、こちらのチャートを扱います。\nページ内で下にスクロールすると、Demo sourceが置いてありますので、コピーします。\n\n\nDemo source\n lt     Styles    gt \n lt style gt \n chartdiv  \n width    \n height  px \n \n lt  style gt \n\n lt     Resources    gt \n lt script src    gt  lt  script gt \n lt script src    gt  lt  script gt \n lt script src    gt  lt  script gt \n\n lt     Chart code    gt \n lt script gt \namcore ready function    \n\n   Themes begin\namcore useTheme amthemes animated  \n   Themes end\n\n   Create chart instance\nvar chart   amcore create  chartdiv   amcharts XYChart  \nchart scrollbarX   new amcore Scrollbar   \n\n   Add data\nchart data     \n  country    USA  \n  visits   \n    \n  country    China  \n  visits   \n    \n  country    Japan  \n  visits   \n    \n  country    Germany  \n  visits   \n    \n  country    UK  \n  visits   \n    \n  country    France  \n  visits   \n    \n  country    India  \n  visits   \n    \n  country    Spain  \n  visits   \n    \n  country    Netherlands  \n  visits   \n    \n  country    Russia  \n  visits   \n    \n  country    South Korea  \n  visits   \n    \n  country    Canada  \n  visits   \n   \n\n   Create axes\nvar categoryAxis   chart xAxes push new amcharts CategoryAxis    \ncategoryAxis dataFields category    country  \ncategoryAxis renderer grid template location    \ncategoryAxis renderer minGridDistance    \ncategoryAxis renderer labels template horizontalCenter    right  \ncategoryAxis renderer labels template verticalCenter    middle  \ncategoryAxis renderer labels template rotation    \ncategoryAxis tooltip disabled   true \ncategoryAxis renderer minHeight    \n\nvar valueAxis   chart yAxes push new amcharts ValueAxis    \nvalueAxis renderer minWidth    \n\n   Create series\nvar series   chart series push new amcharts ColumnSeries    \nseries sequencedInterpolation   true \nseries dataFields valueY    visits  \nseries dataFields categoryX    country  \nseries tooltipText      categoryX   bold  valueY      \nseries columns template strokeWidth    \n\nseries tooltip pointerOrientation    vertical  \n\nseries columns template column cornerRadiusTopLeft    \nseries columns template column cornerRadiusTopRight    \nseries columns template column fillOpacity    \n\n   on hover  make corner radiuses bigger\nvar hoverState   series columns template column states create  hover   \nhoverState properties cornerRadiusTopLeft    \nhoverState properties cornerRadiusTopRight    \nhoverState properties fillOpacity    \n\nseries columns template adapter add  fill   function fill  target   \n return chart colors getIndex target dataItem index  \n   \n\n   Cursor\nchart cursor   new amcharts XYCursor   \n\n       end amcore ready  \n lt  script gt \n\n lt     HTML    gt \n lt div id  chartdiv  gt  lt  div gt \n\n\n\nお気づきでしょうか 既に使用方法欄で記載した内容は、こちらのDemo sourceに必ず記載されているものです。なので、コピーをそのまま利用される方は、CDNを記載する必要はありません。\n\nそれでは、コピーした内容をそのままhtmlにペーストします。\n\n\nindex html erb\n lt    コピー内容をそのままペースト   gt \n\n\n\n以上でチャートの描画ができました。\n\n\n編集方法\n\nこのままでは当然ですが、サンプルデータそのままになってしまいます。\n各機能について 理解している部分 を記載していきます。\n\n\nStyles\n\ncssです。フォントのサイズ等を指定できますが、指定できる項目には限りがあるようです。\n\n\nindex html erb\n lt     Styles    gt \n lt style gt \n  chartdiv  \n width    \n height  px \n font size  px \n  \n lt  style gt \n\n\n\n\nchart code\n\n本体です。以下、コメントアウトにて詳細を記載します。\n\n\nindex html erb\n lt     Chart code    gt \n lt script gt \namcore ready function    \n\n   Themes begin\n  テーマです。\namcore useTheme amthemes animated  \n   Themes end\n\n   Create chart instance\n  始めにインスタンスを作成しています。\nvar chart   amcore create  chartdiv   amcharts XYChart  \n  X軸方向のスクロールバーです。私はいらなかったのでコメントアウトしました。\nchart scrollbarX   new amcore Scrollbar   \n\n   Add data\n  ここにデータを入力していきます。\nchart data     \n  country    USA  \n  visits   \n    \n  country    China  \n  visits   \n    \n  country    Japan  \n  visits   \n    \n  country    Germany  \n  visits   \n    \n  country    UK  \n  visits   \n    \n  country    France  \n  visits   \n    \n  country    India  \n  visits   \n    \n  country    Spain  \n  visits   \n    \n  country    Netherlands  \n  visits   \n    \n  country    Russia  \n  visits   \n    \n  country    South Korea  \n  visits   \n    \n  country    Canada  \n  visits   \n   \n\n   Create axes\n  ここからはX軸方向について\nvar categoryAxis   chart xAxes push new amcharts CategoryAxis    \n  データフィールドのカテゴリーです。今回はcountryですね。\ncategoryAxis dataFields category    country  \n  この辺りはあまり手を加える必要はないかと思います。\ncategoryAxis renderer grid template location    \ncategoryAxis renderer minGridDistance    \ncategoryAxis renderer labels template horizontalCenter    right  \ncategoryAxis renderer labels template verticalCenter    middle  \n  文字の向きです。このままだと度回転して縦方向の文字列になりますので、\n  横向き希望の方はに設定すると良いです。\ncategoryAxis renderer labels template rotation    \ncategoryAxis tooltip disabled   true \ncategoryAxis renderer minHeight    \n\n  ここからはY軸方向について\nvar valueAxis   chart yAxes push new amcharts ValueAxis    \nvalueAxis renderer minWidth    \n\n   Create series\n  ようやくチャートの設定です。上記のデータフィールドの項目を使用します。\nvar series   chart series push new amcharts ColumnSeries    \nseries sequencedInterpolation   true \n   Y軸方向はvisits\nseries dataFields valueY    visits  \n   X軸方向はcountry\nseries dataFields categoryX    country  \n   tooltipはグラフにカーソルを当てると上部に表示される内容です。\nseries tooltipText      categoryX   bold  valueY      \nseries columns template strokeWidth    \n\nseries tooltip pointerOrientation    vertical  \n\nseries columns template column cornerRadiusTopLeft    \nseries columns template column cornerRadiusTopRight    \nseries columns template column fillOpacity    \n\n   on hover  make corner radiuses bigger\n  ホバーした際の設定です。\nvar hoverState   series columns template column states create  hover   \nhoverState properties cornerRadiusTopLeft    \nhoverState properties cornerRadiusTopRight    \nhoverState properties fillOpacity    \n\nseries columns template adapter add  fill   function fill  target   \n return chart colors getIndex target dataItem index  \n   \n\n   Cursor\n  カーソル設定です。非表示にしたい場合は、以下のコメントアウトのように記載します。\nchart cursor   new amcharts XYCursor   \n   chart cursor lineX disabled   true \n   chart cursor lineY disabled   true \n\n       end amcore ready  \n lt  script gt \n\n\n\n\nHTML\n\n最後に、HTMLを記載します。\n\n\nindex html erb\n lt     HTML    gt \n lt div id  chartdiv  gt  lt  div gt \n\n\n\n\n実装例\n\n最後に、私が作成した sommeil というアプリケーションのコードの内、一部のamcharts部分を参考までに記載します。\n\nこちらのコードでは、オリジナル要素としてチャートのグラフ内から詳細ページに飛べる仕組みを導入しています。\nこの部分に関して、いくらGoogle先生に聞いても出てこなかったので、記事を執筆しました   。\n\n全編はgithubをご覧下さい。\n実装イメージについては、グローバルIPで恐縮ですが公開中です。 sommeil \n\n\n chart html erb\n lt     Styles    gt \n lt style gt \n  chartdiv  \n width    \n height  px \n font size  px \n  \n lt  style gt \n lt     Resources    gt \n lt script src    gt  lt  script gt \n lt script src    gt  lt  script gt \n lt script src    gt  lt  script gt \n lt     Chart code    gt \n lt script gt \n amcore ready function     \n    Themes begin\n amcore useTheme amthemes animated  \n    Themes end\n    Create chart instance\n var chart   amcore create  chartdiv   amcharts XYChart  \n   chart scrollbarX   new amcore Scrollbar   \n    Add data\n var sleeping times     lt     sleeping times   gt   replace     g      replace     g      split      \n var dates     lt     dates   gt   replace     g      replace     g      replace   s g      split      \n var sleep ids     lt     sleep ids   gt   replace     g      replace     g      replace   s g      split \n      \n console log sleep ids  \n chart data     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n     \n  day   dates   \n  time   sleeping times   \n  id   sleep ids  \n    \n    Create axes\n var categoryAxis   chart xAxes push new amcharts CategoryAxis    \n categoryAxis dataFields category    day  \n categoryAxis renderer grid template location    \n categoryAxis renderer minGridDistance    \n categoryAxis renderer labels template horizontalCenter    middle  \n categoryAxis renderer labels template verticalCenter    middle  \n categoryAxis renderer labels template rotation    \n categoryAxis renderer labels template fill   amcore color   fff   \n categoryAxis tooltip disabled   true \n categoryAxis renderer minHeight    \n var valueAxis   chart yAxes push new amcharts ValueAxis    \n valueAxis renderer minWidth    \n valueAxis renderer labels template fill   amcore color   fff   \n    Create series\n var series   chart series push new amcharts ColumnSeries    \n series sequencedInterpolation   true \n series dataFields valueY    time  \n series dataFields categoryX    day  \n series tooltipText      categoryX   bold  valueY      \n series columns template strokeWidth    \n series tooltip pointerOrientation    vertical  \n   Go to detail page\n var current user id     lt     user id   gt  \n series columns template url  \n    current user id  sleeps  id urlEncode     \n series columns template column cornerRadiusTopLeft    \n series columns template column cornerRadiusTopRight    \n series columns template column fillOpacity    \n    on hover  make corner radiuses bigger\n var hoverState   series columns template column states create  hover   \n   hoverState properties cornerRadiusTopLeft    \n   hoverState properties cornerRadiusTopRight    \n hoverState properties fillOpacity    \n series columns template adapter add  fill   function  fill  target   \n return chart colors getIndex target dataItem index  \n    \n    Cursor\n chart cursor   new amcharts XYCursor   \n chart cursor lineX disabled   true \n        end amcore ready  \n lt  script gt \n lt     HTML    gt \n lt div id  chartdiv  gt  lt  div gt \n\n\n\n\n最後に\n\n合わせてpiechartの作成方法なども近日公開予定です。\nご覧いただき、ありがとうございました。",
      "link": "https://qiita.com/Ktym44977057/items/0766e4524f7cae1b8d5b",
      "updated": "2019-07-07 17:22:48"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Redmineで入力フィールドをグルーピングして見出しを入れる",
      "description": "\nはじめに\n\n先日書いたRedmineで要否プルダウンが要のときのみ入力できる項目を表示する 否のときは表示しない からの発展で、いくつかのカスタムフィールドをグルーピングして見出しを入れる方法をご紹介します。\n\n\n前提条件\n\n\n環境\n\n\nRedmine   stable \nview customizeプラグイン \n\n\n極端なバージョン依存のコードではないので、その他のバージョンでもたぶん動作します。\n\n\nカスタムフィールドの設定\n\n\n\n\nID\n名称\n形式\n備考\n\n\n\n\n\nコメント\n長いテキスト\n\n\n\n\n起票者\nユーザー\n\n\n\n\nコメント回答\n長いテキスト\n\n\n\n\n回答者\nユーザー\n\n\n\n\n\nこれらのカスタムフィールドは、以下のサンプルコードで使う用に定義していますが、標準フィールドや任意のカスタムフィールドをグルーピングすることが可能です。\n\n\nサンプルコード\n\n  \nサンプルチケット編集画面\nPath pattern    \n挿入位置 チケット入力欄の下\nType  JavaScript\n  \n\nif      div my div   length     自分で定義する my div ブロックない場合\n    div attributes   after      div attributes の後ろに、 my div ブロックを作成、その中身は以下のとおり\n     lt div id  my div  gt    append      my div ブロックを作成\n     lt fieldset id  my fieldset  gt    append      fieldset my fieldset を作成\n     lt legend gt    text  起票者入力欄      見出し「起票者入力欄」\n     input issue custom field values    parent      カスタムフィールド「起票者」\n     input issue custom field values    parent     カスタムフィールド「コメント」\n  \n   \n     lt div id  my div  gt    append      my div ブロックを作成\n     lt fieldset id  my fieldset  gt    append      fieldset my fieldset を作成\n     lt legend gt    text  回答者入力欄      見出し「回答票者入力欄」\n     input issue custom field values    parent      カスタムフィールド「回答者」\n     input issue custom field values    parent     カスタムフィールド「コメント回答」\n  \n  \n   \n \n\n\n\n\n結果\n\nRedmineのチケット編集画面はこんな感じになりました。起票者入力欄、回答者入力欄が、本スクリプトでグルーピングしているところです。\n\n\n\n解説\n\n\n\n   div attributes   after  を使って、自分が作成したいブロックを挿入しています。ちなみに、   div all attributes   after  としてもできそうな気がするのですが、画面が崩れてしまいうまくいきません。この記事を書くためにいろいろ実験をしたのですが、ここが一番ハマりました 汗 \nグルーピングしたいフィールドを    lt fieldset id  my fieldset  gt   のように作成して、そこに    lt legend gt    text  起票者入力欄  のようにして見出しを入れます。ここでは、my fieldsetに起票者入力欄、my fieldsetに回答者入力欄の見出しをつけています。\nさらなる発展として、先日書いたRedmineで要否プルダウンが要のときのみ入力できる項目を表示する 否のときは表示しない と組み合わせれば、グルーピングした領域をステータスや任意のプルダウンと連動させて表示 非表示を制御することも可能になります。\n",
      "link": "https://qiita.com/Mattani/items/d6cd77b843dbb7059f09",
      "updated": "2019-07-07 17:22:13"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【質問】動的某グラフ 順位入れ替わり の作成方法",
      "description": "\n【質問】動的棒グラフ 順位入れ替わり の作成方法\n\n以下の動画のように順位が変動しながら伸びていくグラフを作成したいです。\n\n\n\n feature youtu be\n\nChart jsを使用しても機能は限られていて、実装に手こずっています。\n\nこのようなグラフをどのように実装できるのか、アイディアまたは参考になるアルゴリズムを教えてください。",
      "link": "https://qiita.com/takaunited/items/145cd0633848fb535e5a",
      "updated": "2019-07-07 15:45:38"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "未経験者に向けたAngular入門  ページ遷移",
      "description": "\nおさらい\n\n前回までに下記について学習しました。\n\n\n\n ngFor   ngIf等の基本的なデータバインディング\n  click を使ったボタンと関数の紐づけ\n\n\nこれで枚の画面上に変数の中身を表示したり、またボタンクリックによる処理を記述することができるようになりました。\nただ一般的なアプリケーションは複数の画面で構成されている事が多いのですが、Angularではどのようにしてこれを実現するのでしょうか。\n\n今回は、複数の画面 コンポーネント で構成されたアプリケーションのひな形を作るまでの手順について紹介します。\n\n\n新しいプロジェクトの作成\n\n今回は読書リストアプリを例にしてみたいと思います。\n\nng new profilesから新しいプロジェクトを作成してください。\n※プロジェクト名は profiles としています。\n\n一回目の記事で紹介したとおり、今回はルーティングを利用しますので最初の質問にはyと答えてください。\n\n前回と同様にappフォルダの中を除くとapp component tsが自動的に作成されている事が分かります。\n\n\n前回まではコンポーネントについて意識しませんでしたが、コンポーネントは画面を構成する 部品 のようなイメージで使われます。\n今回は複数の画面を作っていきますから、つの画面ごとにコンポーネントを用意していきます。\nまたapp componentは他の画面を呼び出す親画面の役割として利用しますから、メニューの機能を持たせることにします。\n今回作るコンポーネントは下記のとおりです。\n\n\napp component・・・親画面 メニューの機能を持ちます 。\nprofile a component・・・Aさんのプロフィールを表示する画面とします。\nprofile b component・・・Bさんのプロフィールを表示する画面とします。\n\n\n少しわかりづらいので、画面のイメージをまとめてみました。\n\n\napp componentはAさんとBさんのコンポーネントを切り替えるためのメニューを持っています。\nprofile a componentはAさんの情報を、profile b componentはBさんの情報を表示するためのコンポーネント 画面 として扱いましょう。\n\nそれではまずapp componentを作りましょう。\n\n\napp componentの作成\n\napp componentは他のコンポーネントを表示するための枠として使います。\nこれをコンポーネントシェルと呼びます。\napp component自体はメニューだけを持っている構成としますので、下記の通りとしました。\n\n\napp component html\n lt h gt    title    lt  h gt \n lt p gt \n  lt a href     gt Aさんのプロフィール lt  a gt \n  lt a href     gt Bさんのプロフィール lt  a gt \n lt  p gt \n lt router outlet gt  lt  router outlet gt \n\n\n\n\napp component ts\nimport   Component   from   angular core  \n\n Component  \n selector   app root  \n templateUrl     app component html  \n styleUrls      app component css  \n  \nexport class AppComponent  \n title    profiles  \n \n\n\n\n\napp component css\na  \n margin  px \n \n\n\n\nhtmlの方にはリンクをつ設けていますが、リンク先はまだ指定していません。\nまたapp componentはリンク以外の機能を持っていませんので、tsファイルには特に処理を追加していません。\n\n重要なのはhtmlファイルの一番下に記述してある lt router outlet gt  lt  router outlet gt です。\n\nこれから作成するコンポーネントを表示する場所になりますので、決して削除しないようにしてください。\n\n\nprofile a componentの作成\n\nさてメニューが作成できたので早速他のコンポーネント 画面 を作っていきましょう。\n次のコマンドを入力して新しいコンポーネントを作成してください。\nまずはAさんのコンポーネントを作成しましょう。\n\nng g component profile a\n\n\ngはgenerateの略ですから、省略せずにgenerateとしても問題ありません。\n\nさて作成が完了したら新しいフォルダが作成されている事と思います。\n\n\nフォルダの中を除くとapp componentと同じようにhtml  ts  cssファイルが自動的に作成されている事が確認できるかと思います。\n\n早速htmlの中身をAさんの自己紹介ページに作り上げていきましょう。\n\n\nprofile a component\n  lt h gt Aさんの自己紹介 lt  h gt \n  lt hr gt \n  lt ul gt \n  lt li gt 氏名   name   lt  li gt \n  lt li gt 年齢   age  才 lt  li gt \n  lt  ul gt \n  lt h gt 自己紹介 lt  h gt \n  lt span gt   introduction   lt  span gt \n\n\n\n\nprofile a component\nimport   Component  OnInit   from   angular core  \n\n Component  \n selector   app profile a  \n templateUrl     profile a component html  \n styleUrls      profile a component css  \n  \nexport class ProfileAComponent implements OnInit  \n\n name    A  \n age    \n introduction    こんにちは私はAです。趣味はサッカーで、今年の目標はインターハイに出場する事です。  \n\n constructor      \n\n ngOnInit    \n  \n\n \n\n\n\nこれを画面で確認できるようにしましょう。早速ルーティングの設定を行っていきます。\n\n\nルーティングの設定 Aさんの自己紹介ページの表示 \n\nAngularではページ遷移 コンポーネントの表示切替 のための機能が提供されています。\n\napp componentのファイルがあるフォルダまで戻っていくと、app routing module tsというファイルがあるかと思いますが、Angularではページ遷移の設定をこのファイルに行います。\n\n\n\napp routing module tsの中を覗いてみると下記のように記述されています。\n\n\napp routing module ts\nimport   NgModule   from   angular core  \nimport   Routes  RouterModule   from   angular router  \nimport   ProfileAComponent   from    profile a profile a component  \n\nconst routes  Routes      \n\n NgModule  \n imports   RouterModule forRoot routes   \n exports   RouterModule \n  \nexport class AppRoutingModule    \n\n\n\nAngularでは、const routes  Routes      の中身に設定を追加することで、ページ遷移が行えるようになります。\n\n先程作成したprofile a componentへのページ遷移の設定を追加しましょう。\nコードを下記のように書き換えてください。\n\n\napp routing module ts\nimport   NgModule   from   angular core  \nimport   Routes  RouterModule   from   angular router  \nimport   ProfileAComponent   from    profile a profile a component  \n\nconst routes  Routes    \n   path   a   component  ProfileAComponent  \n  \n\n NgModule  \n imports   RouterModule forRoot routes   \n exports   RouterModule \n  \nexport class AppRoutingModule    \n\n\n\npathはそのページにアクセスするためのURLのようなものです。\n設定の例では、aにアクセスされたら、先程作成したProfileAComponentに表示を切り替えるようになっています。\n\n最後にaというURLにアクセスするためのリンクを作成しましょう。\napp component htmlのリンクを次のように書き換えてください。\n\n\napp component html\n lt h gt    title    lt  h gt \n lt p gt \n  lt a href  a  gt Aさんのプロフィール lt  a gt \n  lt a href     gt Bさんのプロフィール lt  a gt \n lt  p gt \n lt router outlet gt  lt  router outlet gt \n\n\n\nブラウザを更新して、Aさんのリンクをクリックして動作を確認してみましょう。\n\n\n Aさんのプロフィール というリンクをクリックするとブラウザのURLがhttp   localhost  aに変化することを確認できました。\nまた lt router outlet gt  lt  router outlet gt の中の表示もprofile a componentの表示に切り替わっている事が確認できましたね。\n\nこのようにAngularではコンポーネントの表示をURLによって切り替える事ができます。\n\nこれをルーティングと呼びます。\n\n同じようにBさんのコンポーネントも作成をしてルーティングの設定を次の通りとしました。\n\n\napp routing module ts\nimport   NgModule   from   angular core  \nimport   Routes  RouterModule   from   angular router  \nimport   ProfileAComponent   from    profile a profile a component  \nimport   ProfileBComponent   from    profile b profile b component  \n\nconst routes  Routes    \n   path   a   component  ProfileAComponent   \n   path   b   component  ProfileBComponent  \n  \n\n NgModule  \n imports   RouterModule forRoot routes   \n exports   RouterModule \n  \nexport class AppRoutingModule    \n\n\n\n\napp component html\n lt h gt    title    lt  h gt \n lt p gt \n  lt a href  a  gt Aさんのプロフィール lt  a gt \n  lt a href  b  gt Bさんのプロフィール lt  a gt \n lt  p gt \n lt router outlet gt  lt  router outlet gt \n\n\n\nBさんのコンポーネントもリンクから表示できるようになりました。\n\n\n\n次回\n\n今回は新しくコンポーネントを作成する方法と、ルーティングを使って表示を切り替えていく事でページ遷移が実現できることを確認しました。\n\n次回はコンポーネントとコンポーネントの間で変数を共有する方法を学びたいと思います。",
      "link": "https://qiita.com/toguchi/items/9973397dd03676d71aae",
      "updated": "2019-07-07 15:42:58"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "React ×WebAssemblyの動作サンプルをAssemblyScriptで簡単に作成してみる",
      "description": "こちらのReactとAssemblyScriptの環境作成が、簡単にできて素晴らしかったので、自分でも試して投稿させていただきました。m     m\n\n\nAdding a WebAssembly component to a React App\n\n\nAssemblyScriptについてはこちら\n\n\nThe AssemblyScript Book\n\n\n自分で試してみたコードはこちらです\n\n\n\n\n\n\ncreate react appでプロジェクト作成\n\n  reactのプロジェクト作成\nyarn create react app webassembly in react\n\ncd webassembly in react \n  VSCodeを使っている場合はこれでVSCodeを開き直せる\n  code  r webassembly in react  \n\n  AssemblyScriptを追加する\nyarn add  D AssemblyScript assemblyscript\n\n  AssemblyScriptの例となるtsファイルの追加、package jsonにscriptの追加を自動でやってくれる\nyarn asinit  \n\n\n\npackage json\n\n\npackage jsonのscriptを書き換えて、AssemblyScriptのビルドと、reactのビルドを行うように変更します。\n\n\npackage json\n  scripts    \n  jsstart    react scripts start  \n  jsbuild    react scripts build  \n  asbuild    asc assembly index ts  b public as api wasm  \n  start    npm run asbuild  amp  amp  npm run jsstart  \n  build    npm run asbuild  amp  amp  npm run jsbuild \n   \n\n\n\npackage json全体\n\n\npackage json\n \n  name    webassembly in react  \n  version       \n  private   true \n  dependencies    \n  react        \n  react dom        \n  react scripts      \n   \n  scripts    \n  jsstart    react scripts start  \n  jsbuild    react scripts build  \n  asbuild    asc assembly index ts  b public as api wasm  \n  start    npm run asbuild  amp  amp  npm run jsstart  \n  build    npm run asbuild  amp  amp  npm run jsbuild \n   \n  eslintConfig    \n  extends    react app \n   \n  browserslist    \n  production    \n   gt    \n  not dead  \n  not op mini all \n   \n  development    \n  last  chrome version  \n  last  firefox version  \n  last  safari version \n  \n   \n  devDependencies    \n  assemblyscript    AssemblyScript assemblyscript \n  \n \n\n\n\n\n\n\n\nassembly index ts\n\n\nyarn asinit  で自動生成された、assembly index tsにscriptを追加します。\n\n※TypeScriptではなく、AssemblyScriptで書かれています\n\n\nassembly index ts\nexport function factorial value  f   f  \n if  value        value      return  \n return value   factorial value     \n \n\n\n\n\nsrc wa api js\n\n\n↑のAssemblyScriptを読み込むためのローダーを作成\n\n\nsrc wa api js\nimport  instantiateStreaming  from  assemblyscript lib loader  \nexport default instantiateStreaming \n fetch    as api wasm  \n  \n\n\n\n\nsrc App js\n\n\nReactからAssemblyScriptを実行するように変更\n\n\nsrc App js\nimport React    useState   from  react  \nimport waApi from    wa api  \n\nfunction App    \n const  value  setValue    useState   \n const  result  setResult    useState   \n\n return  \n  lt div className  App  gt \n  lt p gt \n The factorial of\n  lt input value  value  onChange  evt   gt  setValue evt target value     gt \n is  result \n  lt  p gt \n  lt button onClick  async      gt  setResult  await waApi  factorial value    gt \n Calculate\n  lt  button gt \n  lt  div gt \n   \n \n\nexport default App \n\n\n\nこの状態でビルドします\n\nyarn start\n\n\n\n\n\n\nReactからAssemblyScriptを実行できました、素晴らしいチュートリアルでした。\n\n読んでいただいてありがとうございました。m     m",
      "link": "https://qiita.com/okumurakengo/items/4e97b0d43c1f12331fd3",
      "updated": "2019-07-08 04:19:57"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Vuex入門 初心者向けガイドをやってみた",
      "description": "以下のページがVuexの勉強でとても参考になりましたので、勉強させていただきました。m     m\n\n\nGetting Started with Vuex  a Beginner s Guide\n\n\n※この投稿にかいてあることは全て↑のリンクにかいてあることと同じです。自分の備忘録として投稿させていただきました。\n\n\nwebpackで環境作成\n\n参考サイトでは vue cliで環境作成を行っていましたが、私はwebpackで作成しました。\n\n【自分で試したサンプル】\n\n\nbash\nyarn init  y\nyarn add vue vuex\nyarn add  D webpack webpack cli webpack dev server vue loader vue template compiler vue style loader css loader babel loader  babel core  babel preset env\n\n\n\n\n  webpack config js\nconst VueLoaderPlugin   require  vue loader lib plugin   \n\nmodule exports    \n mode   development  \n module   \n rules   \n  \n test     vue   \n loader   vue loader \n   \n  \n test     js   \n loader   babel loader \n   \n  \n test     css   \n use    vue style loader    css loader  \n  \n  \n   \n plugins   new VueLoaderPlugin    \n resolve   \n extensions     vue     js   \n alias   \n vue    vue dist vue esm js \n  \n   \n devServer   \n port   \n contentBase       \n publicPath    dist   \n open   Google Chrome \n  \n  \n\n\n\n\n   babelrc\n \n  presets    \n  \n   babel preset env  \n  \n  targets    \n  node    current \n  \n  \n  \n  \n \n\n\n\n\n  src index js\nimport Vue from  vue \nimport App from    App \nimport store from    store \n\nnew Vue  \n store \n el    app  \n template    lt App  gt   \n components    App  \n  \n\n\n\n\n  src App vue\n lt template gt \n  lt div id  app  gt \n  lt h gt Sample Counter lt  h gt \n  lt Counter   gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport Counter from    components Counter  \n\nexport default  \n components   \n Counter\n  \n  \n lt  script gt \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count     count      lt  p gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default  \n data    \n return  \n count  \n   \n  \n  \n lt  script gt \n\n\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n    put variables and collections here\n   \n mutations   \n    put sychronous functions for changing state e g  add  edit  delete\n   \n actions   \n    put asynchronous functions that can call one or more mutation functions\n  \n  \n\n\n\n\n  index html\n lt  DOCTYPE html gt \n lt meta charset utf  gt \n lt title gt sample lt  title gt \n lt script src dist main js defer gt  lt  script gt \n lt div id app gt  lt  div gt \n\n\n\n\nbash\nyarn webpack dev server\n\n\n\nこの実行環境をもとにガイドを進めていきます。\n\n\n\n\nStore\n\nアプリケーション全体で参照するデータのかたまりの部分です。\n\nstore jsで以下のように設定してみます。\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n products     \n count   \n loggedInUser   \n name   John  \n role   Admin \n  \n   \n  \n\n\n\n\n storeを使ってストアを参照する\n\nstoreに全てのデータを集約させるので、 lt script gt タグのdataの部分は削除します\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count      store state count      lt  p gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default    \n lt  script gt \n\n\n\ncomputedを使うことでスッキリ書くことができます。\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count     count      lt  p gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default  \n computed   \n count    \n return this  store state count \n  \n  \n  \n lt  script gt \n\n\n\n\n\n\nmapState Helper\n\nmapStateを使うことでcomputedをさらに簡潔に書けます\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Welcome     loggedInUser name     lt  p gt \n  lt p gt Count     count      lt  p gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapState   from  vuex  \n\nexport default  \n computed  mapState  \n count  state   gt  state count \n loggedInUser  state   gt  state loggedInUser\n   \n  \n lt  script gt \n\n\n\nまた、↑のようにただstoreの値とするだけなら、文字列だけをmapStateに渡すことでさらに簡潔に書けます。\n↓で同じ意味となります。\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Welcome     loggedInUser name     lt  p gt \n  lt p gt Count     count      lt  p gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapState   from  vuex  \n\nexport default  \n computed  mapState  \n  count    loggedInUser \n   \n  \n lt  script gt \n\n\n\n\n\n\n\ncomputedにstore以外の値を渡したい場合は、スプレッド構文を使えばうまくいきます。\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Welcome     loggedInUser name     lt  p gt \n  lt p gt Count     count      Count is    parity     lt  p gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapState   from  vuex  \n\nexport default  \n computed   \n    mapState  \n  count    loggedInUser \n    \n parity     \n return this count            even     odd  \n  \n  \n  \n lt  script gt \n\n\n\n\n\n\nGetters\n\nvuexのsotreでgetterを設定すると、computedと同じように扱うことができます。\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n products   \n   id    name   Hoge   stock     \n   id    name   Fuga   stock     \n   id    name   Piyo   stock     \n   \n count   \n loggedInUser   \n name   John  \n role   Admin \n  \n   \n getters   \n depletedProducts  state   gt   \n return state products filter product   gt  product stock  lt    \n  \n   \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt ul gt \n  lt li v for   product  i  in depletedProducts   key  i  gt \n name     product name   \n  lt  li gt \n  lt  ul gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default  \n computed   \n depletedProducts    \n return this  store getters depletedProducts \n  \n  \n  \n lt  script gt \n\n\n\n\n\n\nmapGetters Helper\n\nこちらもmapGettersというヘルパーが用意されており、簡潔に書くことができます。\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt ul gt \n  lt li v for   product  i  in depletedProducts   key  i  gt \n name     product name   \n  lt  li gt \n  lt  ul gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapGetters   from  vuex \n\nexport default  \n computed   \n    mapGetters  \n  depletedProducts  \n  anotherGetter \n   \n  \n  \n lt  script gt \n\n\n\n\n\n関数を返すことで、getterに引数を渡すこともできます。\n\n getProductById  state   gt  id   gt   \n return state products find product   gt  product id     id  \n  \n\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n products   \n   id    name   Hoge   stock     \n   id    name   Fuga   stock     \n   id    name   Piyo   stock     \n   \n count   \n loggedInUser   \n name   John  \n role   Admin \n  \n   \n getters   \n depletedProducts  state   gt   \n return state products filter product   gt  product stock  lt    \n   \n getProductById  state   gt  id   gt   \n return state products find product   gt  product id     id  \n  \n   \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n product     getProductById   name   \n  lt ul gt \n  lt li v for   product  i  in depletedProducts   key  i  gt \n name     product name   \n  lt  li gt \n  lt  ul gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapGetters   from  vuex  \n\nexport default  \n computed   \n    mapGetters  \n  depletedProducts   \n  getProductById  \n   \n  \n  \n lt  script gt \n\n\n\n\n\n\nMutations\n\nstoreの内容を直接変更してはいけません。\n必ずmutationsから変更するようにしましょう。\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n count  \n   \n mutations   \n increment state   \n state count  \n  \n  \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count      store state count      lt  p gt \n  lt button  click  updateCount  gt increment lt  button gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default  \n methods   \n updateCount    \n this  store commit  increment   \n  \n  \n  \n lt  script gt \n\n\n\n\n\n\n\nパラメータを渡すこともできます\n\n incrementBy state  n   \n state count    n \n  \n\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n count  \n   \n mutations   \n incrementBy state  n   \n state count    n \n  \n  \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count      store state count      lt  p gt \n  lt button  click  updateCount  gt increment lt  button gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default  \n methods   \n updateCount    \n this  store commit  incrementBy     \n  \n  \n  \n lt  script gt \n\n\n\n\n\n\n\nオブジェクトをパラメータに渡すこともできる。\n\n mutations   \n incrementBy state    amount     \n state count    amount \n  \n  \n\n\n methods   \n updateCount    \n this  store commit  incrementBy     amount      \n  \n  \n\n\n\n\nこのようなオブジェクトを渡しても大丈夫です。\n\n methods   \n updateCount    \n this  store commit  \n type   incrementBy  \n amount  \n    \n  \n  \n\n\n\nmapMutations Helper\n\nmapMutationsを使って簡潔に書くことができます。\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n count  \n   \n mutations   \n increment state   \n state count   \n   \n incrementBy state    amount     \n state count    amount \n   \n  \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count      store state count      lt  p gt \n  lt button  click  increment  gt increment lt  button gt \n  lt button  click  incrementBy   amount       gt increment    lt  button gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapMutations   from  vuex  \n\nexport default  \n methods   \n    mapMutations  \n  increment   \n  incrementBy \n   \n  \n  \n lt  script gt \n\n\n\n\nActions\n\nmutationをcommitするときに、間に入って実行する機能です。\n非同期処理を行うときはActionsからcommitします。\n\nこちらが参考になりました\n参考  composing actions\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n count  \n   \n mutations   \n increment state   \n state count   \n   \n   \n actions   \n increment  context   gt   \n return new Promise resolve   gt  setTimeout      gt   \n context commit  increment   \n resolve context state count \n      \n  \n  \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count      store state count      lt  p gt \n  lt button  click  increment  gt increment lt  button gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nexport default  \n methods   \n async increment    \n console log await this  store dispatch  increment    \n  \n  \n  \n lt  script gt \n\n\n\n\n\n\n\n increment  context   gt   \n return new Promise resolve   gt  setTimeout      gt   \n context commit  increment   \n resolve context state count \n      \n  \n\n\ncontextは以下それぞれ受け取ることができます。\n\n\n\ncontext commit \n\n\n\nmutationをcommitする\n\n\n\ncontext state \n\n\n\nstateを取得\n\n\n\ncontext getters \n\n\n\ngettersを取得する\n\n\n\n\nなのでこのように書くと簡潔になります\n\n increment     commit  state  getters     gt   \n       \n   \n\n\n\nmapActions Helper\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n count  \n   \n mutations   \n increment state   \n state count   \n   \n   \n actions   \n increment     commit      gt  commit  increment   \n incrementAsync     commit  state      gt   \n return new Promise resolve   gt  setTimeout      gt   \n commit  increment   \n resolve state count \n      \n  \n  \n  \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Count      store state count      lt  p gt \n  lt button  click  increment  gt increment lt  button gt \n  lt button  click  incrementAsync  gt incrementAsync lt  button gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapActions   from  vuex \n\nexport default  \n methods   \n    mapActions  \n  increment  \n  incrementAsync  \n   \n  \n  \n lt  script gt \n\n\n\n\n\n\nVuexを使ったカウンター作成\n\nここまでのを組み合わせたカウンターを作成します。\n\n\n  src store js\nimport Vue from  vue \nimport Vuex from  vuex \n\nVue use Vuex \n\nexport default new Vuex Store  \n state   \n count  \n   \n getters   \n parity  state   gt  state count            even     odd \n   \n mutations   \n increment state   \n state count   \n   \n decrement state   \n state count   \n  \n   \n actions   \n increment     commit      gt  commit  increment   \n decrement     commit      gt  commit  decrement   \n incrementIfOdd     commit  getters      gt  getters parity      odd    commit  increment     false \n incrementAsync     commit      gt   \n setTimeout      gt    commit  increment        \n  \n  \n   \n\n\n\n\n  src components Counter vue\n lt template gt \n  lt div gt \n  lt p gt Clicked    count    times  Count is    parity     lt  p gt \n\n  lt button  click  increment  gt Increment lt  button gt \n  lt button  click  decrement  gt Decrement lt  button gt \n  lt button  click  incrementIfOdd  gt Increment if Odd lt  button gt \n  lt button  click  incrementAsync  gt Increment Async lt  button gt \n  lt  div gt \n lt  template gt \n\n lt script gt \nimport   mapState  mapGetters  mapActions   from  vuex  \n\nexport default  \n name   Counter  \n computed   \n    mapState   count    \n    mapGetters   parity   \n   \n methods  mapActions  \n  increment  \n  decrement  \n  incrementIfOdd  \n  incrementAsync \n   \n  \n lt  script gt \n\n\n\n\n\n\n\n最後まで読んでいただいてありがとうございました。m     m",
      "link": "https://qiita.com/okumurakengo/items/0521049e79f927632cab",
      "updated": "2019-07-07 15:07:10"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Markdownをプレビューなしで見たまま編集できるOSSを発見 ",
      "description": "\n何がMarkdownプレビューの問題か \n\n多くのマークダウンエディタは、エディタとプレビューが分かれているのが当たり前になっています。対応関係を考えなら編集する不便があります。長くなるに連れてスクロール同期の質なども影響してきます。LaTeXで論文書くときも同じ苦労があります。例えば、Microsoft WordやScrapboxのようにWYSIWYG   What You See Is What You Getだとより書きやすくなります。\n\n 見たままの編集 とエンジニアに優しいMarkdownの融合はとても強力だと思います。\n\n\nOSSのMarkdownエディタ  HyperMD\n\n念願だった見たまま編集できるHyperMDというOSSのプロジェクトを発見しました \n\n\n\n\nGiHubリポジトリ\n\n\n\nTyporaという同じような目的のプロジェクトは以前から知られていますが、OSSでないことが欠点だと思っていました。それがHyperMDによって解決していてそこがHyperMDの魅力だと思います。\n\n\n表 テーブル の編集\n\n表の編集もすごくスムーズです。\n\n表の編集は見たままの編集がすごく発揮するのではないかと思います。\n\n\n数式の対応\n\nQiita同様MathJaxで数式に対応しています。\n\n\n\nHTMLの編集\n\nMarkdownで併用できるHTMLもパカっと開いて編集できます。\n\n嬉しいことに、公式READMMEに And more に HTML in Markdown    WYSIWIG MDX is possible という項目があるためHTMLの編集もより見たままで編集できるようになる可能性があります。\n\n今日見つけたばかりで、かゆいところに手が届く感じしてて、まだまだ便利だなと思う機能が沢山ありそうです。\n\n\n手元でHyperMDを試したい \n\nGlitchで手軽に試します。以下がnpmのプロジェクトです。HyperMD公式にあったものを利用してます。Parcelが使われていて短い設定でHyperMDが試せます。\nGitHubリポジトリ  \n実際のアプリケーション  \n\n上記のリポジトリをcloneしたりして、自分でいろいろ試していけると思います。\n\n\nもっと例が見たい\n\n以下がオンラインで動く例のまとめです。\n\n\n実際のコードはこちらにあります。\n\n\n最初の例だと、HyperMDの見たままの編集と通常のエディタ CodeMirrorが使われている に切り替える例です。\n\n例には、その他絵文字のサジェスト機能とか、Mermaidによるフローチャートやシーケンス図を書いたりする例などいろいろ見つかります。\n\n\nきっかけのツイート\n\nきっかけになった自分のツイートです。\n\nTyporaみたいにMarkdownの編集とプレビューの境目がないOSSついに発見  pic twitter com eEOkuZNーRyo Ota   nwtgck  July   \n  \n\nTwitter上では、「HyperMDでローカルファイル編集したいな」などの声も上がっていて、HyperMDから色々と派生アプリ作られたらいいなと思っています。",
      "link": "https://qiita.com/nwtgck/items/902b8c66b462f89df90c",
      "updated": "2019-07-07 15:03:24"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【注意 】JavaScriptでTwitterのuidを扱うときの注意点",
      "description": "こんにちは、プログラミングスクールのレビューサイト「スクールレポート」を運営しているアカネヤ  ToshioAkaneya です。\n\n\n【注意 】JavaScriptでTwitterのuidを扱うときの注意点\n\nTwitterのユーザー識別番号であるuidは、非常に大きい場合があります。\n例  \nこのような数字は、JavaScriptでは正確に表現することができません。 この数字リテラルは近い値に変換されます。 \n\nこのようなことを防ぐために、id strという、uidをstringで取得するためのプロパティがTwitter APIには用意されています。そちらを使うようにしましょう。\n\nこの記事が参考になれば幸いです。\n\n\nはてなブックマーク・Pocketはこちらから\n\nはてなブックマークに追加\nPocketに追加",
      "link": "https://qiita.com/ToshioAkaneya/items/b95bf16b5c8a6b99cfc5",
      "updated": "2019-07-07 13:52:30"
    },
    {
      "name": "JavaScriptタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Reactをわかった気になる  Reactの特徴",
      "description": "\n本記事の目的\n\nReactという単語は聞いたことあるけれど概要はよく知らない人向けの記事になります 元々はバイト先のディレクターさんにReactを教えるためのスライドです 。\nなのでReactの特徴やその大まかな意味などの上流の部分に触れていきます。\nその中でも今回はReactの特徴について触れていこうと思います。\n\n\nReactとは\n\nReactとはユーザインタフェースを構築するためのJSライブラリです。\nよくAngularやjQueryと比較されますが、React自体はウェブフレームワークではなく「ライブラリ」ですので、実際の開発の際には他にも様々なライブラリを組み合わせて使うことが多いです。\nReactの開発はFacebookが行っていて年月現在githubのスターは以上もあるwebフロント界隈では主流になりつつあるライブラリです。\n\n\nReactの特徴\n\nReactの特徴は大きく分けてつあります。\n\n\ncomponentベース\nvirtialDOM\nJSX\n\n\nまずつ目はcomponentベースです。\n基本的にReactではつのwebページを複数のコンポーネントに分けて実装していきます。\nコンポーネントに分けることで共通部品として使ったり、コンポーネントごとに状態を管理できるためソースコードの見通しが良くなります。\nまた、Atomicデザインと非常に相性がいいです。\n→Reactのコンポーネントについて 後日公開予定 \n\n例えばReactのホームページでは上記のようにコンポーネントを分けて実装されていました 便宜上、元ページの両端をカットしています 。\n\nつ目にvirtualDOMがあげられます。\nvirtualDOMはJSで作られた仮想のDOMです。内容はJSONのため、ブラウザのDOMよりも軽量で扱いやすいという利点があります。\nwebページの部分を動的に変化させるときにReactでは度virtualDOMを作成し、もとのvirtualDOMと比較をして変化のある部分だけをブラウザに通知します。\nそうすることでブラウザのDOMの更新による処理の負担を減らし、パフォーマンスの最適化を図ることができます。この技術によりSPA Single Page Application を現実的にしました。\n→ReactのvirtualDOMについて 後日公開予定 \n\n※この図はこちらの記事を参考に作成させていただきました。\n\n最後はJSXになります。\nJSXはソースコードの記述方法に関する特徴になります。\nJSXは簡単に言うとJSのコードの中にHTMLを書くことができる仕組みです。\nロジックとマークダウンが混在している点が初見の方にとっては違和感に感じる部分だと思います。\nReactではそもそもつ目であげたとおり、webページをコンポーネント単位で実装していきます。\nなのでJSファイルとHTMLファイルで分割して実装していくよりも、つのコンポーネントにつきつのJSXファイルで実装したほうが利点が大きいです。\n→ReactのJSXについて 後日公開予定 \n\n\nhelloMessage jsx\nexport default class HelloMessage extends React Component  \n render    \n const helloStyle     color   red    \n return  \n  lt div style  helloStyle  gt \n Hello JSX\n  lt div gt \n   \n  \n \n\n\n\n上記の例だと、「Hello JSX」の文字が赤くなります。\n\n\n終わりに\n\n今回はReactの特徴について触れていきました。\n次からは各特徴をもう少し深掘りして見たいと思います。",
      "link": "https://qiita.com/fiftie/items/37b2212739b29f3e79cc",
      "updated": "2019-07-08 04:26:56"
    }
  ],
  [
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "記事の新着情報をSlackに通知するフローを考えてみた サーバーレス ",
      "description": "\n概要\n\nタイトルの内容を実現したかったが、\nコスト優先で、\n\n処理環境  gt  Lambda\nDB   gt  DynamoDB\n\n\nと、サーバレス環境で作ってみた。\n\n\n利用ツール\n\n\nAWS CloudWatch\n\n\n定期的にlambdaのスクリプトをを呼び出す\n\n\nAWS Lambda\n\n\nCloudWatchで呼び出される\n主な処理部\npythonで実装\n\n\nAWS DynamoDB\n\n\n過去のスクレイピング情報の保管\nPynamoDBを利用\n\n\nlambda uploader\n\n\nワンクリックデプロイを実現 \n\n\n\n\n\n処理フロー\n\n\nAWS CloudWatch\n\n\nLambdaの処理を呼び出す\n\n\nAWS Lambda\n\n\n過去のスクレイピングデータをDynamoDBから取得\n現ページに記載されている情報をスクレイピング\n過去登録されているかどうかを比較\n登録されていないデータをDynamoDBへ登録\n登録されたデータをSlackへ通知\n\n\n\n\n\nまとめ\n\n詳しくはリポジトリ参照ください。\n\n\n\nTODO\n\n\nSlack通知部だけまだできてない。もうすぐできる。想定。\nモデル周りがこれだけだとわかりにくいかも。もうちょっとドキュメント周り調整する。\n\n\n\n所管\n\nありがちなフローな気がするけど同じ構成で全て通して作ってる人の記事があまりなかった。\n\nNoSQLをまだあまり使ってないので使用方法がこなれてない感にやきもきで思ったより時間かかってしまった。\nhash keyとかrange keyとかの概念をようやく掴めてきた。\n\n複数のサイトのスクレイピングにも対応できるよう作ったつもりだが、いけそうかな・・ \n\nCloudWatchを今までログ確認用としか使ってなかったけど、cronのようなスケジューラとしても使えるんじゃん。\n活用の幅が広がりそう。",
      "link": "https://qiita.com/tashua314/items/dff86d89f54ef4c4a834",
      "updated": "2019-07-08 00:38:57"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS CloudFormationのLambda backedカスタムリソースでリソースの更新・削除をする方法",
      "description": "AWS CloudFormationのLambda backedカスタムリソースを利用するとAWS CloudFormationで管理できないリソースも管理することができますが、Lambda backedで作成したリソースの更新・削除するのにリソースのIDをどうやって取り回そうか悩みました。\n\n下記は解決策の案となりますが、他に良い方法があれば教えてほしいです \n\n\n前提\n\n\nAWSアカウントがある\nAWS CLIが利用できる\nAWS Lambda、CloudFormationの作成権限がある\n\n\n\nCloudFormationのテンプレート作成\n\n gt  mkdir 任意のディレクトリ\n gt  cd 任意のディレクトリ\n gt  touch cfn template yaml\n\n\nLambda backedカスタムリソースを利用して何かしらのリソースを作成・更新・削除するテンプレートとなります。\nポイントとしてはCreateResourceのひとつで完結できたら良かったのですが、CreateResourceで作成したリソースのIDを自前で取り回せなかったので、更新と削除を別リソースUpdateResourceで行うようにしました。うーん、めんどうです\n\n\ncfn template yaml\nResources \n CreateResource \n Type  Custom  CustomResource\n Properties \n ServiceToken   GetAtt CreateResourceFunction Arn\n\n UpdateResource \n Type  Custom  CustomResource\n Properties \n ServiceToken   GetAtt UpdateResourceFunction Arn\n ResourceId   GetAtt CreateResource Id\n\n CreateResourceFunction \n Type  AWS  Lambda  Function\n Properties \n Handler  index handler\n Role   GetAtt FunctionExecutionRole Arn\n Code \n ZipFile   Sub  \n import cfnresponse\n def handler event  context  \n if event  RequestType       Create  \n  なんかリソース作成\n response     Id    hoge  \n print  create resources     response  Id   \n cfnresponse send event  context  cfnresponse SUCCESS  response \n return\n\n  他のRequestTypeは無視\n cfnresponse send event  context  cfnresponse SUCCESS     \n Runtime  python\n\n UpdateResourceFunction \n Type  AWS  Lambda  Function\n Properties \n Handler  index handler\n Role   GetAtt FunctionExecutionRole Arn\n Code \n ZipFile   Sub  \n import cfnresponse\n def handler event  context  \n Id   event  ResourceProperties    ResourceId  \n if event  RequestType       Update  \n  なんかリソース更新\n print  update resources     Id \n cfnresponse send event  context  cfnresponse SUCCESS     \n return\n\n if event  RequestType       Delete  \n  なんかリソース削除\n print  delete resources     Id \n cfnresponse send event  context  cfnresponse SUCCESS     \n return\n\n  他のRequestTypeは無視\n cfnresponse send event  context  cfnresponse SUCCESS     \n Runtime  python\n\n FunctionExecutionRole \n Type  AWS  IAM  Role\n Properties \n AssumeRolePolicyDocument \n Version      \n Statement \n   Effect  Allow\n Principal \n Service \n   lambda amazonaws com\n Action \n   sts AssumeRole\n Path     \n Policies \n   PolicyName  root\n PolicyDocument \n Version      \n Statement \n   Effect  Allow\n Action \n   logs CreateLogGroup\n   logs CreateLogStream\n   logs PutLogEvents\n Resource   arn aws logs       \n\n\n\n\n動作確認\n\n\nスタック作成\n\n gt  aws cloudformation create stack  \n   stack name cfn lambda backed test  \n   template body file   cfn template yaml  \n   capabilities CAPABILITY IAM\n\n \n  StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe \n \n\n\nスタック作成して各リソースが作成できたらLambda関数のログを確認します。\n各リソースと関数のPhysicalResourceIdをパラメータにaws logs get log eventsコマンドで取得します。\n\n gt  aws cloudformation list stack resources  \n   stack name cfn lambda backed test\n\n \n  StackResourceSummaries    \n  \n  LogicalResourceId    CreateResource  \n  PhysicalResourceId         LATEST bbbfabaaffaa  \n  ResourceType    Custom  CustomResource  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    CreateResourceFunction  \n  PhysicalResourceId    cfn lambda backed test CreateResourceFunction LBGWBFVB  \n  ResourceType    AWS  Lambda  Function  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    FunctionExecutionRole  \n  PhysicalResourceId    cfn lambda backed test FunctionExecutionRole PMYTJNSZ  \n  ResourceType    AWS  IAM  Role  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    UpdateResource  \n  PhysicalResourceId         LATEST ecfbecfbfeabb  \n  ResourceType    Custom  CustomResource  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    UpdateResourceFunction  \n  PhysicalResourceId    cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n  ResourceType    AWS  Lambda  Function  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n  \n  \n \n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test CreateResourceFunction LBGWBFVB  \n   log stream name       LATEST bbbfabaaffaa   \n   output text  \n   query  events    message \n\nSTART RequestId  eadad be ea adf ce Version   LATEST\n create resources hoge\n  略 \n Response body \n   Status    SUCCESS    Reason    See the details in CloudWatch Log Stream       LATEST bbbfabaaffaa    PhysicalResourceId         LATEST bbbfabaaffaa    StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe    RequestId    cbf f ef b ddedcc    LogicalResourceId    CreateResource    NoEcho   false   Data     Id    hoge   \n Status code  OK\n END RequestId  eadad be ea adf ce\n REPORT RequestId  eadad be ea adf ce Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n   log stream name       LATEST ecfbecfbfeabb   \n   output text  \n   query  events    message \n\nSTART RequestId  dfa    bbfe Version   LATEST\n  略 \n Response body \n   Status    SUCCESS    Reason    See the details in CloudWatch Log Stream       LATEST ecfbecfbfeabb    PhysicalResourceId         LATEST ecfbecfbfeabb    StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe    RequestId    febba aff a acf fbf    LogicalResourceId    UpdateResource    NoEcho   false   Data      \n Status code  OK\n END RequestId  dfa    bbfe\n REPORT RequestId  dfa    bbfe Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\nスタック作成時にはCreateResourceでリソースの作成、UpdateResourceは呼び出しのみとなることが確認できました。\n\n\nスタック削除\n\nスタックを削除して動作を確認します。\n\n gt  aws cloudformation delete stack  \n   stack name cfn lambda backed test\n\n \n  StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe \n \n\n\nスタック削除すると当然のことながらリソースが取得できなくなるので、ログストリーム名を取得してからログを確認します。\n\n gt  aws logs describe log streams  \n   log group name  aws lambda cfn lambda backed test CreateResourceFunction LBGWBFVB  \n   output text  \n   query  logStreams    logStreamName \n\n     LATEST bbbfabaaffaa      LATEST dfacecbbbfabb\n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test CreateResourceFunction LBGWBFVB  \n   log stream name       LATEST dfacecbbbfabb   \n   output text  \n   query  events    message \n\nSTART RequestId  aca ca b  aaca Version   LATEST\n\n\n\n gt  aws logs describe log streams  \n   log group name  aws lambda cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n   output text  \n   query  logStreams    logStreamName \n\n     LATEST cedeebcefc      LATEST ecfbecfbfeabb\n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n   log stream name       LATEST cedeebcefc   \n   output text  \n   query  events    message \n\nSTART RequestId  fcde b e aef ddcffec Version   LATEST\n delete resources hoge\n  略 \n Response body \n   Status    SUCCESS    Reason    See the details in CloudWatch Log Stream       LATEST cedeebcefc    PhysicalResourceId         LATEST cedeebcefc    StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe    RequestId    da b ddd  cab    LogicalResourceId    UpdateResource    NoEcho   false   Data      \n Status code  OK\n END RequestId  fcde b e aef ddcffec\n REPORT RequestId  fcde b e aef ddcffec Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\nスタック削除時にはCreateResourceは呼び出しのみ、UpdateResourceでリソースの削除がされることが確認できました。\n\n\nまとめ\n\n若干定義が面倒になりますがAWS CloudFormationのLambda backedカスタムリソースを利用してリソースを更新・削除できることが確認できました。\n\n\n参考\n\nBlue  lambdaのログをaws cliで見る\n",
      "link": "https://qiita.com/kai_kou/items/7be2eb9a36611bb5da12",
      "updated": "2019-07-08 00:00:14"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ServerlessFrameworkでLambda ALBを使う",
      "description": "\nはじめに\n\nAPI Gatewayを利用するのであればfunctionsに記載するだけであったが\nALBをLambdaのトリガーとする場合別途ALBを作成する必要がありそうだったのでやってみた\n\n\nサービス構成\n\nALB   gt  Lambda\n\nLambdaは引数をそのままレスポンスとして返すだけのものを作成。\nALBはつのパスでのルーティングをそれぞれ同じLambdaへ、それ以外をとするものを作成。\n\n※事前に用意したもの\n  LambdaにつけるIAMロール\n  ALBにつけるVPC のサブネットIDつとセキュリティグループID \n\n\nフォルダ構成\n\n \n├ーhandler js\n└ーserverless yml\n\n\n\nLambdaソースコード\n\nコードは内容には特に関係ないので別になんでもよい\n\n\nhandler js\nmodule exports main   async  event    gt   \n return response    \n headers   \n  Content Type    application json \n   \n body  JSON stringify event  \n statusCode  \n  \n \n\n\n\n\n\nserverless yml\n\nAPI Gatewayと若干書き方は変わるが\n単純にALBをLambdaのAPIとしての口としたいだけなら\nALBとリスナーをresourcesで作成して\nfunctionsでそれに紐づけるだけでよさそう。\n\n\nserverless yml\nservice  alb lambda example\n\nprovider \n name  aws\n runtime  nodejs x\n region  ap northeast \n\nfunctions \n echo \n handler  handler main\n role  arn aws iam   role iam role   IAMロールのARN\n events \n   alb \n listenerArn  \n Ref  exampleLoadBalancerListener\n priority  \n conditions \n path   hello\n   alb \n listenerArn  \n Ref  exampleLoadBalancerListener\n priority  \n conditions \n path   world\n\nresources \n Resources \n exampleLoadBalancer \n Type  AWS  ElasticLoadBalancingV  LoadBalancer\n Properties \n Name  alb lambda example alb\n Scheme  internet facing\n Subnets \n   subnet XXXXXXXXXXXXX  サブネットID\n   subnet YYYYYYYYYYYYY  サブネットID\n SecurityGroups \n   sg ZZZZZZZZZZZZZZ  セキュリティグループID\n exampleLoadBalancerListener \n Type   AWS  ElasticLoadBalancingV  Listener \n Properties \n DefaultActions \n   Type  fixed response\n FixedResponseConfig  \n ContentType  text html\n MessageBody   lt h gt Not Found lt  h gt \n StatusCode  \n LoadBalancerArn  \n Ref  exampleLoadBalancer\n Port  \n Protocol  HTTP\n\n\n\n\n結果\n\n\nAWSコンソール上のALBの振り分けルール\n\n\n\n作成されたALBのDNSにブラウザでアクセスすると helloと worldではJSONがレスポンスされそれ以外ではNot Foundページが表示された。\n\n\nトラブルシュート\n\n\nsls deployに成功するがLambdaがALBと関連付けられてない\n\nserverlessをアップデートすると治るかも",
      "link": "https://qiita.com/aki_lua87/items/6cd4db59c45a6dd794bc",
      "updated": "2019-07-07 19:09:53"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【AWS】API Gateway経由でWebアプリにアクセスした際のレスポンスがBase形式となる",
      "description": "AWS Lambdaなどに構築したWebアプリにAPI Gateway経由でアクセスした際に、既定の設定ではレスポンスがHTMLコンテンツではなくBase形式となってしまう。\n\n\n\nこのような場合は、API Gatewayの設定でAPIの「バイナリメディアタイプ」を追加する必要がある。\n\n\n解決方法\n\n対象のAPIで 設定   バイナリメディアタイプ で バイナリメディアタイプの追加 をクリックし、text htmlまたは   を指定して、 変更の保存 をクリック。\n\n\n\nステージに反映させるためにAPIのデプロイを忘れずに行う。\n\n再度Webアプリにアクセスすると正常にHTMLコンテンツが表示された。\n\n\n\n以上",
      "link": "https://qiita.com/r-wakatsuki/items/de90155d26a7655f3d24",
      "updated": "2019-07-07 14:16:56"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Lambda用のVPC環境を、CloudFormationで構築する",
      "description": "\nはじめに\n\n外部APIにリクエストを投げるLambda関数を作ったのですが、外部API側でIP制限をかける事になったので、Lambda関数のIPを固定化する必要が出てきました。\nそこで、以下を参考にVPC環境を構築して、外部APIにリクエストを投げる際の、リクエスト元のIPアドレスを固定化しました。\n\nLambdaの実行IPアドレスを固定し、kintoneセキュアアクセスに対応する方法\n\nその際、設定のドキュメントも併せて作ろうと思ったのですが、AWS CloudFormationのテンプレートファイルを作れば、設定ドキュメント兼環境構築プログラムになると思い、以下を参考にしながらVPC環境を定義したテンプレート作ってみました。\n\nAWS CloudFormationによるVPC作成  public及びprivateサブネット作成\n\nちなみに、最初はAWS SAMでLambdaも含めて一つのテンプレートを作ろうとしましたが、AWS SAMからはVPCの構築は対応していない 年月時点 という事だったので、VPCはCloudFormationから構築する事にしました。\n\n\n作るもの\n\n今回構築するVPC環境は以下のようなイメージです。\n\n使い捨てリソースであるLambda関数に固定IPを割り当てるのは無理なので、VPC環境を構築して、Lambda関数がVPCの外に出る時に通過する、NAT GatewayにIPを割り振ることで、リクエスト元のIPアドレスの固定化を実現しています。\n\nサブネットがAvailability Zone別にセット作ってあるのは、実際にLambda関数にPrivateサブネットを割り当てた時、複数のPrivateサブネットを割り当てる事を推奨する警告が表示されたので、一応セット用意しました。\n\n\n\n\nテンプレートサンプル\n\nCloudFormationのテンプレートは以下の通りです。\n\nテンプレートリファレンスに書き方が一通り書いてあるので、時間を掛ければ書くのは難しくないですが、とにかく設定する箇所が多いので、慣れないうちは設定漏れ等で苦戦すると思います。\n\nAWSTemplateFormatVersion      \nTransform   AWS  Serverless    \nDescription  hoge環境\nResources \n    VPC環境構築                                                      \n      VPCの作成\n hogeVPC \n Type  AWS  EC  VPC\n Properties \n CidrBlock    \n EnableDnsSupport   false \n EnableDnsHostnames   false \n InstanceTenancy  default\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge vpc\n      インターネットゲートウェイの作成\n hogeIGW \n Type  AWS  EC  InternetGateway\n Properties \n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge igw\n        インターネットゲートウェイをVPCにセット\n AttachVpcGateway \n Type  AWS  EC  VPCGatewayAttachment\n Properties \n VpcId \n Ref  hogeVPC\n InternetGatewayId \n Ref  hogeIGW\n    サブネットの設定 つのAZに作成                                                        \n      Publicのルートテーブルの作成\n hogeRTblPublicAZ \n Type  AWS  EC  RouteTable\n Properties \n VpcId \n Ref  hogeVPC\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge rtb az igw\n hogeRTblPublicAZ \n Type  AWS  EC  RouteTable\n Properties \n VpcId \n Ref  hogeVPC\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge rtb az igw\n        IGW publicルーティング\n hogeRouteIGWPublicAZ \n Type  AWS  EC  Route\n Properties \n RouteTableId   Ref hogeRTblPublicAZ\n DestinationCidrBlock    \n GatewayId   Ref hogeIGW\n hogeRouteIGWPublicAZ \n Type  AWS  EC  Route\n Properties \n RouteTableId   Ref hogeRTblPublicAZ\n DestinationCidrBlock    \n GatewayId   Ref hogeIGW\n      Privateのルートテーブルの作成\n hogeRTblPrivateAZ \n Type  AWS  EC  RouteTable\n Properties \n VpcId \n Ref  hogeVPC\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge rtb az nat\n hogeRTblPrivateAZ \n Type  AWS  EC  RouteTable\n Properties \n VpcId \n Ref  hogeVPC\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge rtb az nat\n        Nat privateルーティング\n hogeRouteNatPrivateAZ \n Type  AWS  EC  Route\n Properties \n RouteTableId   Ref hogeRTblPrivateAZ\n DestinationCidrBlock    \n NatGatewayId \n Ref  hogeNatAZ\n hogeRouteNatPrivateAZ \n Type  AWS  EC  Route\n Properties \n RouteTableId   Ref hogeRTblPrivateAZ\n DestinationCidrBlock    \n NatGatewayId \n Ref  hogeNatAZ\n      NATゲートウェイの作成\n hogeNatAZ \n Type  AWS  EC  NatGateway\n Properties \n AllocationId \n Fn  GetAtt \n   hogeEipAZ\n   AllocationId\n SubnetId \n Ref  hogeSBNPrivateAZ\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge nat az\n hogeNatAZ \n Type  AWS  EC  NatGateway\n Properties \n AllocationId \n Fn  GetAtt \n   hogeEipAZ\n   AllocationId\n SubnetId \n Ref  hogeSBNPrivateAZ\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge nat az\n      EIPの作成\n hogeEipAZ \n Type  AWS  EC  EIP\n Properties \n Domain  vpc\n hogeEipAZ \n Type  AWS  EC  EIP\n Properties \n Domain  vpc\n      Publicサブネットの作成\n hogeSBNPublicAZ \n Type  AWS  EC  Subnet\n Properties \n VpcId \n Ref  hogeVPC\n CidrBlock    \n  指定したregionの最初のazを選択\n AvailabilityZone \n Fn  Select \n   \n   Fn  GetAZs    \n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge sbn public az\n hogeSBNPublicAZ \n Type  AWS  EC  Subnet\n Properties \n VpcId \n Ref  hogeVPC\n CidrBlock    \n  指定したregionの最初のazを選択\n AvailabilityZone \n Fn  Select \n   \n   Fn  GetAZs    \n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge sbn public az\n        サブネットをルートテーブルに関連付け\n hogeSbnRTblAssociationAZ \n Type  AWS  EC  SubnetRouteTableAssociation\n Properties \n SubnetId \n Ref  hogeSBNPublicAZ\n RouteTableId \n Ref  hogeRTblPublicAZ\n hogeSbnRTblAssociationAZ \n Type  AWS  EC  SubnetRouteTableAssociation\n Properties \n SubnetId \n Ref  hogeSBNPublicAZ\n RouteTableId \n Ref  hogeRTblPublicAZ\n      Privateサブネットの作成\n hogeSBNPrivateAZ \n Type  AWS  EC  Subnet\n Properties \n VpcId \n Ref  hogeVPC\n CidrBlock    \n  指定したregionの最初のazを選択\n AvailabilityZone \n Fn  Select \n   \n   Fn  GetAZs    \n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge sbn private az\n hogeSBNPrivateAZ \n Type  AWS  EC  Subnet\n Properties \n VpcId \n Ref  hogeVPC\n CidrBlock    \n  指定したregionの最初のazを選択\n AvailabilityZone \n Fn  Select \n   \n   Fn  GetAZs    \n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge sbn private az\n      セキュリティグループの作成\n hogeSecurityGroup \n Type  AWS  EC  SecurityGroup\n Properties \n GroupDescription  Security Group for hoge VPC\n VpcId \n Ref  hogeVPC\n Tags \n   Key  product\n Value  hoge\n   Key  developer\n Value  jimu\n   Key  Name\n Value  hoge vpc sg\n\n\n\n\nポイント\n\nAvailability Zoneの指定を以下のように書くことで、配置可能なAvailability Zoneのリストを取ってくきて、その件目を選択する事ができます。\n\n  指定したregionの最初のazを選択\n AvailabilityZone \n Fn  Select \n   \n   Fn  GetAZs    \n\n\nこのように書くことで、Availability Zoneをハードコーディングしなくてすむので、このテンプレートファイルはどのリージョンでも使用する事ができるようになります。\n\nまた、セキュリティグループは今回は細かく設定していませんが、以下のように最低限の記述にする事で、インバウンドなし、アウトバウンドはすべてOKという、デフォルトよりはマシな設定にしています。\n\n\n hogeSecurityGroup \n Type  AWS  EC  SecurityGroup\n Properties \n GroupDescription  Security Group for hoge VPC\n VpcId \n Ref  hogeVPC\n\n\n\nまとめ\n\n以上で設定は完了です。\nとりあえず、最初の全体図と、テンプレートファイルがあれば、設計書の代わりになるかと思います。 Elastic IP等の補足メモは必要ですが \n\nテンプレートファイルなら、見て分からなければ、最悪CloudFomationを使えば現物を再現する事もできます。\n\n最初は作るのに時間が掛ると思いますが、たくさん作っていけば、だんだん流用できる部分が増えてくるので、余裕がある時は挑戦してみてはいかがでしょうか。\n\n\n参考\n\n\nテンプレートリファレンス\nLambdaの実行IPアドレスを固定し、kintoneセキュアアクセスに対応する方法\nAWS CloudFormationによるVPC作成  public及びprivateサブネット作成\n",
      "link": "https://qiita.com/tamura_CD/items/fa36b6b80fa4dc591834",
      "updated": "2019-07-04 02:40:49"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Step Functions コールバックパターンを使ったサーバレスな並列分散処理①",
      "description": "\nはじめに\n\nひさしく触っていなかったAWSを触ることになったので、せっかくだからサーバレスに並列分散処理が組めないか検討してみたというお話です。\n構築まで含めると長くなるので、今回は構成の検討と実際の動作だけ紹介して、次回以降で構築をしていきます。\n\n\nサーバレスな並列分散処理\n\nLambdaを使ったサーバレスなシステムでは、実行時間等の制約からひとつの関数ですべての処理を終わらせるのでなく、細かいプロセスに分けた複数の関数を組み合わせて実行することが多くなります。\n\n処理の中には並列化やホストを分散させても問題ないものもあるので、ワークフローに関数を複数同時起動させて分散処理を行い、単一のプロセスに収束させる仕組みを組み込んで処理時間短縮を行うことができます。\nただ、サーバレスの環境において単一のプロセスに収束させるのは意外と組み込むのが大変な処理になります。\n分散処理は、S Putのようなトリガーで対処するなり、単に複数の関数を起動するスクリプトを前段プロセスの関数に記述するなりでできますが、プロセスの収束は分散したプロセスがすべて終了したうえで、後段プロセスを確実に一回だけ実行させられることが求められます。\n\n実装として真っ先に考え付くのは、分散したプロセスがDynamoDB等にログを書き出し、終了時に他のプロセスのログを確認して、最後に終了したプロセスが後段プロセスをトリガーするという構成です。\n非常に稀なケースではあるものの、最後に終了するプロセスがつ以上同時になることも考えられ、プロセスが同時に後段プロセスをトリガーしてしまう可能性や実装によっては後段プロセスをトリガーされない可能性があります。\n\n確実に一回だけ後段プロセスを実行させたければ、分散させたプロセスとは別の関数から定期的に状態をポーリングし、すべてのプロセスが終了したことを確認して後段プロセスをトリガーするという方法があります。\nただ、ポーリング間隔によっては処理を行っていない無駄な時間が発生し、なによりイベント駆動型のサービスを使いながらポーリングに頼るのはあまりナウい感じがしません。\n\n\n\nStep FunctionのParallelステート\n\nStep Functionには、ワークフローの途中に並列処理を挿入する「Parallel」ステートがあります。JSON形式で並列処理のワークフローを記述して簡単に並列処理を実装できます。\n\nただ、このParallelステートは、並列処理部分を決め打ちで設計しなければならず、動的に並列数を決定できません。\n数十の並列実行を記述しようものなら、ワークフロー定義のJSONもVisual Workflowも長大で可読性に乏しくなるといった問題があります。\n\n\n\nStep Functionsのコールバックパターン\n\nStep FunctionのTaskステートでは、Lambda、ECS Fargate、SNS、SQSにおいてResourceに「 waitForTaskToken」をつけることでコールバック待機状態にすることができます。\n\n  \n  StartAt   Run Workers  \n  States    \n  Run Workers    \n  Type   Task  \n  Resource   arn aws states   lambda invoke waitForTaskToken  \n  Parameters    \n  FunctionName   run workers  \n  Payload    \n  Payload        \n  TaskToken        Task Token \n  \n   \n  End  true\n  \n  \n \n\n\nこの待機状態は、ステートマシンで発行した「Task Token」を外部プロセスから「SendTaskSuccess」のAPIで受け取ることで解除することができ、複数のTask Tokenを受け取ったとしても、最初の回以外は棄却されるようになっています。\nこれを応用すれば、分散プロセスから確実に単一プロセスに復帰させることができそうです。\n\n\n\nコールバックパターンを使った並列分散処理\n\nコールバックパターンを使って並列分散処理を構成してみました。\n構成としては、メインのステートマシンと分散処理のワーカーにあたるサブのステートマシンを用意して、メインのステートマシンのLambdaからサブのステートマシンを任意の数だけ起動して並列分散処理を実行します。\n\n\n\n実際に動かしてみた\n\n上記の構成を実際に組んで実行してみました。\n以下は、構成図の「create task」でのランダムな数値 float を個キューに投げて、並列で起動させたサブステートマシンの「executor」で順次メッセージを受け取って数値の秒数Sleepするという簡単な実行例です。\nメインステートマシンを実行するとサブステートマシンが個実行され、順次成功してすべて完了するとメインステートマシンの「Start Workers」も「進行中」から「成功」に変化することが確認できました。\n\n\n\n参考\n\n\nAWSドキュメント  AWS Step Functions\n\n\nサービス統合パターン\nStep FunctionsでLambdaを呼び出す\n\n\n",
      "link": "https://qiita.com/KcMichael/items/0cc8a47ba757d293d6f4",
      "updated": "2019-07-02 11:08:23"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "cluster on AWS Lambda",
      "description": "\nLambdaの中でclusterを使ってみる\n\n\nLambdaはCPU何個 \n\nCPU   個\n\n\nメモリを増やすと性能が上がるけど \n\nCPU   個\n\n\nやってみる \n\nnode fetchを非同期で個まわす\nそれをさらにclusterでworkerをつ立てる\n\nconst os   require  os   \nconst cluster   require  cluster   \nconst fetch   require  node fetch   \nconst uuidv   require  uuidv   \n\nconst logger   console \nconst list    \n a     id        id        id        id        id      \n b     id        id        id        id        id      \n  \nconst run    \n url     \n generateOptions data   \n data subscribeId   uuidv   \n data eventTimestampUtc   new Date   toISOString   \n return  \n method   POST  \n headers   \n  Content Type    application json  \n   \n body  JSON stringify data  \n   \n   \n post data   \n return fetch run url  run generateOptions data  \n  then res   gt  res text   \n  then logger log  \n   \n all    \n const promise      \n for  const key of Object keys list    \n for  const event of list key    \n promise push run post event   \n  \n  \n return Promise all promise  \n   \n clustering disconnect   \n const numCPUs   os cpus   length \n const exit    code  signal    gt   \n if  signal   \n logger log  worker was killed by signal    signal    \n   else if  code        \n logger log  worker exited with error code    code    \n   else  \n logger log  worker success    \n  \n const keys   Object keys cluster workers  \n if  keys length        \n disconnect   \n  \n   \n for  let i   numCPUs  i  i     \n cluster fork   on  exit   exit  \n  \n   \n worker    \n logger info    cluster worker id  cluster start    \n return run all  \n  then      gt   \n logger info    cluster worker id  cluster finish    \n cluster worker kill   \n    \n   \n master    \n const promise      \n promise instance   new Promise     argv    gt   \n  promise resolve  promise reject    argv\n    \n const disconnect        gt   \n cluster disconnect   \n promise resolve  \n statusCode   \n body   succeeded  \n    \n   \n run clustering disconnect  \n return promise instance \n   \n start    \n return cluster isMaster   run master     run worker   \n   \n  \nexports handler   async      gt  run start   \n\n\n\n動いたけど\n\nこれって意味があるのかは謎です。見識があれば教えてください。\n\n以上",
      "link": "https://qiita.com/jobscale/items/39278663ef1124971b85",
      "updated": "2019-07-02 10:25:39"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "LambdaからAPI Gateway IAM認可 へのアクセス方法 Python版 ",
      "description": "AWS上でServerlessかつMicroservicesな構成の場合、Lambda内から別サービスのAPI Gatewayにアクセスすることがある。\nAPI GatewayがIAMでアクセス制限がかかっている場合のアクセス方法が分からなかったので調査した。\n\n※Node js版はこちら\n\n\n前提\n\n\nLambda  Python  \nLambdaにIAM Role付与済み\nAPI Gateway IAM認可 作成済み\n\n\n※API Gatewayの設定方法やIAM Roleの内容などはここでは取り扱わない\n\n\n結論\n\nbotocoreのSigVAuthで署名する。\n\n\nlambda function py\nimport os\nimport urllib request\nfrom botocore credentials import Credentials\nfrom botocore awsrequest import AWSRequest\nfrom botocore auth import SigVAuth\n\ndef lambda handler event  \n ENDPOINT     \n PATH     YOUR RESOURCE \n\n awsreq   AWSRequest method  GET   url ENDPOINT PATH \n\n    Sign\n credentials   Credentials \n os environ  AWS ACCESS KEY ID   \n os environ  AWS SECRET ACCESS KEY   \n os environ  AWS SESSION TOKEN   \n SigVAuth credentials   execute api   os environ  AWS REGION    add auth awsreq \n\n    Request\n req   urllib request Request awsreq url  method awsreq method \n headers awsreq headers \n with urllib request urlopen req  as res \n body   res read  \n\n    Do something \n\n\n\nSigVAuthでAWSRequestを署名する。\n⇒下記のようなHeadersが設定される\n\n\nreq headers\nX Amz Date  TZ\nX Amz Security Token  AgoJbJpZluXVjEBgaDmFwLWvcnRoZWFzdCxIkcwRQIgXKqUWxJYRxjIrA J UZadVZTZXJhZdoCIQCtfBGGCJQRRIGAaFMUzXdrMXYzVapqdUVtNQWGSqAAghhEAIaDDMOTAzMDQzODQMSIMuUdqbU zKQDSGKtBOkhXekZuxELCLuktGjEeQJGHXiRvyfQohzGRlWzmqlAoArjTjYBBwhbnFsEUylvwcXBvbEG OEyWmrpbZqeUJgZHYUvJwrjKTHOBMIYMXkZdHudjIx dApEFSAtyMsaGgFuNOxfrGTZFjt pT HfxEVbxaBXPvRvOIDfLXqvUacYeqgRkkCNOlBm Z iHyQLkWxkOeGYIxroeeBqN KeKU pDJMcc QbnTilCQx oedEmlV HRfVIswmPbYAUtAHD gcTRszlgQgZaytsIDJvZCaO SYxCmxG vrQQivQlxCkKanWK sVugoOxaycs wov TVtL ybPYsMJIvPb zNSjyhVaYOZEp i VHrvPVgVLLLyMhkPZqMecDiniomEzfBchLYpB eZ  B  SnqOBuooz QiF ydoSBMjahhEaaErGC rTIVuTGSNaNhpkZOVlg \nAuthorization  AWS HMAC SHA Credential ASIAVHFXXXXXXXXXXXXX  ap northeast  execute api aws request  SignedHeaders host x amz date x amz security token  Signature acaaabbfeaefdacaabbaabddbbd\n\n\n\nAWSRequest自体はリクエストを発行してくれないので、別途urllibやrequestsなどでリクエストを送る必要がある。\n\n\n参考\n\n\n\n\n\n",
      "link": "https://qiita.com/t_sugawara-01/items/d85e963e35b7fb3212bf",
      "updated": "2019-07-01 02:42:13"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS CloudFormationのAWS Lambda backedカスタムリソースで最新のAWS SDKを利用する",
      "description": "AWS Lambda backedカスタムリソースを利用するとAWS CloudFormationが対応していないリソースを管理することができて便利なのですが、AWS Lambdaで利用できるAWS SDK ここではPythonのboto のバージョンが最新じゃない場合に困ることがあります。\n\nAWS Lambda backedカスタムリソース  AWS CloudFormation\n\n\nそんなときにどうしたら良いものか悩んでいたのですが、AWS Lambda Layersが利用できるみたいだったので試してみました。\n\n\n前提\n\n\nAWSアカウントがある\nAWS CLIが利用できる\nAWS Lambda、Lambda Layers、CloudFormationの作成権限がある\n\n\n\nAWS Lambda Layersに最新のAWS SDKのLayerを作成する\n\nAWS Lambda Layersで最新のAWS SDKを利用する方法は下記を参考にさせてもらいました。 感謝\n\nLambda Layersで最新のAWS SDKを使用する  Qiita\n\n\n gt  mkdir 任意のディレクトリ\n gt  cd 任意のディレクトリ\n\n gt  mkdir python\n gt  pip install  t   python boto\n gt  zip  r python zip   python\n\n gt  aws lambda publish layer version  \n   layer name boto  \n   zip file fileb   python zip  \n   compatible runtimes python\n\n \n  Content    \n  Location     略   \n  CodeSha    JZMsEEyGBPgips y F XrHXJIPkcLYeazyXiLkTk   \n  CodeSize   \n   \n  LayerArn    arn aws lambda us east  xxxxxxxxxxxx layer boto  \n  LayerVersionArn    arn aws lambda us east  xxxxxxxxxxxx layer boto   \n  Description      \n  CreatedDate      T     \n  Version    \n  CompatibleRuntimes    \n  python \n  \n \n\n\n\nAWS CloudFormationのテンプレートを作成する\n\nAWS Lambda backedカスタムリソースでbotoのバージョンを確認するテンプレートを作成します。\n比較のためにLayer利用する しないのリソースを準備します。\n\n gt  touch cfn template yaml\n\n\n\ncfn template yaml\nResources \n NonUseLambdaLayer \n Type  Custom  CustomResource\n Properties \n ServiceToken   GetAtt NonUseLambdaLayerFunction Arn\n\n UseLambdaLayer \n Type  Custom  CustomResource\n Properties \n ServiceToken   GetAtt UseLambdaLayerFunction Arn\n\n  標準のbotoを利用\n NonUseLambdaLayerFunction \n Type  AWS  Lambda  Function\n Properties \n Handler  index handler\n Role   GetAtt FunctionExecutionRole Arn\n Code \n ZipFile   Sub  \n import cfnresponse\n import boto\n def handler event  context  \n print boto   version   \n cfnresponse send event  context  cfnresponse SUCCESS     \n Runtime  python\n\n   Lambda Layerのbotoを利用\n UseLambdaLayerFunction \n Type  AWS  Lambda  Function\n Properties \n Handler  index handler\n Role   GetAtt FunctionExecutionRole Arn\n Code \n ZipFile   Sub  \n import cfnresponse\n import boto\n def handler event  context  \n print boto   version   \n cfnresponse send event  context  cfnresponse SUCCESS     \n Runtime  python\n Layers \n   arn aws lambda us east  xxxxxxxxxxxx layer boto \n\n FunctionExecutionRole \n Type  AWS  IAM  Role\n Properties \n AssumeRolePolicyDocument \n Version      \n Statement \n   Effect  Allow\n Principal \n Service \n   lambda amazonaws com\n Action \n   sts AssumeRole\n Path     \n Policies \n   PolicyName  root\n PolicyDocument \n Version      \n Statement \n   Effect  Allow\n Action \n   logs CreateLogGroup\n   logs CreateLogStream\n   logs PutLogEvents\n Resource   arn aws logs       \n\n\n\nAWS CLIからCloudFormationのスタックを作成します。Lambda関数実行用のロールを作成するので、  capabilities CAPABILITY IAMオプションを指定します。\n\n gt  aws cloudformation create stack  \n   stack name cfn lambda backed test  \n   template body file   cfn template yaml  \n   capabilities CAPABILITY IAM\n\n \n  StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test xxxxxxxx xxxx xxxx xxxx xxxxxxxxxxxx \n \n\n\nスタック作成できたらリソース一覧からAWS Lambdaの関数名を取得します。\n\n gt  aws cloudformation list stack resources  \n   stack name cfn lambda backed test\n\n \n  StackResourceSummaries    \n  \n  LogicalResourceId    FunctionExecutionRole  \n  PhysicalResourceId    cfn lambda backed test FunctionExecutionRole XXXXXXXXXXXX  \n  ResourceType    AWS  IAM  Role  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    NonUseLambdaLayer  \n  PhysicalResourceId         LATEST ebbffcbacfaec  \n  ResourceType    Custom  CustomResource  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    NonUseLambdaLayerFunction  \n  PhysicalResourceId    cfn lambda backed te NonUseLambdaLayerFunctio XXXXXXXXXXXXX  \n  ResourceType    AWS  Lambda  Function  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    UseLambdaLayer  \n  PhysicalResourceId         LATEST facdeccbdacddfceb  \n  ResourceType    Custom  CustomResource  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    UseLambdaLayerFunction  \n  PhysicalResourceId    cfn lambda backed test UseLambdaLayerFunction XXXXXXXXXXXXX  \n  ResourceType    AWS  Lambda  Function  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n  \n  \n \n\n\nリソースが取得できたらLambda関数のログからbotoのバージョンを確認します。\n上記リソースリストにあるResourceTypeがAWS  Lambda  FunctionのPhysicalResourceIdが関数名になります。\nスタック作成時のログを確認しても良いのですが、ここでは簡単にしたかったので関数を実行して確認します。\n\n 標準のboto\n gt  aws lambda invoke  \n   function name cfn lambda backed te NonUseLambdaLayerFunctio XXXXXXXXXXXX  \n   log type Tail  \n outputfile txt  \n   query  LogResult    tr  d       base  D\n\nSTART RequestId  eeb  fa bf abdcc Version   LATEST\n \n    responseUrl   event  ResponseURL  e   in sendCCESS     \nEND RequestId  eeb  fa bf abdcc\nREPORT RequestId  eeb  fa bf abdcc Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\n  Lambda Layerのboto\n gt  aws lambda invoke  \n   function name cfn lambda backed test UseLambdaLayerFunction XXXXXXXXXXXXX  \n   log type Tail  \n outputfile txt  \n   query  LogResult    tr  d       base  D\n\nSTART RequestId  efb ae  ba cedaec Version   LATEST\n \n    responseUrl   event  ResponseURL  e   in sendCCESS     \nEND RequestId  efb ae  ba cedaec\nREPORT RequestId  efb ae  ba cedaec Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\nLayerを利用している関数で最新のAWS SDKを利用できることが確認できました。\n\n\nまとめ\n\nAWS Lambda LayersへのLayer作成部分もスタックに含めることができればよいのですが、Zipファイルを事前にSへ上げるなりの準備が必要で、そうなるとSのリソースを事前に作成しなきゃ。。。\nなど、、、\nどうしてもアクションで完結しなさそうだったので、Layer作成は手動ですることにしました。\n\nもう少し考えればうまくまとまりそうな気がしてますが、今のところはこれで満足です。\n\n\n参考\n\nAWS Lambda backedカスタムリソース  AWS CloudFormation\n\n\nLambda Layersで最新のAWS SDKを使用する  Qiita\n",
      "link": "https://qiita.com/kai_kou/items/db0924f8bbd0fd03eb94",
      "updated": "2019-07-01 00:00:14"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS Lambdaで形態素解析",
      "description": "\n結論\n\nMeCabではなくNagisaを使おう\nMeCabではなくJanomeを使おう\n\n\n理由\n\naws lambdaでは、pip installをしてパッケージをあげることはできるが、brewなどを使ってダウンロードしたファイルを使うのは手間がかかる。\n\n後々記事はアップデートする予定です。",
      "link": "https://qiita.com/jre233kei/items/0033372e6fe1a72f47fc",
      "updated": "2019-07-01 00:28:22"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "KotlinのローカルAWS Lambdaをデバッグ実行しました。",
      "description": "お疲れ様です。 naokiurです。\n最近の業務中は、AWS Lambdaを構築しております。\n\n業務中はPythonで書いているのですが、\n前々から興味のあるKotlinでもやりたいなあと思い、\nAWS Toolkit for Intellijを用いて、\nローカルでデバッグ実行できるように、\n準備しました。\n\nAWS Toolkitプラグインを導入している場合、\nAWS Serverless Applicationプロジェクトを作ることができるのですが、\nどんなKotlinプロジェクトの場合でもLambdaを作成することができるようになるために、\nなんの変哲もないKotlinプロジェクト上で、\n準備しました。\n\n\n環境\n\n\nMacBook Pro  Retina  inch、Early  \nJava   \nIntelliJ IDEA CE  \nSAM CLI  \n\n\n\n実施したこと\n\n\nなんの変哲もないKotlinプロジェクトを用意する\nAWS SUM CLIで雛形を生成する\nKotlin・Lambdaの構成を整える\nRunnerを作成する\n実行 \n\n\n\nなんの変哲もないKotlinプロジェクトを用意する\n\nなんの変哲もないKotlinプロジェクトを作成します。\n\n\n\nなんの変哲もないですね。\n\n\nAWS SUM CLIで雛形を生成する\n\nAWS Toolkitプラグインは、AWS SAM CLIを前提としているので、\nインストールされていると思います。\n\nせっかくなので、AWS SAM CLIに雛形を作成してもらいます。\n\nsam init  r java  d gradle\n\n\n\n\n\nKotlin・Lambdaの構成を整える\n\n\nソース移動\n\n作成してもらった雛形の以下ファイルを、\n任意のパッケージに移動します。\n\n\nApp java\nGatewayResponse java\n\n\n\n\n\n依存性追加\n\nライブラリの依存関係をgradleファイルに追加します。\n\n\ngradle build\ndependencies  \n implementation  com amazonaws aws lambda java core      追加\n implementation  org jetbrains kotlin kotlin stdlib jdk \n \n\n\n\nありがたいことにIntellijが教えてくれるので、\nimport changeをします。\n\n\n\n\nKotlin変換\n\n移動したファイルをKotlinコードに変換します。\nこちらもありがたいことにIntellijがやってくれます。\n\n\n\n自分が試した際、App ktでコンパイルエラーが発生しまっていたので、こちらを修正しました。\n return br lines   collect lt String    gt  Collectors joining System lineSeparator    \n return br lines   collect Collectors joining System lineSeparator    \n\n\n\nRunnerを作成する\n\nLambda実行用のRunnerを作成します。\n\n\n\nHandlerには変換したAppクラスを指定します。\n\n\n\n\n実行 \n\nデバッグ実行します。\n\n\n\nブレークポイントで止まりました  \n\nこれでKotlinでも、\nローカルでLambda開発ができますね \n\n\n結果\n\n\nKotlinで書かれたLambdaを、ローカルでデバッグ実行することができました。\n\n\n\n今後\n\n\nデプロイできるかをまだ試せてないので、試していきたいと思います。\nテストコードを書けるようになっていきたいと思います。\n",
      "link": "https://qiita.com/naokiur/items/548fc01240c2488de15c",
      "updated": "2019-06-30 23:34:39"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "LocalStackでAWSのサービスを試してみる",
      "description": "\n概要\n\nLocalStackとは・・・提供しているGithubによると以下のようなものです。\n\nLocalStack   Github\n\n\nLocalStackは、クラウドアプリケーションを開発するための使いやすいテスト モッキングフレームワークを提供する。\n現在、主にAWSクラウドスタックのサポートに注力している。\n\n\nAWSのサービスを個人で実際に使う前に試してみたいと常々思っていたのですが、これを使えばAWSのテスト環境を構築できそうです。\n今回はAPI GatewayとLambdaを使って簡易APIを作ってみようと思います。\n\n\n実行環境\n\nmacOS Mojave  \nDocker version  \n\n\n手順\n\n\nAWS CLIのインストール\n\n下記のコマンドでAWSをコマンドラインから操作するAWS CLIをインストールします。\n\n  pip install awscli\n\n\n\nAWS CLI   Credentialの設定\n\nAWSをCLIから使うにはCredentialが必要になるが、LocalStackではダミー値を入力します。\n\n  aws configure   profile localstack\nAWS Access Key ID  None   dummy\nAWS Secret Access Key  None   dummy\nDefault region name  None   us east \nDefault output format  None   json\n\n  tree  aws\n aws\n├ーconfig\n└ーcredentials\n\n\nこのままでもいいのですが、しばらくはAWSサービスはLocalStackしか使わないので、今作成した値をデフォルト値としてcredentialとconfigに設定しました。設定しない場合はオプションで毎回指定する必要があります。\n\n\n  aws credentials\n localstack \naws access key id   dummy\naws secret access key   dummy\n\n default \naws access key id   dummy\naws secret access key   dummy\n\n\n\n\n\n  aws config\n profile localstack \nregion   us east \noutput   json\n\n default \nregion   us east \noutput   json\n\n\n\n\nLocalStackをGithubからクローン\n\n  git clone \n\n\n\nLocalStack起動\n\n  docker compose up\n\n\nこれでdocker上にサーバが立ち上がります。\n※私はMacで試しているのですが、このコマンドではエラーになりました。対策は別の記事で書くのでそちらを参照してください。\n→localstackがエラーで起動できない\n\n立ち上げるといろいろなサービスがローカル上に構築されています。詳しくはこちら↓\n overview\n\n\nLambdaの作成\n\nAPI Gatewayを通して呼び出されるLambda Functionを作成します。\n\n下記のPythonファイルをzipにして、アップロードします。\n\n\napi practice py\nimport json\n\ndef lambda handler event  context  \n return response   message    Hello LocalStack       \n\ndef response message  status code  \n return  \n  statusCode   str status code  \n  body   json dumps message  \n  headers    \n  Content Type    application json  \n  Access Control Allow Origin      \n   \n  \n\n\n\nLambda作成コマンドを実行\n※Roleを指定していますが、ダミー値でいいようです。適当に入力します。\n\n  aws lambda create function  \n  function name api practice handler  \n  runtime python  \n  handler api practice lambda handler  \n  memory size   \n  zip file fileb   api practice zip  \n  role arn aws iam   role role name  \n  endpoint url http   localhost \n\n\nLambdaについてはここまでです。\n\n\nAPI Gateway   REST APIを作成\n\nまず適当な名前のREST APIを作成します。\n\n  aws apigateway create rest api   name  API Test    endpoint url http   localhost \n \n  id      \n  name    API Test  \n  createdDate      T  Z \n \n\n\nREST APIのルートリソースIDを取得する。\n\naws apigateway get resources   rest api id    endpoint url http   localhost \n \n  items    \n  \n  id    A ZA Z  \n  path       \n  resourceMethods    \n  GET     \n  \n  \n  \n \n\n\nルート配下に子リソースを追加する。\n今回は helloでアクセスできるように作ってみます。\n\n  aws apigateway create resource   rest api id    \n  parent id A ZA Z  \n  path part hello  \n  endpoint url http   localhost \n \n  id    A Z  \n  parentId    A ZA Z  \n  pathPart    hello  \n  path     hello  \n  resourceMethods    \n  GET     \n  \n \n\n\n作成したリソースを以下のコマンドを実行し、GETメソッドで参照できるようにします。\n\n  aws apigateway put method  \n   rest api id   \n   resource id A Z  \n   http method GET  \n   authorization type  NONE   \n   endpoint url http   localhost \n \n  httpMethod    GET  \n  authorizationType    NONE \n \n\n\n\nAPI Gateway   Lambdaの設定\n\nLambdaのARNを使うため、下記のコマンドを実行し確認する。\n\n  aws lambda list functions   endpoint url http   localhost \n \n  Functions    \n  \n  FunctionName    api practice handler  \n  FunctionArn    arn aws lambda us east   function api practice handler  \n  Runtime    python  \n  Handler    api practice lambda handler  \n  Timeout    \n  Version     LATEST \n  \n  \n \n\n\n\nAPI GatewayとLambdaの設定\n\n  aws apigateway put integration  \n   rest api id   \n   resource id A Z  \n   http method GET  \n   type AWS PROXY  \n   integration http method POST  \n   uri arn aws apigateway us east  lambda path    functions arn aws lambda us east   function api practice handler invocations  \n   passthrough behavior WHEN NO MATCH  \n   endpoint url http   localhost \n \n  type    AWS PROXY  \n  httpMethod    GET  \n  uri    arn aws apigateway us east  lambda path    functions arn aws lambda us east   function api practice handler invocations  \n  integrationResponses    \n      \n  statusCode    \n  responseTemplates    \n  application json   null\n  \n  \n  \n \n\n\n\nデプロイ\n\nここまでの設定をデプロイする。これで最後です。\n\n  aws apigateway create deployment  \n   rest api id   \n   stage name test  \n   endpoint url http   localhost \n \n  id    A Z  \n  description      \n  createdDate      T  Z \n \n\n\n\n確認コマンド\n\n長かったですが、最後に公開されているか確認しましょう。\n\n  curl http   localhost  restapis  test  user request  hello\n  body       message      Hello LocalStack         headers     Access Control Allow Origin         Content Type    application json     statusCode      \n\n\nレスポンスもちゃんと返ってきました \n\n\n終わりに\n\n今回、LocalStackというサービスでAWSのサービスを疑似体験できることがわかりました。実際の業務ではAWSを使っているので、利用方法も同じなのでサービスを試すのにはとてもよさそう \n\n\n参考\n\n",
      "link": "https://qiita.com/libra_lt/items/882d5835f7f2626f19c8",
      "updated": "2019-06-29 11:42:13"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS CognitoとなんちゃってOpenID ConnectサーバをIDフェデレーションする",
      "description": "ずいぶん前に、なんちゃってOpenID Connectサーバを自作しました。\n\n なんちゃってOAuth OpenID Connectサーバを自作する\n\n今回は、AWS Lambdaを使って実際に立ち上げてみたのち、CognitoのIDフェデレーションを使って、Cognitoから自作のOpenID ConnectにつないでID連携してみます。\n\nソース一式は以下に置きました。\n \n\n\nAWS LambdaでOpenID Connectを立ち上げる\n\nなんちゃってOpenID Connectの説明は、当時の投稿を見ていただくとして、手っ取り早く、Gitからソース一式をダウンロードしてください。\n\n gt  git clone \n gt  cd openid server\n\n\nオレオレ証明書を使うので、先に、以下を実行して公開鍵ペアを作成します。\n\n gt  mkdir api controllers oauth jwks\n gt  openssl genrsa  out api controllers oauth jwks privkey pem \n\n\n注意事項があります。\n生成されたprivkey pemの改行コードはLFである必要があります。 そうしないとnpmモジュールのrsa pem to jwkが失敗しました。私はこれではまりました \n\nとりあえず以下で、なんちゃってOpenID Connectサーバが立ち上がるかと思います。\n\n gt  npm install\n gt  node app js\n\n\n envに何も指定していなければ、ポートで立ち上がっているはずです。\n\n\n試してみる\n\nブラウザから、以下を開いてみましょう。\n\n http   localhost  login\n\n\n\nさっそく、認証してみましょう。\nclient id、scope、stateに適当な値を入れて、「サインイン開始」ボタンを押下しましょう。 なんちゃってサーバなので、値は何も見ていないのです。。 \n\n\n\nログインページが表示されます。\nここでも、ユーザID・パスワードには適当に値をいれて、「サインイン」ボタンを押下します。\n\n\n\nログインが成功し、なんちゃってIDトークンとアクセストークンが生成されました \nフローを図で示しておきます。\n一番最初のページで、response typeをtokenにした時とcodeにした時でちょっとフローが異なります。\n response type codeにおいて、本来ならセキュリティ上アプリクライアントシークレットやトークンはブラウザに渡ることがないように実装しますが、今回はわかりやすさを優先していますので、ご注意ください \n\n\n\n\n\nできたトークンを以下のサイトで内容を確認できます。\n\n \n\n\n\n\nAWS API Gateway Lambdaで立ち上げる\n\n\nLambdaアップロードのためのZIPファイルを作成する\n\n上記のようなローカルホストに立ち上げてもよいのですが、HTTPSで接続したいので、API Gateway Lambdaで立ち上げてみます。\n 独自にHTTPSで立ち上げられる場合はこの作業は不要です。 \n\nLambdaにZIPで固めてアップロードします。\n\n gt  cd api controllers oauth\n gt  npm init  y\n gt  npm install   save rsa pem to jwk\n gt  npm install   save jsonwebtoken\n gt  mkdir helpers\n gt  cp       helpers response js helpers response js\n gt  cp       helpers redirect js helpers redirect js\n\n\n最後に、oauthフォルダにあるファイルとフォルダ一式をZIPファイルに固めます。例えばindex zipとします。\n\n\nLambdaにZIPファイルをアップロードする\n\nそれでは、Lambda関数を作成しZIPファイルをアップロードします。\n\n\n\n適当に、test oauthという名前の関数にしました。\nアップロードしてみましょう。\n\n\n\nこんな感じでアップロードできました。\nnode modulesやjwks、helpersも一緒にアップロードされました。\n\n最後に、環境変数で、HELPER BASEとして「  helpers 」と設定します。\nまた、オレオレ証明書のファイルの場所がLambdaからは違って見えるので以下のようにソースコードを修正します。\n\nvar priv pem   fs readFileSync    api controllers oauth jwks privkey pem   \n→\nvar priv pem   fs readFileSync    jwks privkey pem   \n\n\nいくつか環境に合わせて修正が必要なのですが、とりあえず、API Gatewayの作成に移ります。\n\n\nAPI GatewayでAPIを作成する\n\nAPI Gatewayに新しいAPIを作成します。\n新しいAPIの作成のチェックボックスは、「Swaggerからインポート」を選択します。\nそして、エディットボックスに、api swagger swagger yamlのファイルの内容をコピペします。\nそして、「インポート」ボタンを押下します。\n\n\n\n途中警告が出ますが、「インポートして警告を無視する」ボタンを押下して、インポートを継続します。一部API Gatewayではサポートしていないものがあるためです。\n\n\n\n\n\nまず、 swaggerは使わないので削除します。\n次に、各GETやPOSTに、先ほど作ったLambda関数「test oauth」を割り当てます。\nこのとき、Lambdaプロキシ統合の使用のチェックボックスをOnにしてください。\n\n\n\n次に、各GETやPOSTに対して、CORSを有効化します。\n\n\n\n最後に、デプロイします。\nステージ名はvとしてみました。\n\n\n\nこれで、デプロイされ、URLが割り当たりました。以下の感じになっていると思います。このURLを覚えておきます。\n\n https         execute api ap northeast  amazonaws com v\n\n次に、test oauthのLambda関数に戻ります。\n\n環境変数で、BASE URLとして先ほどのAPI Gatewayで割り当たったURLを設定します。\n\n\n\n\n試してみる\n\nさっそく、Lambdaに上げたなんちゃってOpenID Connectサーバにアクセスしてみます。\nアクセスには、先ほど立ち上げたローカルのサーバを使います。\nstart jsとstart login jsとstart redirect jsのbase urlをAPI GatewayのURLに変更します。\nそうすることで、ローカルサーバではなく、API Gatewayに立ち上げた認証エンドポイント・トークン取得エンドポイントを呼び出すようになります。\nローカルで試した時と全く同じような動作となったかと思います。\n\nフローにしてみると、こんな感じです。\n\n\n\n\n\n\nCognitoのIDフェデレーション連携してみる\n\nCognitoには、それ自身でユーザアカウントのサインアップ・サインインのための機能やOpenID ConnectのIDトークン アクセストークンを生成・検証する機能提供していますが、そのアカウントとして、GoogleやLINE、YahooIDなどのソーシャルアカウントを取り込むことができます。\n\n参考\n AWS CognitoにGoogleとYahooとLINEアカウントを連携させる\n\n今回は、それと同様に、なんちゃてで立ち上げたOpenID ConnectサーバをCognitoに組み込みたいと思います。\n\nまずは、Cognitoでユーザプールを作成します。\n適当に「DummyUserPool」という名前にしました。よく使うである属性emailは標準属性として選択しておきました。\n\n\n\nあとは適当に。\n\n\n\n\n\n\n\n\n\n\n\nとりあえず、アプリクライアントは後で作成するので、次のステップに進めます。\n\n\n\n\n\nユーザプールが作成されました。\n\n\n\nついでに、ドメイン名も設定しておきます。\n\n\n\nそれではさっそく、IDフェデレーションの設定をしていきます。\n左のナビゲーションから、フェデレーションのIDプロバイダを選択します。\n\n\n\nそして、当然、OpenID Connectを選択します。\n設定していきますが、なんちゃってサーバは、値を見ないので、適当な値で構いません。\nただし、発行者のところは、\napi controllers oauth index jsの以下のところに指定した値にします。\n\n const issuer   process env ISSUER     https   localhost  \n\n\n何も変えていなければ、「https   localhost」です。 このURLに接続するわけではないのでなんでもよいですが、HTTPSである必要があります。 \n\n\n\n以下は例です。\n\nプロバイダ名 test oauth\nクライアントID test client というより、何もチェックしていないので何でもよいです \nクライアントシークレット test secret というより、何もチェックしていないので何でもよいです \n属性のリクエストメソッド GET\n承認スコープ openid profile email\n発行者 https   localhost\n\n「検出の実行」を押下します。が、反応しないので、認証エンドポイント等々の入力テキストボックスが表示されます。\n以下を設定します。\n\n認証エンドポイント 【API Gatewayで割り当てられたURL】 oauth authorize\nトークンエンドポイント 【API Gatewayで割り当てられたURL】 oauth token\nユーザ情報エンドポイント 【API Gatewayで割り当てられたURL】 oauth userInfo\nJwks uri 【API Gatewayで割り当てられたURL】  well known jwks json\n\n最後に、「プロバイダの作成」ボタンを押下します。\n\n属性マッピングの設定もします。\nOIDC属性として「email」、ユーザープール属性として「Email」を選択し、「変更の保存」を押下します。\n\n\n\n\nアプリクライアントを作成する\n\nなんちゃってOpenID Connectサーバの準備ができましたので、認証するための準備を進めます。\nまずは、いつものようにアプリクライアントを作成します。「クライアントシークレットを生成」はOnにします。\n\n\n\n以下の感じで作成されました。\nアプリクライアントIDとアプリクライアントのシークレットが割り当たりました。\n\n\n\n次に、アプリクライアントの設定に移ります。\n有効なIDプロバイダとして、先ほど設定したtest oauthを選択しておきます。ついでに、Cognito User PoolもOnにしておきましょう。後で、その意味がわかります。\nコールバックURLとログアウトURLには、ローカルに立ち上げたサーバのURLを入力します。例えば、「http   localhost  login redirect html」ってな感じです。\n許可されているOAuthフローには、Authorization code grantとImplicit grantを選択しておきます。\n許可されているOAuthスコープには、email、openid、profileを選択しておきます。\n\n\n\n\n試してみる\n\nそれでは、さっそくCognito経由でなんちゃってOpenID Connectサーバで認証してみます。\n\nローカルに立ち上げたWebサーバの一部を書き換えます。\n\nstat jsとstart redirect jsのbase urlをCognitoのドメイン名に変更します。ドメイン名は、Cognitoのアプリの統合で設定したドメインプレフィックスを含んだもので、以下の通りです。\n\n https   【ドメインのプレフィックス】 auth ap northeast  amazoncognito com\n\nstart login jsは、変更せず、API GatewayのURLのままにします。\nそれでは、ブラウザから、ローカルに立ち上げたWebサーバにアクセスしてみましょう\n\n http   localhost  login \n\n\n\nまずは、response type codeで試してみましょう。\nclient idには、Cognitoで払い出したアプリクライアントIDを指定します。\nscopeには、「openid profile email」を指定します。\nstateには適当でよいです。たとえば、「abcd」とでもしておきます。\n\n\n\nログイン方法として、test oauthとCognito User Poolのつを選択しました。ですので、表示されたページにはどちらでログインするか選択するようになっています。もちろん、test oauthを選択します。\n\nそうすると、いつものログイン画面が現れます。ユーザIDとパスワードは適当に入力します。\n\n\n\nリダイレクトされてきました。ブラウザのURLに、認可コードが返ってきているのがわかります。\n\n\n\nここで、Cognitoで払い出されたアプリクライアントIDとアプリクライアントのシークレットを入力し、「トークン生成」ボタンを押下します。\n\n無事に、Cognitoからトークンが払い出されました \n「userInfo」も呼び出せるようです。\n\n\n\nフローにしてみると、こんな感じです。\n ややこしー \n\n\n\n\n\n以上",
      "link": "https://qiita.com/poruruba/items/6ea55dd2befb92df6628",
      "updated": "2019-06-29 09:39:36"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "LambdaからAPI Gateway IAM認可 へのアクセス方法 Node js版 ",
      "description": "AWS上でServerlessかつMicroservicesな構成の場合、Lambda内から別サービスのAPI Gatewayにアクセスすることがある。\nAPI GatewayがIAMでアクセス制限がかかっている場合のアクセス方法が分からなかったので調査した。\n\n※Python版はこちら\n\n\n前提\n\n\nLambda  Node js  x \nLambdaにIAM Role付与済み\nAPI Gateway IAM認可 作成済み\n\n\n※API Gatewayの設定方法やIAM Roleの内容などはここでは取り扱わない\n\n\n結論\n\naws sdkのSigners Vで署名する。\n\n\nindex js\nconst https   require  https  \nconst AWS   require  aws sdk  \n\nexports handler   async  event    gt   \n const HOST    XXXXXXXXXX execute api ap northeast  amazonaws com \n const STAGE    dev \n const PATH     your resource \n\n const url    https     HOST    STAGE   PATH  \n const req   new AWS HttpRequest url  process env AWS REGION \n req method    GET     Default   gt   POST \n req headers host   HOST\n\n    Sign\n const signer   new AWS Signers V req   execute api   true \n signer addAuthorization AWS config credentials  AWS util date getDate   \n\n    Request\n const res   await new Promise resolve   gt  https get req endpoint href  req  resolve  \n const body   await new Promise resolve   gt  res on  data   resolve  \n\n    Do something \n \n\n\n\nAWS Signers VでAWS HttpRequestを署名する。\n⇒下記のようなHeadersが設定される\n\n\nreq headers\n \n  X Amz Date    TZ  \n  x amz security token    AgoJbJpZluXVjEBgaDmFwLWvcnRoZWFzdCxIkcwRQIgXKqUWxJYRxjIrA J UZadVZTZXJhZdoCIQCtfBGGCJQRRIGAaFMUzXdrMXYzVapqdUVtNQWGSqAAghhEAIaDDMOTAzMDQzODQMSIMuUdqbU zKQDSGKtBOkhXekZuxELCLuktGjEeQJGHXiRvyfQohzGRlWzmqlAoArjTjYBBwhbnFsEUylvwcXBvbEG OEyWmrpbZqeUJgZHYUvJwrjKTHOBMIYMXkZdHudjIx dApEFSAtyMsaGgFuNOxfrGTZFjt pT HfxEVbxaBXPvRvOIDfLXqvUacYeqgRkkCNOlBm Z iHyQLkWxkOeGYIxroeeBqN KeKU pDJMcc QbnTilCQx oedEmlV HRfVIswmPbYAUtAHD gcTRszlgQgZaytsIDJvZCaO SYxCmxG vrQQivQlxCkKanWK sVugoOxaycs wov TVtL ybPYsMJIvPb zNSjyhVaYOZEp i VHrvPVgVLLLyMhkPZqMecDiniomEzfBchLYpB eZ  B  SnqOBuooz QiF ydoSBMjahhEaaErGC rTIVuTGSNaNhpkZOVlg   \n Authorization   AWS HMAC SHA Credential ASIAVHFXXXXXXXXXXXXX  ap northeast  execute api aws request  SignedHeaders host x amz date x amz security token  Signature acaaabbfeaefdacaabbaabddbbd \n \n\n\n\nAWS HttpRequest自体はリクエストを発行してくれないので、別途httpsやrequestsなどでリクエストを送る必要がある。\n\n\n参考\n\n\n\n\n\n",
      "link": "https://qiita.com/t_sugawara-01/items/bd247b4326b81056396c",
      "updated": "2019-07-01 02:14:41"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ElasticTranscoder   Lambda   Notification SNS ",
      "description": "\nElastic Transcoder\n\n\n概要\n\n動画ファイルをストリーミング配信に適した形に変換するMediaサーバー\n\n\n使い方\n\n\nSにInput Output ThumbnailのBucketを作成する\n\nPipelineを作成する 上で作ったSのBucketを指定する \n\nNotification\nここでは完了時とエラー時にDBのステータスを変更したかったので、CompleteとErrorを設定\n\n\n\n\nJOBの作成\n\n\nテスト位ならコンソールから実施可能\n今回はSにClientからUploadされたら自動でJOBを作成する\n\n\n\n  Sのトリガーは、現時点    ではSNS・SQS・Lambdaしかないので、今回はLambda\n\n\n use strict  \n\nconst AWS   require  aws sdk   \nvar transcoder   new AWS ElasticTranscoder  \n region   ap northeast  \n   \n\nexports handler   function  event  context   \n\n console log  start   \n const bucket   event Records   s bucket name \n const key   event Records   s object key \n const pipelineId    xxxxxxx  \n const genericpPresetId         System preset  Generic p\n const hlsMPresetId         System preset  HLS M\n const srcKey   decodeURIComponent event Records   s object key replace     g        \n const newKey    hoge  \n\n const params    \n PipelineId  pipelineId \n OutputKeyPrefix   dest   \n Input   \n Key  srcKey \n FrameRate   auto  \n Resolution   auto  \n AspectRatio   auto  \n Interlaced   auto  \n Container   auto \n   \n Outputs   \n  \n Key  newKey     ts  \n ThumbnailPattern   hls     newKey      count   \n PresetId  hlsMPresetId \n SegmentDuration     \n    \n Playlists   \n  \n Format   HLSv  \n Name  newKey     plst  \n OutputKeys   \n newKey     ts  \n   \n   \n   \n   \n\n transcoder createJob params  function  err  data   \n if  err   \n console log  Error       err  \n   else  \n console log  Success   \n  \n    \n console log  end   \n \n\n\nざっくりとsample\n\n\nSimple Notification Service\n\n\nやること\n\n\nTopicの作成\nSubscriptionの作成\n作成したTopicに対して作る\n今回はAPIにて受けるため、以下のような感じ\n\nAPI\n\n\n\n const obj   JSON parse req body  \n if  obj Type      SubscriptionConfirmation    \n await sns confirmSubscription   TopicArn  obj TopicArn  Token  obj Token    promise   \n return \n   else if  obj Type      Notification   amp  amp  obj Message     undefined   \n log debug loglocation  obj Subject         obj Message  \n    DB更新処理\n return \n  \n\n\nSubscriptionConfirmationはSubscription作成時のConfirmを自動で対応するような仕組み\n\n\nいざ実行\n\n\nSのinput bucketにupload\n\nJOBが生成されて、しばらくするとOutput bucketに作成される\n\n\n\n\n暗号化とかセキュリティー\n\n\nTranscode後はPipelineにてKMS暗号化を設定している Pipelineを作成する参照 \nElasticTranscorderではDRMかHLS AESが可能\nDRMは他者サービス使わないとだめなので、お金がかかるし、大変\nHLS AESで考えてみたら、暗号化キーの流出が心配\nどうしよう\n結果今回は、CloudFrontで認証ユーザーのみ閲覧できるようにした\n",
      "link": "https://qiita.com/rmdroid/items/c33260e3668f3db61ad9",
      "updated": "2019-07-06 01:53:05"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS Lambda API Gatewayを使ってSlackをカスタマイズする",
      "description": "\n概要\n\nSlackのスラッシュコマンドを使って業務効率化する話です。\nサーバーレスとの相性が良く、Lambdaを使ったんですがまとまった記事がなかなか見つからなかったので書いてみました。\n\n\nスラッシュコマンドとは \n\n公式ドキュメント\n\n から始まる特定の形式のメッセージをトリガーにして、別のアプリを起動してごにょごにょできるやつです。\nデフォルトでビルトインされてるコマンドもいくつかあります。 一覧 \n feed subscribe  url でRSSを購読したり、\n remind  channel   message   when でリマインダーを登録したりといった感じです。\n\n\n\n\n独自のスラッシュコマンドを作ってみよう\n\nボットを作るときと同じようにSlackアプリを作成します。\n以下のURLから Create New App を選択してごにょごにょしてください。\n\n\n設定するのは Slash Commands のみです。\n\n\n Slash Commands ではトリガーとなるコマンド名と、エンドポイントを入力します。\nコマンド名は  この部分 です。イカした名前をつけましょう。\nエンドポイントに関して、httpsじゃないとダメみたいな記事を見かけますが、\nドキュメント読む限り自分のワークスペース内だけならhttpでもいいようです。\n\n\nRequest URL   the URL we ll send the data payload to  when the command is invoked by a user  You ll want to use a URL that you can setup to receive these payloads as we ll describe later in this doc  If public distribution is active for your app  this needs to be an HTTPS URL  and self signed certificates are not allowed   If you re just building an internal integration solely for your own workspace  it can be plain HTTP \n\n\n画像では Permissions にもチェックがついてますが、\nこれはいまから作るものでチャンネルへの投稿とファイルのアップロードがしたかったからです。\nchat write botとfiles write userを付与してあります。\n\nSlack側の設定はこれだけです。\nあとで使うので OAuth Access Token と Verification Token をメモっておきます。\n\n\nアプリを作る\n\nトリガーするアプリを作成します。\nさくっとサーバーレスでやりたいので、\nAWS LambdaとAmazon API Gatewayを使うことにしました。\n\n\n\nで、何を作るかなんですが、がっつり業務に関係のあるもので、\n入力したEANコードのバーコード画像を生成して指定のチャンネルにポストするというものです。\n      渋い。\n\nバーコード生成するとこなんて大半の人は一生使わない実装だと思うので読み飛ばしてください。\n\n\nlambda function rb\nrequire  slack \nrequire  barby barcode ean  \nrequire  barby outputter png outputter \n\nCHANNEL     バーコード生成 \nVERIFY TOKEN   ENV  VERIFY TOKEN  \n\ndef lambda handler event   context  \n Slack configure do  config \n config token   ENV  SLACK TOKEN  \n end\n\n if event  token      VERIFY TOKEN\n Slack chat postMessage  channel   gt  CHANNEL   text   gt   SlackのVerifyトークンが違う  \n return\n end\n\n ean code   event  text  \n file path     tmp   ean code  png \n begin\n baryby   Barby  EAN new ean code \n rescue ArgumentError   gt  e\n return    text   gt   EANコードの形式ではありません    response type   gt   ephemeral   \n end\n\n File open file path   w   do  f \n f write baryby to png  xdim   gt   \n end\n\n Slack files upload \n  file   gt  Faraday  UploadIO new file path   image png   \n  channels   gt  CHANNEL \n  initial comment   gt  ean code\n  \n return    text   gt   実行終了    response type   gt   ephemeral   \nend\n\n\n\n\nVerification Tokenについて\n\nif event  token      VERIFY TOKEN\n\n\nそのリクエストが本当にSlackから呼び出されたものかどうかを検証しています。\nこのVerificaction Tokenはすでに非推奨なんですが、簡単にできるので。。。\n\n\nThis deprecated Verification Token can still be used to verify that requests come from Slack  but we strongly recommend using the above  more secure  signing secret instead \n\n\nいまはSigned secretsを使うのが推奨されてます。\nちなみにSlackから送られてくるリクエストペイロードにはトークンの他にもいろいろメタ情報が含まれてます。\n\n\ntoken gIkuvaNzQIHgATvDxqgjtO\n amp team id T\n amp team domain example\n amp enterprise id E\n amp enterprise name Globular Construct Inc\n amp channel id C\n amp channel name test\n amp user id U\n amp user name Steve\n amp command  weather\n amp text \n amp response url \n amp trigger id  dfe\n\n\n\n\nレスポンスについて\n\nreturn    text   gt   実行終了    response type   gt   ephemeral   \n\n\nContent Type  application jsonで返します。\ntextに設定されているメッセージが、スラッシュコマンドを実行したチャンネルに投稿されます。\nresponse typeはephemeralの場合、自分にしか見えないメッセージになります。\n他の人にも見えるようにしたい場合はin channelを指定します。\n\n\nスラッシュコマンドのレスポンスのタイムアウトは秒です。\n秒以内にレスポンスを返せない場合は、Timeout reachedというメッセージが投稿されます。\n\nこのタイムアウトはどうやっても伸ばせないんですが、リクエストペイロードに含まれているresponse urlに対してPOSTするという代替策が用意されています。\nこちらは分以内回までという上限で使用できるそうです。\n\n\nThe response url can be used up to  times  within  minutes of the command being invoked  Just like a cake left around for  minutes will definitely be eaten  don t leave it too late \n\n\n\nファイルのアップロードについて\n\n一時的に生成している画像のため、メッセージのattachmentとして投稿することはできません。そのため、files uploadAPIを使用しています。\n\nLambdaで tmpディレクトリ使えるの地味に便利だなと思いました。\n\n\nLambdaをごにょごにょする\n\nbarbyなどのgemはLambdaの環境にないので、ソースと一緒にパッケージングする必要があります。\n\nvendor bundle配下にインストールして\n\nbundle install   path vendor bundle\n\n\nこれらをまとめてzipでポンします。\n\nlambda function rb\nGemfile\nGemfile lock\nvendor bundle\n\n\nLambdaに直接アップロードできるのはMBまでという認識だったんですが、MBでも普通に動きます。 インライン展開はされませんが \n\n\nアップロードしたら、環境変数にSlackのトークンを登録します。\n\n\nAPI Gatewayをごにょごにょする\n\nざっくり説明するとLambda関数にユニークなURLを与えて外の世界に開放してくれるやつです。\nLambda以外にも色々使えます。\n\nぱっと見設定するところは多いんですがそんなに複雑ではないです。\n\n\nAPIの作成\nステージの作成\nリソースの作成\n\n\nPOSTを作成\n統合リクエストでLambda関数を指定\nマッピングテンプレートを指定\n\n\nデプロイ\n\n\n\nAPIの作成\n\nRESTで新しいAPIを指定。\n\n\n\nステージの作成\n\n環境みたいなものです。外に公開するようなものでもないのでつでいいです。\n適当にdefaultって名前をつけました。\n\n\n\nリソースの作成\n\nつしかエンドポイント作らないんだけどRESTになるように設定しないといけない。\nスラッシュコマンドはContent Type  application x www form urlencodedのPOSTリクエストを送ってくるので、POSTを作成します。\n\n\n一番肝となるのは 統合リクエスト の設定です。\n\n\nさっき作ったLambda関数を指定するのと、マッピングテンプレートの追加を行います。\nこのマッピングテンプレートなるものは、リクエストヘッダー、ボディ、クエリストリングをいい感じにごにょごにょしてLambdaに渡す機能を担っています。\n\nSlackがtoken foo amp text bar   みたいな感じで送ってくるものをLambdaの利便性のためにjsonに変換しています。\n\n\n\nなにこれ独自のDSL と思ったらVelocity Template LanguageというApacheが提供してるテンプレート言語みたいです。初めて見た。\n変数の詳細は公式ドキュメントをご覧ください。\n\n \n  set   qs    input body  \n  foreach   pair in  qs split    amp      \n  set   arr    pair split         \n   arr        arr    if   foreach hasNext    end\n  end\n \n\n\nQiitaがVTLのシンタックスハイライトに対応してないだと    \n\nこれはすごい単純な例なので、空のチェックしたいとか、ネストさせたいとかある場合はもっとごにょごにょする必要があります。がんばってください。\n\nなにはともあれこれでLambda側でeventからハッシュで取り出してごにょごにょできます。\n\n アクション    gt   APIのデプロイ を行って、発行されたURLをSlackAppのスラッシュコマンドに設定して終わりです。\n\n\n終わり\n\n\n\nいろいろ業務効率化ツール作れると思います。\nLambda   API Gatewayのさくっと作れる感とも親和性良いですね ",
      "link": "https://qiita.com/sakuraya/items/bc9af6f8983c80e75b99",
      "updated": "2019-07-01 03:21:33"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS CodeBuildを使ってRuby lambda functionにデプロイする",
      "description": "方法論的なものが少なかったので。\nRubyだけでなく、他の言語でも応用が効くはず。\nCodePipelineと組み合わせると強力。\n\n\nTLDR\n\n\nbuildspec yml\n 抜粋\nphases \n install \n runtime versions \n ruby  \n commands \n   rbenv install    amp  amp  rbenv global  \n   bundler install   path vendor bundle等\npost build \n commands \n   zip data zip   rb  r vendor  amp  amp  aws lambda update function code   function name  FUNCTION NAME    zip file  fileb   data zip \n\n\n\n\n解説\n\nbundle installした状態のファイル群をまとめてlambdaに投げたい。\n\n\nRubyバージョンの乖離\n\n  現在、lambdaのRubyは通常しか選択できない。\nCodeBuildはRubyしか選択できない。\nbundle install   path vendor bundleしたときに困ってしまう。\nCodeBuildのログを見る感じrbenvが入っているので使いたいRubyバージョン入れてしまおうというアプローチ。\n\n\nlambdaへの反映\n\nawsコマンドが使えるので使う。zipを作って普通にアップロード。CodeBuildの権限ロールにlambdaのアクセス権限つけるのを忘れずに。 AWSLambdaFullAccessをつければ動くことは確認 \n\n\n別のアプローチ\n\nカスタムコンテナ使えば解決ではあるが・・・。\n\n\n補足\n\n実際のbuildspec ymlのコマンド欄にはhoge shみたいにシェルスクリプトを指定したほうが見通しや管理が楽。\nzipするところは必要なファイル群を入れておくこと。",
      "link": "https://qiita.com/ir-yk/items/caba25746b7ef1df3035",
      "updated": "2019-06-26 12:08:01"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "New RelicのAWS Lambdaサポートに NET Coreが追加されるようです まだアルファ版 ",
      "description": "New RelicはAWS Lambdaで動くコードのAPM機能をリリースしているのですが、 NET Coreについては未対応でした。New Relicでは NET CoreのAPMは NET Coreのプロファイリング機能を利用しているのですが、Lambdaの上では動かなかったようです。\n\nIntroduction to New Relic monitoring for AWS Lambda\n\n最近、以下のページが公開されており、どうやら NET Coreもサポートされるようになりそうです。ドキュメントの注意書きにある通りまだアルファ版で未確定ですのでご留意ください。\n\nEnable New Relic  NET Core agent for AWS Lambda monitoring\n\nこの機能を試してみようと思いますが、まずはNew Relicを設定しない NET Coreコードを動かしてみましょう。\n\n\nまず NET Core をLambdaで動かしてみる\n\nC のAWS Lambdaデプロイパッケージを参考にしつつ進めていきます。\n\nLambdaで NET Coreを動かす場合プロジェクトのフォーマットとしてはクラスライブラリでOKです。TargetFrameworkはnetcoreappとしています。プロジェクトファイル プロジェクト名 csproj は以下のようになりますが、つNuGetパッケージを追加しています。\n\n lt Project Sdk  Microsoft NET Sdk  gt \n\n  lt PropertyGroup gt \n  lt TargetFramework gt netcoreapp lt  TargetFramework gt \n  lt  PropertyGroup gt \n\n  lt ItemGroup gt \n  lt PackageReference Include  Amazon Lambda Core  Version        gt \n  lt PackageReference Include  Amazon Lambda Serialization Json  Version       gt \n  lt PackageReference Include  Amazon Lambda SEvents  Version      gt \n  lt  ItemGroup gt \n\n lt  Project gt \n\n\nAmazon Lambda Coreはラムダ関数の実行時の情報やログプロバイダーを提供します。Amazon Lambda Serialization Jsonはラムダのトリガーとなるイベントペイロードをシリアル化する際に必要になる汎用シリアルライブラリです。このつは通常必ず追加すると考えて良さそうです。\n最後のAmazon Lambda SEventsですがこれはトリガーとなるイベントごとにライブラリが提供されています。今回はSイベントをテストに使うのでこれを選択しています。これらの情報は以下のページに記載があります。\n\nC によるLambda関数のビルド\n\nこれらを踏まえて、Sのイベントで発火して、イベント情報を出力するだけのコードは次のようになります。\n\nusing System \nusing Amazon Lambda Core \nusing Amazon Lambda SEvents \n\n assembly LambdaSerializer typeof Amazon Lambda Serialization Json JsonSerializer   \nnamespace LambdaNetCore\n \n public class Function\n  \n public string FunctionHandler SEvent input  ILambdaContext context \n  \n foreach  var record in input Records \n  \n context Logger LogLine   S  record EventName  event fired   record S Bucket Name   record S Object Key    \n  \n return  OK  \n  \n  \n \n\n\nこのつのファイルを含むプロジェクトをビルド・発行して、アプリのdll含む一式をzipしたものをLambdaにアップロードすればOKです。\n\n\n\n一連のアップロード作業が面倒であれば、Lambdaの NET Core CLI拡張を使うと次のコマンドでアップロードできます。\n\ndotnet lambda deploy function  lt lambdaの名前 gt    region  lt us west など gt    function runtime dotnetcore\n\n\nあとはSイベントをトリガーにしてテスト実行するとLambdaが実行できます。\n\n\n\nさて、New RelicのAWS Cloud Integationを有効にしていると、Cloud Watchで取得できる情報はモニタリングできます。New Relic Oneではこのように表示されます。\n\n\n\nこれだけでもそれなりに情報はあるのですが、ErrorsやInvocationsにある詳細な情報をみることはできません。また実際、「Set up AWS Lambda」というメッセージが表示されています。そこで最初に紹介した手順にしたがって有効化してみましょう。\n\n\n NET CoreでNew Relic Lambdaを有効にする\n\nNew RelicでLambdaモニタリングを有効化するには、アプリケーションコードに手を入れることと、ログの転送などを有効化する必要があります。後者はどの言語のコードでも共通ですので、先ほどの「Set up AWS Lambda」を開いて手順を実施してください。\nアプリケーションコード側の対応ですが、まずNuGetライブラリを追加してます。プロジェクトファイルのPackageReferenceのあるところに次の行を追加しましょう。\n\n  lt PackageReference Include  NewRelic OpenTracing AmazonLambda Tracer  Version    alpha    gt \n\n\nなんとなく名前からして、OpenTracingに基づいたTracer実装のような気配を感じます。New RelicとしてOpenTelemetryプロジェクトにコミットしていくという記事が先日出ていますので、よろしければこちらもご参照ください。\n\nOpenTracing  OpenCensus  OpenTelemetry  and New Relic\n\nさて、NuGetライブラリの追加に加えて、コードそのものも多少変更します。ドキュメントにある雛形通りに追加するのですが、今回であれば次のようになります。\n\n\nusing System \nusing Amazon Lambda Core \nusing Amazon Lambda SEvents \nusing NewRelic OpenTracing AmazonLambda \nusing OpenTracing Util \n\n assembly LambdaSerializer typeof Amazon Lambda Serialization Json JsonSerializer   \nnamespace LambdaNetCore\n \n public class Function\n  \n static Function  \n  \n    Register The NewRelic Lambda Tracer Instance\n GlobalTracer Register NewRelic OpenTracing AmazonLambda LambdaTracer Instance  \n  \n\n public string FunctionWrapper SEvent input  ILambdaContext context \n  \n    Instantiate NewRelic TracingWrapper and pass your FunctionHandler as \n    an argument\n return new TracingRequestHandler   LambdaWrapper FunctionHandler  input  context  \n  \n\n public string FunctionHandler SEvent input  ILambdaContext context \n  \n foreach  var record in input Records \n  \n context Logger LogLine   S  record EventName  event fired   record S Bucket Name   record S Object Key    \n  \n return  OK  \n  \n  \n \n\n\nFunctionHandlerという元のハンドラをラップするFunctionWrapperを追加し、こちらが新しいハンドラになるのでラムダの設定も変更します。また、New Relicのライセンスキーを環境変数で追加します。\n\n\n\nこれでログの転送もできていればNew Relicでモニタリングできます。テスト実行してみましょう。SummaryをみるとSet upのメッセージが消えています。\n\n\n\nそしてInvocationsが表示されています。Errorsも表示されますが、まだ件です。\n\n\n\nこれで NET CoreでAWS Lambdaを動かした場合もNew Relicでモニタリングできました。Lambdaでモニタリングできて嬉しいのは分散トレーシングを使っている場合ですので、そちらについては別途記事を書きたいと思います。",
      "link": "https://qiita.com/takayoshitanaka/items/ba16bad9d695f82d22da",
      "updated": "2019-06-26 04:03:49"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "docker lambdaで関数連続実行をワンコンテナで",
      "description": "\nワンコンテナでlambada関数を連続実行したい\n\nlambci docker lambdaでローカルテストする際に、下記のように普通に関数を連続実行すると   \n\ndocker run   rm  v   PWD   var task lambci lambda python sample function lambda handler    Hello   World   \ndocker run   rm  v   PWD   var task lambci lambda python sample function lambda handler    Hello   World   \n\n\n下記のようにコンテナが別々に生成される。 ※docker container statsの結果を表示↓↓ \n\n\nこれをワンコンテナでやりたい場合は、下記のようにentrypointを上書きしてやれば可能。\n\ndocker run   rm  v   PWD   var task   entrypoint    lambci lambda python bash  c    \n python  var runtime awslambda bootstrap py sample function lambda handler     Hello     World      amp   \n python  var runtime awslambda bootstrap py sample function lambda handler     Hello     World      \n \n\n\n",
      "link": "https://qiita.com/rindaman1982/items/b4e3bcfdd5d55542513d",
      "updated": "2019-06-24 15:29:23"
    },
    {
      "name": "lambdaタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Amazon Inspectorの結果を前回結果と比較してレポートする",
      "description": "Inspectorの実行結果をCSVファイルとして出力するの続き。\n\n\n概要\n\n前回紹介した「Inspectorの実行結果をCSVファイルとして出力する」のLambdaファンクションの終了をトリガーに、今回紹介する新しいLambdaファンクションを起動する。このLambdaファンクションは以下の処理を実行する。\n\n\n直近と前回のInspectorの実行結果を比較して差分をCSVレポートに出力する。\nCSVファイルをSへアップロードし、Pre Signed URLを生成して関係者へSNS経由で通知する。\n\n\n\n\n図の上段のLambdaが前回紹介した部分。今回紹介する部分は下段のLambda。\n\n前提として、前回紹介したInspectorの実行結果をCSVファイルとして出力するのLambdaファンクションにより、毎回のInspectorの実行結果はCSVでS上に出力されているものとする。これらを分析するため、あらかじめAthenaのテーブルを作成しておく必要がある。\n\n\nAthenaのテーブル作成\n\n今回、CSVファイルの差分検出にはAthenaを使う。前回の記事で生成したCSVファイルのレイアウトをAthenaが認識できるよう、テーブルを作成しておく必要がある。ファイルの容量は大したことないので、パーティションは考慮しない。\n\n\ninspector sql\nCREATE EXTERNAL TABLE  inspector  \n  arn  string COMMENT  from deserializer   \n  assessmentrunarn  string COMMENT  from deserializer   \n  agentid  string COMMENT  from deserializer   \n  rulespackagename  string COMMENT  from deserializer   \n  severity  string COMMENT  from deserializer   \n  numericseverity  string COMMENT  from deserializer   \n  confidence  string COMMENT  from deserializer   \n  id  string COMMENT  from deserializer   \n  title  string COMMENT  from deserializer   \n  description  string COMMENT  from deserializer   \n  recommendation  string COMMENT  from deserializer  \nROW FORMAT SERDE \n  org apache hadoop hive serde OpenCSVSerde  \nWITH SERDEPROPERTIES   \n  escapeChar    n   \n  quoteChar        \n  separatorChar       \nSTORED AS INPUTFORMAT \n  org apache hadoop mapred TextInputFormat  \nOUTPUTFORMAT \n  org apache hadoop hive ql io HiveIgnoreKeyTextOutputFormat \nLOCATION\n  s   hoge bucket \nTBLPROPERTIES  \n  skip header line count    \n \n\n\n\n\n差分抽出クエリ\n\nInspectorの前回結果と今回結果の差分を取得するクエリは、Lambdaファンクション内で以下のコードによって生成している。\n\n\nsample py\n Inspectorの前回結果と今回結果の差分を取得するクエリを生成して返す\ndef createQuery assessmentRuns  \n return    \nwith bef as  \n select \n agentid \n rulespackagename \n severity \n numericseverity \n id\n from inspector\n where\n assessmentrunarn       \n  \naft as  \n select \n agentid \n rulespackagename \n severity \n numericseverity \n id\n from inspector\n where \n assessmentrunarn       \n  \nmst as  \n select distinct\n computername \n instanceid\n from ssminventory aws instanceinformation\n \n\nselect \n     as diff \n bef agentid \n mst computername \n bef rulespackagename \n bef severity \n bef numericseverity \n bef id\nfrom bef\nleft outer join aft on \n bef agentid   aft agentid\n and bef rulespackagename   aft rulespackagename\n and bef severity   aft severity\n and bef id   aft id\nleft outer join mst on\n bef agentid   mst instanceid\nwhere\n aft id is null\nunion\nselect\n     as diff \n aft agentid \n mst computername \n aft rulespackagename \n aft severity \n aft numericseverity \n aft id\nfrom aft\nleft outer join bef on \n aft agentid   bef agentid\n and aft rulespackagename   bef rulespackagename\n and aft severity   bef severity\n and aft id   bef id\nleft outer join mst on\n aft agentid   mst instanceid\nwhere\n bef id is null\norder by  \n    format \n  前回のassessmentRunsArn\n assessmentRuns    arn   \n  今回のassessmentRunsArn\n assessmentRuns    arn  \n  \n\n\n\nまずはwith句でつのサブテーブルを定義。 ここではbef、aft、mstのつ。 \nbefはInspectorの前回実行結果、aftは今回実行結果。mstはECのインスタンスIDからホスト名を導出するためだけに使用しており、今回のテーマにはあまり関係ない。\nAWS Systems Managerインベントリマネージャーの機能を使ってECのインベントリ情報をSに集約して管理しており、それをさらにAthenaで利用可能にしているのだが、機会があれば別記事で紹介するものとして、ここでは深く掘り下げない。\n\nbefにしか存在しない脆弱性は解消したものとして 、aftにしか存在しない脆弱性は新たに検出されたものとして を付加する。\n のリストと のリストはunion句で連結し、回のクエリでまとめて取得できるようにする。\n\n\n出力サンプル\n\n出力ファイルのイメージはこんなかんじ。\n\n\nsample csv\n diff   agentid   computername   rulespackagename   severity   numericseverity   id \n     i abcdefg   samplehost   Common Vulnerabilities and Exposures   High      CVE  xxxxx \n     i abcdefg   samplehost   Common Vulnerabilities and Exposures   High      CVE  yyyyy \n     i abcdefg   samplehost   Common Vulnerabilities and Exposures   High      CVE  zzzzz \n     i abcdefg   samplehost   Common Vulnerabilities and Exposures   High      CVE   \n     i abcdefg   samplehost   Common Vulnerabilities and Exposures   High      CVE   \n     i abcdefg   samplehost   Common Vulnerabilities and Exposures   High      CVE   \n\n\n\n前回のInspector実行結果と比較してsamplehostというサーバでは新規に件の脆弱性が検出されて、かわりに件の脆弱性が解消されたことがわかる。Excelで開くとまあまあいい感じのレポートである。\n\n以下、Lambdaファンクションの実装の内容について紹介する。\n\n\nLambdaのトリガー\n\n前回記事のLambdaファンクションの終了をトリガーに本処理をキックできると、Inspector終了→脆弱性リスト出力→前回と今回の差分出力と一連の流れがスムーズに実行される。\n特に連携すべきデータもないので、前回作成したLambdaファンクションの最後にSNSへPublishしているトピックを今回作成するLambdaファンクションがSubscribeすることでLambdaファンクションを数珠つなぎにするのが手っ取り早い。SNSのメッセージ内容は使用せず、単純な起動トリガーとしてのみ使用する。\n Lambdaの数珠つなぎはアンチパターン 今回は単純な処理なので、気にしない。 \n\n\nInspectorのテンプレートARNを取得\n\nAthenaのテーブルを検索するにはInspectorの実行に紐づくassessmentRunArnが必要。\nassessmentRunArnを検索するにはassessmentTemplateのArnが必要なので、\nInspectorテンプレート名をキーにしてテンプレートARNを取得する。\n 相変わらず例外処理はあまり考慮していないので、そこは適宜補完してください。 \n\n\nsample py\n Inspectorテンプレート名をもとにテンプレートARNを取得して返す\ndef get assessment template arn name  \n response   inspector list assessment templates \n filter  \n  namePattern   name\n  \n  \n return response  assessmentTemplateArns    \n\n\n\n\nassessmentRunArnsのリストを取得\n\nテンプレートARNをもとに、assessmentRunのリストを新しい順で取得する。\n一方で、list assessment runsは最大件のassessmentRunArnsを返す。\nInspectorの実行開始日時でフィルタすることで、件以上は取得対象が存在しないことを前提としている。\n\n\nsample py\n テンプレートARNをもとに、今日days日前の期間中のassessmentRunのリストを実施日の逆順にソートして返す\ndef get assessment run arn assessmentTemplateArn days   \n  基準日の範囲内のassessment runを取得 最大件。指定期間内に件以上のassessmentRunが存在することは考慮しない \n assessmentRunArns   inspector list assessment runs \n assessmentTemplateArns   \n assessmentTemplateArn\n   \n filter  \n  states    \n  COMPLETED \n   \n  startTimeRange    \n  beginDate   datetime datetime now     datetime timedelta days  \n  endDate   datetime datetime now  \n  \n   \n maxResults   \n    assessmentRunArns  \n if len assessmentRunArns    \n raise Exception  該当期間のassessmentRunArnsが見つかりません。  \n\n assessmentRuns   \n  ARNをもとに詳細を取得\n  describe assessment runsは件ずつしか処理できないため、件 上限値 ずつassessmentRunを取得\n for i in range      len assessmentRunArns           assessmentRunArnsの件数をで割り算して切り上げ\n response   inspector describe assessment runs \n assessmentRunArns  assessmentRunArns  i    min  i     len assessmentRunArns     \n  \n assessmentRuns extend response  assessmentRuns   \n  nameの逆順でソート  新しい順にソート \n sortedList   sorted assessmentRuns  key lambda x x  createdAt   reverse True \n return sortedList\n\n\n\n\nクエリ実行\n\n材料は揃ったのでクエリ実行してみよう。\n\n\nsample py\n Athenaのクエリを実行し、QueryExecutionIdを返す\ndef athena query execution bucketname dbname querystring  \n try \n response   athena start query execution \n QueryString querystring \n QueryExecutionContext  \n  Database   dbname\n  \n  \n ResultConfiguration  \n  OutputLocation    s       bucketname         os environ  AWS LAMBDA FUNCTION NAME      \n  \n  EncryptionConfiguration    \n  EncryptionOption    SSE S \n  \n  \n  \n queryexecutionid   response  QueryExecutionId  \n except Exception as e \n raise e\n return queryexecutionid\n\n\n\nAthenaのクエリ実行は非同期処理になる点に注意。\nクエリ実行すると、実行状況や結果をトレースするのに必要なQueryExecutionIdが得られる。これを使って処理完了までポーリングしてステータスをチェックする。\n\n\nクエリ実行結果取得\n\nAthenaのクエリ処理時間が長い場合、ひとつのLambdaファンクション内で完了を待ち続ける設計はよろしくないと思われるが、今回のクエリであれば所要時間はせいぜい秒くらいなので、同一Lambdaファンクション内でAthenaのクエリ実行結果をポーリングして待つことにする。\n\n\nsample py\n Athenaのクエリ実行結果をポーリングして待ち、出力ファイルののSパスを返す\ndef athena get query results queryexecutionid  \n  実行結果を秒おきに確認する\n x \n for x in range   \n sleep  \n response   athena get query execution \n QueryExecutionId queryexecutionid\n  \n status response  QueryExecution    Status    State  \n if status in   QUEUED   RUNNING   \n continue\n else \n break\n\n if status     SUCCEEDED  \n  Sのファイルパスを返す\n return response  QueryExecution    ResultConfiguration    OutputLocation  \n elif status     CANCELLED  \n raise Exception  sql execution CANCELLED error  \n elif status     FAILED  \n raise Exception  sql execution FAILED error  \n elif status in   QUEUED   RUNNING   \n raise Exception  sql execution NOT COMPLETE error  \n else \n raise Exception  sql execution UNKNOWN STATUS error  \n\n\n\n\nクエリ実行結果ファイルの文字コード変換\n\n最終的なcsvファイルをExcelで開けるようにするため、文字コード変換を行う。そのためにわざわざ一時的にSからLambda実行環境へ一時的にダウンロードする。以下の関数に与えるfilepathは先程のathena get query resultsの戻り値、つまりAthenaの実行結果が格納されているSのパス。\n\n\nsample py\n Sに出力されたAthenaのクエリ実行結果ファイルをローカルにダウンロードし、文字コード変換を行い、変換後ファイルのパスを返す\ndef get csv file filepath  \n  CSVファイル取得\n bucket    hogehoge \n key   filepath split bucket         \n local filepath     tmp     os path basename filepath  replace   csv    utf csv  \n try \n s   boto resource  s   config Config signature version  sv   \n s Bucket bucket  download file key  local filepath \n except Exception as e \n print e \n   UTF ファイルのパス\n utf csv path   local filepath\n   Shift JISファイルのパス\n shiftjis csv path   local filepath replace   utf csv    csv  \n  文字コードをShift JISに変換して保存\n fin   codecs open utf csv path   r    utf   \n fout utf   codecs open shiftjis csv path   w    cp    ignore  \n for row in fin \n fout utf write row \n fin close  \n fout utf close  \n return shiftjis csv path\n\n\n\nsignature version  sv でさり気なくSの署名バージョン対応を入れている。参考記事。\n\n\nCSVファイルをSへアップロードしてPresignedURLを生成\n\nこれ以降の処理は前回の記事Inspectorの実行結果をCSVファイルとして出力するとさほど変わらない。\n\n\nsample py\ndef s upload filepath basetime  \n  IAMロールを使用してSのPresignedURLを作成すると、IAMロールの有効期限 時間 が切れると無効になってしまうため\n  より長い有効期間のクレデンシャルを使用する。get parameters  の内容は前回記事参照。\n accesskey secretkey   get parameters  \n try \n s   boto resource \n  s  \n aws access key id accesskey \n aws secret access key secretkey \n config Config signature version  sv  \n  \n sc   boto client \n  s  \n aws access key id accesskey \n aws secret access key secretkey \n config Config signature version  sv  \n  \n  Sバケット名\n bucket   s Bucket  hogehoge  \n  Sファイル名\n s filepath    inspector diff     basetime strftime   Y  m  d       csv \n except Exception as e \n print e \n  Sアップロード\n try \n bucket upload file filepath  s filepath \n except Exception as e \n print e \n  Pre SignedURLの作成\n try \n url   sc generate presigned url \n ClientMethod    get object  \n Params    \n  Bucket    bucket name \n  Key    s filepath\n   \n ExpiresIn   expired hour   \n HttpMethod    GET \n  \n except Exception as e \n print e \n return url\n\n\n\nファイル名は脆弱性診断実施日ごとにユニーク。同一日に複数回脆弱性診断を実行している場合はファイルを上書きする仕様。\n\n\nSNSへPublish\n\n前回の記事と同じ。\n\n\nsample py\n SNSへ通知する\ndef sns publish topicarn url  \n  SNS title\n title    Inspectorの差分分析結果レポート \n  SNS本文\n message u   Inspector分析結果レポートを作成しました。\n  \nこちらのURLからダウンロード可能です。\nダウンロード期限は  時間です。\n\n    format \n url \n expired hour\n  \n  SNS Publish\n try \n response   sns publish \n TopicArn   topicarn \n Message   message \n Subject   title\n  \n except Exception as e \n print e \n raise e\n\n\n\n\nまとめ\n\n全体を繋げた処理はこんな感じ \n\n\nsample py\n      coding  utf     \nimport boto\nimport os\nimport datetime\nimport csv\nfrom time import sleep\nimport codecs\nfrom botocore client import Config\n\nsns   boto client  sns  \nssm   boto client  ssm  \ninspector   boto client  inspector  \nathena   boto client  athena  \n\n Presigned URLの有効期間 時間 \nexpired hour   \n\n Sアップロード用IAMユーザの認証情報を格納したパラメータストア名\naccesskey name   hogehoge accesskey \nsecretkey name   hogehoge secretkey \n\n Athenaデータベース名\ndbname    hoge athena dbname \n Inspector実行テンプレート名\ntemplatename    hoge template name \n\ndef lambda handler event  context  \n  テンプレート名に対応するTemplateArnを取得\n assessmentTemplateArn   get assessment template arn templatename \n  テンプレートARNに紐づく、過去の実行履歴 日前まで検索 \n assessmentRuns   get assessment run arn assessmentTemplateArn days  \n  直近の実行日時\n basetime   assessmentRuns    completedAt  \n\n  Inspectorの前回結果と今回結果の差分を取得するクエリ\n query   createQuery assessmentRuns \n  クエリ実行結果ファイル ローカル の取得\n filepath   get csv file athena get query results athena query execution dbname query   \n  Sへアップロードし、PresignedURLを生成\n url   s upload filepath basetime \n  SNSパブリッシュ\n topicarn    arn aws sns ap northeast  hogehoge \n sns publish topicarn url \n\n",
      "link": "https://qiita.com/onooooo/items/2c3d17010ce553f38a8d",
      "updated": "2019-06-26 10:39:25"
    }
  ],
  [
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【RDS for Oracle】ノーアーカイブログモードへ変更",
      "description": "\nはじめに\n\nAWSのユーザガイドを確認すると。。。\n\n\n大量のデータをロードしている間など、特定の状況では、自動バックアップを一時的に無効にすることをお勧めします。\n\n\nとあります。\nデフォルトはアーカイブログモードのため、DataPumpなどでインポートしたとき、アーカイブログが大量に生成されディスクを圧迫する恐れがあるということですね。\n\nオンプレのOracleでもインポート前にSQL gt  alter database noarchivelog で、ノーアーカイブログモードにしてました。\n\nしかし、RDSではSYSやSYSTEMユーザでログインできず、alter database文も実行できません。\n\nどうやら、RDS for Oracleのばあい、自動バックアップを無効化することで、アーカイブログモード デフォルト からノーアーカイブログモードに変更できるようです。\n\n自動バックアップを無効化した場合、以下の注意点があります。\n移行などの作業時に一時的に無効するのがよさそうです。\n\n\n重要\n自動バックアップは、無効にするとポイントインタイムリカバリも無効になるため、無効にしないことを強くお勧めします。DBインスタンスの自動バックアップを無効にすると、そのインスタンスの既存の自動バックアップがすべて削除されます。自動バックアップを無効にしてから再度有効にした場合、自動バックアップを再度有効にした時点からしか復元できません。\n\n\nまた、自動バックアップの無効化 ノーアーカイブログモードへの変更 時にはDBインスタンスが再起動するようです。\n\n\n自動バックアップの無効方法 ノーアーカイブログモード \n\n\nRDSコンソールで対象インスタンスを選択\n変更を選択すると、DBインスタンスの変更ページが表示される\n\nバックアップの保存期間で days  日 を選択\n次へを選択\n\n\n\n\n\n\nすぐに適用を選択\n確認ページで、DBインスタンスの変更を選択して変更を保存し、自動バックアップを無効にします\n\n\n\n\n\n対象インスタンスのステータスが変更中になります\n\n利用可能となったので変更が反映されます\n\nメンテナンスとバックアップからスナップショットを確認すると削除されています\n\nインスタンスに接続しノーアーカイブログモードになったか確認します。\n\n\nSQL gt  select name log mode from v database \n\nNAME LOG MODE\n                      \nORCL NOARCHIVELOG  ARCHIVELOGからNOARCHIVELOGに変わった\n\n\n\n自動バックアップの有効方法 アーカイブログモード \n\n\nRDSコンソールで対象インスタンスを選択\n\n変更を選択するとDBインスタンスの変更ページが表示されます。\nバックアップの保存期間で、ゼロ以外の正の値 日など を選択\n\n次へを選択\nすぐに適用を選択\n\n確認ページで、DBインスタンスの変更を選択して変更を保存し、自動バックアップを有効化\n\n\n\n対象インスタンスのステータスが変更中から利用可能になったら変更が反映される\nインスタンスに接続しアーカイブログモードになったか確認します。\n\n\n\n\nSQL gt  select name log mode from v database \n\nNAME LOG MODE\n                      \nORCL ARCHIVELOG\n\n\n\n自動バックアップを有効化した場合、スナップショットも自動で取得してくれるようです。\n\n\n\n\n\n手動でスナップショットを取る場合\n\n\nRDSコンソールで対象インスタンスを選択\nアクションで、スナップショットの取得を選択\n\nDBスナップショットの取得ウィンドウが表示されます\nスナップショット名ボックスにスナップショットの名前を入力します。\nスナップショットの取得を選択\n\n\n\n\n\n\nメンテナンスとバックアップからスナップショットが取得されたことが確認できます\n\n\n\n\n\nAWS CLIでの自動バックアップの無効 有効方法\n\n\nAWS CLIでの自動バックアップの無効\n\nmodify db instanceコマンドを使用して、バックアップ保持期間をに設定し  apply immediatelyを指定します。\n\n\nLinux、OS X、Unixの場合\n\n\naws rds modify db instance  \n   db instance identifier  lt インスタンス名 gt   \n   backup retention period   \n   apply immediately\n\n\n\nWindowsの場合\n\n\naws rds modify db instance  \n   db instance identifier  lt インスタンス名 gt   \n   backup retention period   \n   apply immediately\n\n\n変更が有効になるタイミングを確認するには、バックアップ保持期間の値がになりmydbinstanceのステータスがavailableになるまで、DBインスタンスに対してdescribe db instancesを呼び出します。\n\naws rds describe db instances   db instance identifier  lt インスタンス名 gt \n\n\n\nAWS CLIでの自動バックアップの有効\n\n自動バックアップをすぐに有効にするには、modify db instanceコマンドを使用します。\n\n 例 バックアップ保持期間を日に設定することで、自動バックアップを有効にします。\n\n\nLinux、OS X、Unixの場合\n\n\naws rds modify db instance  \n   db instance identifier  lt インスタンス名 gt   \n   backup retention period   \n   apply immediately\n\n\n\nWindowsの場合 \n\n\naws rds modify db instance  \n   db instance identifier  lt インスタンス名 gt   \n   backup retention period   \n   apply immediately\n",
      "link": "https://qiita.com/ghogho-seki/items/2f8799868449d569079c",
      "updated": "2019-07-08 04:34:34"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【RDS for Oracle】自動統計情報収集 自動オプティマイザ統計収集 変更方法",
      "description": "\n\nOracleではつの自動メンテナンス・タスクがあります\n\n\n自動統計情報収集 自動オプティマイザ統計収集 \n自動セグメント・アドバイザ\n自動SQLチューニング・アドバイザ\n\n\nこれらのタスクはメンテナンス・ウィンドウ メンテナンス可能として設定した時間帯 で実行されます。\n自動化メンテナンス・タスクの有効 無効の確認\n\n\nSQL gt  col CLIENT NAME for a\nSQL gt  select client name  status from dba autotask client \n\nCLIENT NAME STATUS\n                                                 \nauto optimizer stats collection ENABLED\nauto space advisor ENABLED\nsql tuning advisor ENABLED\n\n\n※注意\n自動SQLチューニング・アドバイザを使用するにはEnterprise EditionのオプションのOracle Tuning Packオプションが必要です。このオプションがない場合は自動SQLチューニング・アドバイザを無効化する必要があります。\n\n\n\n\n自動メンテナンス・タスク\nCLIENT NAME\n\n\n\n\n自動統計情報収集\nauto optimizer stats collection\n\n\n自動セグメント・アドバイザ\nauto space advisor\n\n\n自動SQLチューニング・アドバイザ\nsql tuning advisor\n\n\n\n\n\n自動メンテナンス・タスクの無効化の方法\n\n\n  例 自動SQLチューニング・アドバイザの無効\n\nBEGIN\n dbms auto task admin disable \n client name   gt   sql tuning advisor  \n operation   gt  NULL \n window name   gt  NULL  \nEND \n \n\n\n\n自動SQLチューニング・アドバイザの確認\n\n\nSQL gt  select client name  status from dba autotask client \n\nCLIENT NAME STATUS\n                                                 \nauto optimizer stats collection ENABLED\nauto space advisor ENABLED\nsql tuning advisor DISABLED  無効化されてる\n\n\n\n自動メンテナンス・タスクの有効化の方法\n\n\n  例 自動SQLチューニング・アドバイザの有効\n\nBEGIN\n dbms auto task admin enable \n client name   gt   sql tuning advisor  \n operation   gt  NULL \n window name   gt  NULL  \nEND \n \n\n\n\n自動SQLチューニング・アドバイザの確認\n\n\nSQL gt  select client name  status from dba autotask client \n\nCLIENT NAME STATUS\n                                                 \nauto optimizer stats collection ENABLED\nauto space advisor ENABLED\nsql tuning advisor ENABLEDD  有効化に戻った\n\n\n\nメンテナンス・ウィンドウのスケジュールを確認\nデフォルトの実行時間\n\n\n\n\n\n曜日\n実行時間\n\n\n\n\n月金\n 翌日  時間 \n\n\n土日\n 翌日  時間 \n\n\n\n\nSQL gt  col REPEAT INTERVAL for a\nSQL gt  col DURATION for a\nSQL gt  select WINDOW NAME  REPEAT INTERVAL  DURATION  ENABLED from DBA SCHEDULER WINDOWS \n\nWINDOW NAME REPEAT INTERVAL DURATION ENABL\n                                                                                                                                \nWEEKEND WINDOW freq daily byday SAT byhour  byminute  bysecond       FALSE\nWEEKNIGHT WINDOW freq daily byday MON TUE WED THU FRI byhour  byminute   bysecond       FALSE\nSUNDAY WINDOW freq daily byday SUN byhour  byminute   bysecond       TRUE\nSATURDAY WINDOW freq daily byday SAT byhour  byminute   bysecond       TRUE\nFRIDAY WINDOW freq daily byday FRI byhour  byminute   bysecond       TRUE\nTHURSDAY WINDOW freq daily byday THU byhour  byminute   bysecond       TRUE\nWEDNESDAY WINDOW freq daily byday WED byhour  byminute   bysecond       TRUE\nTUESDAY WINDOW freq daily byday TUE byhour  byminute   bysecond       TRUE\nMONDAY WINDOW freq daily byday MON byhour  byminute   bysecond       TRUE\n\n rows selected \n\n\n\nメンテナンス・ウィンドウの実行予定時間を確認\n\n\nSQL gt  set pages  line \nSQL gt  col WINDOW NAME for a\nSQL gt  col WINDOW NEXT TIME for a\nSQL gt  select WINDOW NAME to char WINDOW NEXT TIME  yyyy mm dd hh mm ss   WINDOW ACTIVE AUTOTASK STATUS OPTIMIZER STATS SEGMENT ADVISOR SQL TUNE ADVISOR HEALTH MONITOR\n from DBA AUTOTASK WINDOW CLIENTS order by   \n\nWINDOW NAME TO CHAR WINDOW NEXT WINDO AUTOTASK OPTIMIZE SEGMENT  SQL TUNE HEALTH M\n                                                                                           \nFRIDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\nMONDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\nSATURDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\nSUNDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\nTHURSDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\nTUESDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\nWEDNESDAY WINDOW       FALSE ENABLED ENABLED ENABLED ENABLED DISABLED\n\n rows selected \n\n\n\nメンテナンス・ウィンドウの開始時刻を変更\n\n\n    例 日曜のメンテナンス・ウィンドウの開始時間を に変更\n\nBEGIN\n DBMS SCHEDULER SET ATTRIBUTE \n NAME   gt  SYS SUNDAY WINDOW \n  ATTRIBUTE   gt  REPEAT INTERVAL \n  VALUE   gt  freq daily byday SUN byhour  byminute   bysecond  \n   \nEND \n \n\n\n\n実行時間を変更\n\n\n 例 日曜のメンテナンス・ウィンドウの実行時間を時間に変更\n\n\n\n\nBEGIN\n DBMS SCHEDULER SET ATTRIBUTE \n NAME   gt  SYS SUNDAY WINDOW \n  ATTRIBUTE   gt  DURATION \n  VALUE   gt  NUMTODSINTERVAL    HOUR  \n   \nEND \n \n\n\n\n確認\n\n\nSQL gt  select WINDOW NAME  REPEAT INTERVAL  DURATION  ENABLED from DBA SCHEDULER WINDOWS \n\nWINDOW NAME REPEAT INTERVAL DURATION ENABL\n                                                                                                                                \nWEEKEND WINDOW freq daily byday SAT byhour  byminute  bysecond       FALSE\nWEEKNIGHT WINDOW freq daily byday MON TUE WED THU FRI byhour  byminute   bysecond       FALSE\nSUNDAY WINDOW freq daily byday SUN byhour  byminute   bysecond       TRUE     時間 に変更\nSATURDAY WINDOW freq daily byday SAT byhour  byminute   bysecond       TRUE\nFRIDAY WINDOW freq daily byday FRI byhour  byminute   bysecond       TRUE\nTHURSDAY WINDOW freq daily byday THU byhour  byminute   bysecond       TRUE\nWEDNESDAY WINDOW freq daily byday WED byhour  byminute   bysecond       TRUE\nTUESDAY WINDOW freq daily byday TUE byhour  byminute   bysecond       TRUE\nMONDAY WINDOW freq daily byday MON byhour  byminute   bysecond       TRUE\n\n rows selected \n\n\n\n参考\n\nDBMS SCHEDULERジョブの変更",
      "link": "https://qiita.com/ghogho-seki/items/51231a410ec27242a969",
      "updated": "2019-07-08 02:22:48"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Amazon SageMakerの推論パイプラインで、独自コンテナを組み合わせる方法",
      "description": "\n概要\n\nAmazon SageMakerで、独自コンテナを含む推論パイプラインエンドポイントを構築する方法について書きます\n\nSageMakerで提供されるコンテナで所望の前処理が実現できず、独自コンテナによる前処理をしたい場合、カスタム前処理コンテナはどのようにつくるか、どのように組み込むか調べた際のメモです\n\n\n組み込みアルゴリズムと推論パイプライン\n\nAmazon SageMakerでは組み込みアルゴリズムが提供されており、解きたい問題にフィットするものがあれば、MLアルゴリズムに関するコードを書かずに手軽にMLモデルを作れるようになっています\nただし組み込みアルゴリズムに引き渡す入力形式はアルゴリズムごとに決まっており、基本的には、保有するデータを整形する前処理が必要です\n\nこうした前処理は学習 推論時で同じ処理なため、共通化されていることが望ましいということで、SageMakerでは前処理アルゴリズムをコンテナ化して学習 推論時に実行させる仕組みが提供されています\n\n前処理コンテナはSageMakerでのコンテナの挙動を踏襲しており、fitでインスタンス上に前処理コンテナを展開してデータ整形 Sバケットへ出力したり、deployでインスタンス上に前処理コンテナを展開してリクエストデータ整形 次処理へ渡す、といった挙動をします\n\nとくに推論処理では、エンドポイントへ届いたリクエストを前処理して後続の推論アルゴリズムへ渡すために、「前処理 推論処理  後処理 」等をセットにした推論パイプラインエンドポイントが構築できるようになっています\n\n\nAmazon SageMaker組み込みアルゴリズムを使用する\nAmazon SageMaker推論パイプライン\nAmazon SageMakerトレーニングと推論でデータ処理コードの一貫性を確保する\n\n\n\n前処理で独自コンテナが必要になるケース\n\n推論パイプラインのサンプルには、SageMakerでMLフレームワークを使うときと同様sagemaker sklearn estimator SKLearnなどEstimatorを利用して、任意のスクリプトを所定のコンテナ環境下で実行させる例が載っています\n提供されているコンテナで所望の処理が実現できれば、カスタムコンテナは不要です\n\nfrom sagemaker sklearn estimator import SKLearn\n\nscript path    sklearn abalone featurizer py \n\nsklearn preprocessor   SKLearn \n entry point script path \n role role \n train instance type  ml c xlarge  \n sagemaker session sagemaker session \n\n\n\nしかし提供されるコンテナにないライブラリや、インストールされていないミドルウェアを使いたい場合には、独自の環境をコンテナ化して実行させる必要があります\n\nたとえば自然言語を扱う課題でMeCabで形態素解析したいといった場合には、MeCabが動作してかつAmazon SageMakerの仕様に従ったコンテナが必要になります\n\n\n前処理コンテナのつくりかた\n\nAmazon SageMakerで提供されているコンテナ群はGitHubで公開されているので、ライブラリを追加する程度であれば、既存コンテナを拡張するのが楽かもしれません\n用意されているコンテナをFROM文で呼び出した上に、追加のインストールやセットアップを施します\n\n\naws sagemaker containers\nPyTorchコンテナの拡張\n\n\nそうした提供済みコンテナで都合が悪い ミドルウェアのバージョンが合わないとか、既存の資産を使いたいとか 場合には独自コンテナを作ります\n\n\nscikit bring your own\nAmazon SageMakerで独自のアルゴリズムを使用する\n\n\n\n前処理コンテナの要件\n\n基本的な挙動はSageMakerの独自のトレーニングイメージの仕様にあわせる必要があります\nfitやdeployといった命令をうけて所定の処理がRUNされる必要があるため、scikit bring your ownなどをベースに、枠組みそのままで中身の処理を書き換えるのがよさそうです\nまた、パイプラインエンドポイントのための独自の設定がいくつか必要です\n\n\n構成要素\n\n\ntrain トレーニング fit 時に呼び出される処理\nserve エンドポイント構築 deploy 時に呼び出される処理\npredictor py 推論エンドポイントで呼び出される処理\n\n\nその他の要件\n\n\nDockerfileに次のコードが必要\n\n\n\nLABEL com amazonaws sagemaker capabilities accept bind to port true \n\n\nエンドポイントのリクエスト受け付けポートが次の環境変数に依存する\n\n\nSAGEMAKER BIND TO PORT\n\n\nDockerコンテナリポジトリに、次のポリシー付与が必要\n\n\nイメージをプルするためのAmazon SageMakerのアクセス許可\n\n\n\n\n\n\n\n前処理コンテナで実装する内容\n\nscikit bring your own等をベースに、下記の機能を実装します\n\n\ntrain\n\n生データを受け取って整形加工し、MLモデルトレーニングに使用する訓練データをつくり、Sへ配置します\n訓練データ生成の際に作成される「辞書データ」や「ベクトライザ」は推論時にも同じものを使うため、推論エンドポイントへ引き継ぐ必要がありますが、SageMakerのアルゴリズムで opt ml model以下を共有する仕組みにのせて、推論エンドポイントの同一ディレクトリへ展開させるようにします\n\n\n\nEstimator fitで呼び出される\n\n\nAmazon SageMakerがトレーニングイメージを実行する方法\n\n\n指定されたタイプのインスタンスに、inputs   raw   s     で引き渡したSパスのデータが opt ml input data  channel 以下へ自動的に展開される\n\n opt ml input data  channel からデータを参照して一括整形する\nMLトレーニングで使用する、加工整形した訓練データをS上の任意のパスへアップロードする\n推論エンドポイントでデータの整形処理に必要になるモデルデータ 辞書やベクトライザなど を opt ml modelへ出力する\n\n\n\njoblib dumpなど\n\n\n\n opt ml model以下のファイル群がtar gz形式でアーカイブされてoutput pathで指定されたSパスへ自動的にアップロードされる\n\n\n\nserve\n\ntrain時に生成したモデルファイルをインスタンス上に配置し、 invocationsのリクエストを受け付けるwebサーバを起動します\n\n\n\nEstimator deployで呼び出される\n\n\nAmazon SageMakerが推論イメージを実行する方法\n\n\n\noutput pathに指定されたSパスから、Training時に生成済みのモデル群をインスタンスの opt ml model以下へ自動的に展開する\n\n\nAmazon SageMakerがモデルアーティファクトをロードする方法\n\n\n\n invocationsと pingに応答するウェブサーバーを起動する\n\n\nパイプラインモデルのエンドポイントは、リクエストを受け付けるポートを環境変数SAGEMAKER BIND TO PORTで指定されるため、webサーバの設定でポートを環境変数を参照するようにする 環境変数がなければを使う \n\n\n\n\n\npredict\n\nserveで起動したエンドポイントで、推論を行いたい生データを所定の書式で受け取り、後続の推論アルゴリズムが要求する書式に整形して返します\n\n\n\nserveの際に配置したモデルファイル群を opt ml model以下から取得し、コード上へ読み出す\n\n\n\njoblib loadなど\n\n\n推論リクエストに含まれる生データを取得し、文字列の正規化を行う\n正規化したデータについて、先に取得したベクトライザ等を使用して推論アルゴリズムが要求する形式のデータへ変換する\n変換した形式のデータをレスポンスとして返す\n\n\n\nコンテナのビルド、ECRへのPUSH\n\n上記実装したコード群をコンテナ化し、ECRのリポジトリへpushします\n\npushしたECRのリポジトリには、SageMakerのパイプラインモデルからの呼び出しを許可するポリシーを付与する必要があります 推論パイプラインのAmazon ECRアクセス許可のトラブルシューティング \n\nECRコンソールからRepositories  gt 該当リポジトリリンク gt  Permissionsと進んで、アクセス許可ポリシーを下記JSONの通り追加します\n\n\n \n  Version        \n  Statement    \n  \n  Sid    allowSageMakerToPull  \n  Effect    Allow  \n  Principal    \n  Service    sagemaker amazonaws com \n   \n  Action    \n  ecr GetDownloadUrlForLayer  \n  ecr BatchGetImage  \n  ecr BatchCheckLayerAvailability \n  \n  \n  \n \n\n\nここまででパイプライン用の前処理コンテナができました\n\n\nMLプロセスへの組み込み\n\n組み込みMLアルゴリズムと、独自コンテナによる前処理を組み合わせ、MLフローを実行します\n\n\n前処理 訓練データの準備 \n\n生データを用意してSへアップロードし、以下のようなコードによって、訓練データへ変換します\n 前処理で生成したベクトライザ等の生成物をSへ格納します\n\n\nimport sagemaker\nsess   sagemaker Session  \nrole   sagemaker get execution role  \n\npreprocess image name    preprocess ecr container image \npreprocess job name     preprocess id  \nrawdata input path    s rawdata path \ntrain dataset path    s preprocessed data path \n\npreprocessor   sagemaker estimator Estimator \n image name   preprocess image name \n role   role \n train instance count   preprocess instance count  \n train instance type   preprocess instance type \n train volume size   preprocess volume size \n output path   train dataset path \n sagemaker session   sess  \npreprocessor fit \n inputs   rawdata   rawdata input path  \n job name   preprocess job name  \n\n\n\n\n学習\n\n前処理で訓練に必要なデータはすべてS上の所定の場所へアップロードされている状態です\n下記のコードによって、組み込みアルゴリズムによるトレーニングを実行します\n\nimport sagemaker\n\nalgorithm name    ml hoge \nml container name   get image uri boto Session   region name  algorithm name \nml model path    s ml model path \nml training job name     algorithm name   preprocess id   timestamp  \n\nml   sagemaker estimator Estimator \n image name   ml container name \n role   role  \n sagemaker session   sess \n train instance count   train instance count  \n train instance type   train instance type \n output path   ml model path  \nml fit \n inputs  \n  train   train dataset path train  \n  validation   train dataset path valid  \n  auxiliary   train dataset path aux  \n  test   train dataset path test   \n job name   ml training job name  \n\n\n\n\nエンドポイント構築\n\n前処理で生成したベクトライザ等を利用するデータ前処理コンテナと、組み込みアルゴリズムの学習済モデルを参照する推論コンテナをセットにして、推論パイプラインエンドポイントを構築します\n  sagemaker pipeline PipelineModelのmodels変数へ渡した配列のモデル順に、リクエストに応答するエンドポイントが構築されます \n\n\nimport sagemaker\n\npreprocessor   sagemaker estimator Estimator attach preprocess job name \npreprocess model   preprocessor create model  \nml   sagemaker estimator Estimator attach ml training job name \nml model   ml create model  \n\npipeline endpoint name     algorithm name  pipeline endpoint  version  \npipeline model   sagemaker pipeline PipelineModel \n name   pipeline model name \n role   role \n models     preprocess model  ml model    \npipeline model deploy \n initial instance count   pipeline instance count  \n instance type   pipeline instance type  \n endpoint name   pipeline endpoint name  \n\n\nここまでで、独自コンテナによる前処理を組み込んだ推論エンドポイントが起動します\n\n\n推論リクエストへの応答\n\n生データを所定の形式で引き渡せば、前処理コンテナで整形  gt 組み込みアルゴリズムの推論コンテナで推論した結果が返ります\n\nimport sagemaker \nfrom sagemaker content types import CONTENT TYPE JSON\n\nml predictor   sagemaker predictor RealTimePredictor \n endpoint   pipeline endpoint name \n serializer   sagemaker predictor json serializer \n deserializer  sagemaker predictor json deserializer \n content type  CONTENT TYPE JSON \n accept   CONTENT TYPE JSON  \n\npayload     target   日本語文字列  \npprint ml predictor predict payload  \n\n\n\nトラブルシューティング\n\n\nエンドポイント構築時にping health checkに失敗する\n\nエンドポイント構築時にpingに失敗して構築できないというエラーが発生する場合\n\nValueError  Error hosting endpoint pipeline endpoint vXX  Failed Reason  The container  for production variant AllTraffic did not pass the ping health check  Please check CloudWatch logs for this endpoint \n\n\n独自コンテナで起動させたWebサーバが、環境変数SAGEMAKER BIND TO PORTポートでリクエストを受け付けていない可能性があります CloudWatchLogsにも何の情報も出ず途方に暮れるのですが、落ち着いて \nWebサーバの設定を見直し、SAGEMAKER BIND TO PORT環境変数が存在する場合には、環境変数の指定するポートで受け付けるように設定します\n\n\nECRコンテナリポジトリに必要な権限が付与されていない\n\nパイプラインエンドポイントを構築しようとする際、権限が足りないといったエラーが出る場合\n\nValueError  Error hosting endpoint pipeline endpoint vXX  Failed Reason  The repository of your image  dkr ecr xxxxx amazonaws com XXXXXXXXXXXXXXXXXXXX does not grant ecr GetDownloadUrlForLayer  ecr BatchGetImage  ecr BatchCheckLayerAvailability permission to sagemaker amazonaws com service principal \n\n\n推論パイプラインエンドポイントを構築する際には、パイプラインに組み込む独自コンテナのリポジトリにSageMakerからの読み出しを許可するポリシー付与が必要です\n推論パイプラインのAmazon ECRアクセス許可を参考に、ECRコンソールから必要なポリシーを付与します\n\n \n  Version        \n  Statement    \n  \n  Sid    allowSageMakerToPull  \n  Effect    Allow  \n  Principal    \n  Service    sagemaker amazonaws com \n   \n  Action    \n  ecr GetDownloadUrlForLayer  \n  ecr BatchGetImage  \n  ecr BatchCheckLayerAvailability \n  \n  \n  \n \n\n\n\nその他\n\n推論パイプラインのトラブルシューティング参照\n\n\nまとめ\n\n独自コンテナを含む推論パイプラインエンドポイントを構築する際、次の点に注意してください\n\n\n基本的な挙動はSageMakerのカスタムコンテナと同じ\n\n\nAmazon SageMakerがトレーニングイメージを実行する方法\nAmazon SageMakerホスティングサービスでの独自の推論コードの使用\n\n\nエンドポイントのリクエスト受け付けポートは次の環境変数に依存する\n\n\nSAGEMAKER BIND TO PORT\n推論パイプラインでリアルタイム予測を実行\n\n\nDockerコンテナリポジトリに、次のポリシー付与が必要\n\n\nイメージをプルするためのAmazon SageMakerのアクセス許可\n\n\n\n\n日本語処理など独自コンテナを必要とするケースで、ぜひ参考にしていただければと思います\n\n\n参考資料\n\n\nAmazon SageMakerで独自の学習 推論用コンテナイメージを作ってみる\n推論パイプラインでリアルタイム予測を実行\n\n\n\nパイプラインのコンテナは、 ではなく  SAGEMAKER BIND TO PORT環境変数で指定されたポートでリッスンします。\n\nコンテナがこの要件に準拠していることを示すには、次のコマンドを使用してDockerfileにラベルを追加します\nLABEL com amazonaws sagemaker capabilities accept bind to port true\n\n\n\nAmazon SageMakerがトレーニングイメージを実行する方法\nAmazon SageMakerホスティングサービスでの独自の推論コードの使用\npipeline PipelineModel\n推論パイプラインのトラブルシューティング\n\n\n\nAmazon SageMaker組み込みアルゴリズムを含むパイプラインでカスタムDockerイメージを使用する場合は、Amazon ECRポリシーが必要です。ポリシーは、イメージをプルするためにAmazon SageMakerのアクセス許可をAmazon ECRレポジトリに与えます。\n",
      "link": "https://qiita.com/yaiwase/items/79f99d2c38ed66729a47",
      "updated": "2019-07-08 01:34:25"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Packer  Ansible  Terraformで作るBlue Greenデプロイメント AWS ",
      "description": "\n概要\n\nAWS環境で、デプロイメントのフローを自動化する仕事を任されたので、成果物をここにメモしておきます。\n\nデプロイをするシステムは、以下のように構成されています。\n\n\nBlue Greenの環境を用意し、普段は片方のみを使用しています。使用していないほうの環境にデプロイをして、ELBの振り分け先を変更することで、ダウンタイムのないデプロイメントを目指します。また、packerやAnsible  Terraformなどのツールを使うことで、このフローを自動化していきます。\n\n尚、こちらの記事やこちらの記事などを参考にして作成しました。\n\n\n作業手順\n\n\nアプリケーションや必要なミドルウェアなどをインストールした、ゴールデンAMIを作成する\n\nゴールデンAMIを参照する起動設定を新規作成し、デプロイ先のAutoScaling Groupにそれを読み込ませる\n\nElastic Load BalancerのListenerが転送するTargetGroupを変更し、本番環境をBlueからGreenに もしくはGreenからBlueに 切り替える\n\n\n\nAMIの作成\n\nPackerとAnsibleを使って、ゴールデンAMIを作成します。AMIの基本的な設定はPackerで、ツールのインストール等の構成管理はAnsibleで、それぞれ行います。\n\nまずは、Packerの設定です。\n\n\npacker json\n \n  variables   \n  version       デプロイのたびに変わる値は、変数としておきます\n    \n  builders      AMIを作成するために一時的に立てられるECインスタンスについて定義 \n   \n   \n  type   amazon ebs   \n  ami name    test AMI v  user  version     \n  region    ap northeast    \n  source ami    ami xxxxxxxxxxx      基とするAMI\n  instance type    t micro  \n  ssh username    centos  \n  tags   \n  version     user  version        起動設定を作成するときに、見つけやすいようにタグをつけておく\n   \n  force deregister   true \n  force delete snapshot   true \n  security group ids    sg xxxxxxx   sg xxxxxxx   \n  iam instance profile    xxxxxxx  \n  subnet id   subnet xxxxxxxx \n  \n   \n  provisioners      AMI作成用のインスタンスの中で行いたい処理を定義\n  \n  \n  type   shell  \n  inline        Ansibleを動かすためのツールをインストール\n  sudo  E yum  y update  \n  sudo  E yum  y install epel release  \n  sudo  E yum  y install python pip  \n  sudo  E yum  y install ansible \n  \n   \n  \n  type   ansible local      Ansibleの起動\n  playbook file   setup yml \n  \n  \n \n\n\n\n\n上のファイルの中から呼ばれている、Ansibleの設定ファイル playbook の設定は、以下の通りです。\n尚、以下ではS上のmavenリポジトリから成果物をとってくるようにしてあります。\n\n\nsetup yml\n  hosts  all\n become  yes\n vars \n proxy env   すべてに共通するプロキシ設定は、変数化しておく\n https proxy  xxxxxxxxx\n http PROXY  xxxxxxxxx\n HTTPS PROXY  xxxxxxxxx\n HTTP PROXY  xxxxxxxxx\n no proxy  xxxxxxxxx\n tasks \n   environment     proxy env   \n name   install boto  required by aws s     botoのインストール\n pip \n name  boto\n   pip \n name  lxml\n name   install lxml    Ansibleのsモジュールを動かすのに必要\n environment     proxy env   \n   maven artifact    S上のmavenリポジトリから成果物を取得\n repository url   s   xxxxxxxx \n group id  jp co xx xx\n artifact id  xxxxxx\n version   \n extension  tgz\n dest   tmp xxxxxx tgz\n\n\n\n\npackerコマンドで上記のpackerおよびAnsibleを実行すると、AMIが作成されるはずです\n\n\nASGの設定変更\n\nTerraformを用いて、まず上で作成したAMIを読み込む起動設定を作成し、そしてデプロイ先のASG 使用中でない環境のASG にその起動設定を読み込ませるようにします。\n\n\ndeploy tf\nprovider  aws   \n region    ap northeast  \n \n\nvariable  target version       デプロイのたびに値が変わるものは、変数化しておく\n\ndata  aws ami   centos       packerとAnsibleで作成したAMIを取得\n most recent   true\n owners     xxx  \n filter     タグで検索\n name    version \n values       var target version   \n  \n \n\nresource  aws launch configuration   test      起動設定を新規作成\n name    config test v  var target version  \n image id      data aws ami centos id     作成したAMIを指定\n instance type    t micro \n enable monitoring   false\n security groups     sg xxxxxxx    sg xxxxxxxxx   sg xxxxxxx  \n key name    xxxxxxxxx \n iam instance profile    xxxxxxxxx \n root block device  \n delete on termination   true\n iops   \n volume size   \n volume type    gp \n  \n \n\nresource  aws autoscaling group   test resource      デプロイ先のASGが上の起動設定を読みこむよう、設定変更\n name    asg test \n launch configuration      aws launch configuration test name     上で作成した起動設定を指定\n min size   \n max size   \n desired capacity   \n target group arns     xxxxx   xxxxxx  \n vpc zone identifier     xxxxxxx   xxxxxxxx  \n lifecycle  \n create before destroy   true\n  \n \n\n\n\n\nterraformコマンドで上の設定を反映させると、使用中でない環境のASGに設定が反映されます。\n\n\nBlue Greenの切り替え\n\n最後に、ELBの振り分けの設定を変えます。適当なスクリプト言語でシェルを書くのですが、今回はCentOSにデフォルトで入っているPythonの系で書きます 今時Pythonかよ、って感じですが 。\n\n処理の手順は、以下の通りです\n  ASGに紐づくECのインスタンスのリストを取得\n  ECがすべて立ち上がって、テストをパスしたか確認する\n  ECがすべて立ち上がっていれば、ELBのListenerのリクエストの転送先のTargetGroupを変更し、Blue Greenを切り替える\n\n以下がコードです。\n\n\nswitch elb sh\n    bin python\n      coding  utf     \n\nfrom time import sleep\nimport json\nimport subprocess\n\n  ASGに紐づくECインスタンスのリストの取得\nGET INSTANCE LIST      aws ec describe instances   filters  Name tag version Values  s   \n  ECインスタンスがヘルシーな状態であるか確認\nGET INSTANCE STATUS    aws ec describe instance status   instance id  s \n  ELBの切り替え\nSWITCH ELB LISTENER    aws elbv modify rule   rule arn  s   actions Type forward TargetGroupArn  s \n\n  ECのリストを取得し、ヘルスチェック\ndef get ec health version  \n instance ids     \n  インスタンスのリスト取得\n instance list   subprocess check output GET INSTANCE LIST   version shell True \n instance dict   json loads instance list \n for instance in instance dict  Reservations   \n  停止中のインスタンスは除く\n if instance  Instances      State    Code       \n instance ids append instance  Instances      InstanceId    \n for i in range   \n  一つずつインスタンスをヘルスチェックし、一つでもダメであれば、秒後に再チェック\n result   check ec status instance ids \n if result    True \n return True\n else \n sleep  \n return False\n\n  ECのヘルスチェック\ndef check ec status instance ids  \n for instance id in instance ids \n result   commands getoutput GET INSTANCE STATUS   instance id \n status dict   json loads result  \n  一つでもヘルスチェックに失敗すれば、Falseを返す\n if status dict  InstanceStatuses      SystemStatus    Details      Status       passed  or status dict  InstanceStatuses      InstanceStatus    Details      Status       passed  \n return False\n return True\n\n  ELBの切り替え\ndef switch elb   \n ELB rules     \n try \n   ELBのListenerのRuleのarnをjson形式で羅列しておくファイル\n file   open        ELB rules arns json   r  \n ELB rules   json load file \n except \n print  ELBのルールの情報が読み込めませんでした  \n return\n target group arns     \n try \n  デプロイ先の環境の、TargetGroupのarnをjson形式で羅列しておくファイル\n file   open    target group arns json   r  \n target group arns   json load file \n except \n print  ターゲットグループの情報が読み込めませんでした  \n return\n for key in ELB rules \n result   subprocess call SWITCH ELB LISTENER    ELB rules key  target group arns key   shell True \n if result     \n print  切り替え成功  \n return \n\n メインの関数\ndef main   \n version   input  デプロイのバージョンを指定してください  \n ec health   get ec health version \n if ec health    True \n switch elb status   switch elb  \n if switch elb status     \n print  ELBの切り替えがうまくいきませんでした  \n else \n print  ELBの切り替えに成功しました  \n else \n print  ECのヘルスチェックが失敗したため、切り替えを行いません  \n\nif   name         main    \n main  \n\n\n",
      "link": "https://qiita.com/mimur/items/9bd7450f16c790774157",
      "updated": "2019-07-08 00:41:07"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS CloudFormationのLambda backedカスタムリソースでリソースの更新・削除をする方法",
      "description": "AWS CloudFormationのLambda backedカスタムリソースを利用するとAWS CloudFormationで管理できないリソースも管理することができますが、Lambda backedで作成したリソースの更新・削除するのにリソースのIDをどうやって取り回そうか悩みました。\n\n下記は解決策の案となりますが、他に良い方法があれば教えてほしいです \n\n\n前提\n\n\nAWSアカウントがある\nAWS CLIが利用できる\nAWS Lambda、CloudFormationの作成権限がある\n\n\n\nCloudFormationのテンプレート作成\n\n gt  mkdir 任意のディレクトリ\n gt  cd 任意のディレクトリ\n gt  touch cfn template yaml\n\n\nLambda backedカスタムリソースを利用して何かしらのリソースを作成・更新・削除するテンプレートとなります。\nポイントとしてはCreateResourceのひとつで完結できたら良かったのですが、CreateResourceで作成したリソースのIDを自前で取り回せなかったので、更新と削除を別リソースUpdateResourceで行うようにしました。うーん、めんどうです\n\n\ncfn template yaml\nResources \n CreateResource \n Type  Custom  CustomResource\n Properties \n ServiceToken   GetAtt CreateResourceFunction Arn\n\n UpdateResource \n Type  Custom  CustomResource\n Properties \n ServiceToken   GetAtt UpdateResourceFunction Arn\n ResourceId   GetAtt CreateResource Id\n\n CreateResourceFunction \n Type  AWS  Lambda  Function\n Properties \n Handler  index handler\n Role   GetAtt FunctionExecutionRole Arn\n Code \n ZipFile   Sub  \n import cfnresponse\n def handler event  context  \n if event  RequestType       Create  \n  なんかリソース作成\n response     Id    hoge  \n print  create resources     response  Id   \n cfnresponse send event  context  cfnresponse SUCCESS  response \n return\n\n  他のRequestTypeは無視\n cfnresponse send event  context  cfnresponse SUCCESS     \n Runtime  python\n\n UpdateResourceFunction \n Type  AWS  Lambda  Function\n Properties \n Handler  index handler\n Role   GetAtt FunctionExecutionRole Arn\n Code \n ZipFile   Sub  \n import cfnresponse\n def handler event  context  \n Id   event  ResourceProperties    ResourceId  \n if event  RequestType       Update  \n  なんかリソース更新\n print  update resources     Id \n cfnresponse send event  context  cfnresponse SUCCESS     \n return\n\n if event  RequestType       Delete  \n  なんかリソース削除\n print  delete resources     Id \n cfnresponse send event  context  cfnresponse SUCCESS     \n return\n\n  他のRequestTypeは無視\n cfnresponse send event  context  cfnresponse SUCCESS     \n Runtime  python\n\n FunctionExecutionRole \n Type  AWS  IAM  Role\n Properties \n AssumeRolePolicyDocument \n Version      \n Statement \n   Effect  Allow\n Principal \n Service \n   lambda amazonaws com\n Action \n   sts AssumeRole\n Path     \n Policies \n   PolicyName  root\n PolicyDocument \n Version      \n Statement \n   Effect  Allow\n Action \n   logs CreateLogGroup\n   logs CreateLogStream\n   logs PutLogEvents\n Resource   arn aws logs       \n\n\n\n\n動作確認\n\n\nスタック作成\n\n gt  aws cloudformation create stack  \n   stack name cfn lambda backed test  \n   template body file   cfn template yaml  \n   capabilities CAPABILITY IAM\n\n \n  StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe \n \n\n\nスタック作成して各リソースが作成できたらLambda関数のログを確認します。\n各リソースと関数のPhysicalResourceIdをパラメータにaws logs get log eventsコマンドで取得します。\n\n gt  aws cloudformation list stack resources  \n   stack name cfn lambda backed test\n\n \n  StackResourceSummaries    \n  \n  LogicalResourceId    CreateResource  \n  PhysicalResourceId         LATEST bbbfabaaffaa  \n  ResourceType    Custom  CustomResource  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    CreateResourceFunction  \n  PhysicalResourceId    cfn lambda backed test CreateResourceFunction LBGWBFVB  \n  ResourceType    AWS  Lambda  Function  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    FunctionExecutionRole  \n  PhysicalResourceId    cfn lambda backed test FunctionExecutionRole PMYTJNSZ  \n  ResourceType    AWS  IAM  Role  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    UpdateResource  \n  PhysicalResourceId         LATEST ecfbecfbfeabb  \n  ResourceType    Custom  CustomResource  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n   \n  \n  LogicalResourceId    UpdateResourceFunction  \n  PhysicalResourceId    cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n  ResourceType    AWS  Lambda  Function  \n  LastUpdatedTimestamp      T  Z  \n  ResourceStatus    CREATE COMPLETE  \n  DriftInformation    \n  StackResourceDriftStatus    NOT CHECKED \n  \n  \n  \n \n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test CreateResourceFunction LBGWBFVB  \n   log stream name       LATEST bbbfabaaffaa   \n   output text  \n   query  events    message \n\nSTART RequestId  eadad be ea adf ce Version   LATEST\n create resources hoge\n  略 \n Response body \n   Status    SUCCESS    Reason    See the details in CloudWatch Log Stream       LATEST bbbfabaaffaa    PhysicalResourceId         LATEST bbbfabaaffaa    StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe    RequestId    cbf f ef b ddedcc    LogicalResourceId    CreateResource    NoEcho   false   Data     Id    hoge   \n Status code  OK\n END RequestId  eadad be ea adf ce\n REPORT RequestId  eadad be ea adf ce Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n   log stream name       LATEST ecfbecfbfeabb   \n   output text  \n   query  events    message \n\nSTART RequestId  dfa    bbfe Version   LATEST\n  略 \n Response body \n   Status    SUCCESS    Reason    See the details in CloudWatch Log Stream       LATEST ecfbecfbfeabb    PhysicalResourceId         LATEST ecfbecfbfeabb    StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe    RequestId    febba aff a acf fbf    LogicalResourceId    UpdateResource    NoEcho   false   Data      \n Status code  OK\n END RequestId  dfa    bbfe\n REPORT RequestId  dfa    bbfe Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\nスタック作成時にはCreateResourceでリソースの作成、UpdateResourceは呼び出しのみとなることが確認できました。\n\n\nスタック削除\n\nスタックを削除して動作を確認します。\n\n gt  aws cloudformation delete stack  \n   stack name cfn lambda backed test\n\n \n  StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe \n \n\n\nスタック削除すると当然のことながらリソースが取得できなくなるので、ログストリーム名を取得してからログを確認します。\n\n gt  aws logs describe log streams  \n   log group name  aws lambda cfn lambda backed test CreateResourceFunction LBGWBFVB  \n   output text  \n   query  logStreams    logStreamName \n\n     LATEST bbbfabaaffaa      LATEST dfacecbbbfabb\n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test CreateResourceFunction LBGWBFVB  \n   log stream name       LATEST dfacecbbbfabb   \n   output text  \n   query  events    message \n\nSTART RequestId  aca ca b  aaca Version   LATEST\n\n\n\n gt  aws logs describe log streams  \n   log group name  aws lambda cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n   output text  \n   query  logStreams    logStreamName \n\n     LATEST cedeebcefc      LATEST ecfbecfbfeabb\n\n\n gt  aws logs get log events  \n   log group name  aws lambda cfn lambda backed test UpdateResourceFunction BMXDZLKXP  \n   log stream name       LATEST cedeebcefc   \n   output text  \n   query  events    message \n\nSTART RequestId  fcde b e aef ddcffec Version   LATEST\n delete resources hoge\n  略 \n Response body \n   Status    SUCCESS    Reason    See the details in CloudWatch Log Stream       LATEST cedeebcefc    PhysicalResourceId         LATEST cedeebcefc    StackId    arn aws cloudformation us east  xxxxxxxxxxxx stack cfn lambda backed test bedd e e fb dfe    RequestId    da b ddd  cab    LogicalResourceId    UpdateResource    NoEcho   false   Data      \n Status code  OK\n END RequestId  fcde b e aef ddcffec\n REPORT RequestId  fcde b e aef ddcffec Duration   ms Billed Duration   ms Memory Size   MB Max Memory Used   MB\n\n\nスタック削除時にはCreateResourceは呼び出しのみ、UpdateResourceでリソースの削除がされることが確認できました。\n\n\nまとめ\n\n若干定義が面倒になりますがAWS CloudFormationのLambda backedカスタムリソースを利用してリソースを更新・削除できることが確認できました。\n\n\n参考\n\nBlue  lambdaのログをaws cliで見る\n",
      "link": "https://qiita.com/kai_kou/items/7be2eb9a36611bb5da12",
      "updated": "2019-07-08 00:00:14"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ゼロから始めるAWS Elastic Beanstalk  独自ドメイン対応、HTTPS対応、HTTP→HTTPSへのリダイレクト対応、Auto Scaling設定",
      "description": "\n概要\n\n\n\nゼロから始めるJavaでAWS Elastic Beanstalk   EB CLIを使ったJava Webアプリ環境構築の続編です\n\nElastic Beanstalkで構築するWebアプリを以下のようにします\n\n\n\nHTTPS対応\n独自ドメイン対応\nうっかりHTTPでリクエスト来たらHTTPSにリダイレクト\n\nAuto Scalingは不要\n\n\nということで、以下のように構成するときの設定例をみていきます\n\n\n\n\n\n本編\n\n\nALB Application Load Balancer をHTTPSに対応する\n\nElastic BeanstalkでつくったWebアプリをHTTPSに対応させるため、拡張設定用のYAMLファイルを作る。\nプロジェクトのルートに ebextensionsというディレクトリをつくり、そこにalb secure listener configというファイルを作成する。\n\nelastic beantalk java app\n├ー ebextensions\n│└ーalb secure listener config\n・\n・その他のファイル達\n・\n\n\n\nalb secure listener configは以下のようにする\n\n\nalb secure listener config\noption settings \n aws elbv listener default \n ListenerEnabled   false  \n aws elbv listener  \n DefaultProcess  https\n ListenerEnabled   true \n Protocol  HTTPS\n SSLCertificateArns  arn aws acm ap northeast   certificate abcdef   caca b\n aws elasticbeanstalk environment process https \n Port    \n Protocol  HTTP \n\n\n\n\n以下、設定内容の説明をしていく。\n\n\n設定その 「HTTPは受け付けません」\n\n aws elbv listener default \n ListenerEnabled   false  \n\n\naws elbv listener defaultはデフォルトのリスナー ポート の設定を意味する。\nここでListenerEnabled   false としていったんポート、つまり、HTTPは利用不可とした。つまりHTTPでのアクセスを受け付けない設定にした。\n\n 「こういう設定もアリだよね」という例のためで、後で変更してHTTP ポート でアクセス来たらHTTPS ポート にリダイレクトするように構成する。 \n\n\n設定その 「HTTPSを受け付けます、証明書はこれです。」\n\n aws elbv listener  \n DefaultProcess  https\n ListenerEnabled   true \n Protocol  HTTPS\n SSLCertificateArns  arn aws acm ap northeast   certificate abcdef   caca b\n\n\naws elbv listener はポートの設定。\nここでは、HTTPSでつかうポートについての設定で、HTTPSプロトコルで使うことを指定している。\nSSLCertificateArnsにはACM Certificate Mangaer で取得した証明書のARNを指定する\n\n\n\n\n設定その 「ロードバランサーにHTTPSでアクセス来たら、ロードバランサーはECのポート番にHTTPでつなぎます」\n\naws elbv listener  以下にあったDefaultProcess  httpsの部分だが、\nこのhttpsというのは、以下の設定にあるaws elasticbeanstalk environment process https のhttpsに対応している。つまり単なる名前。httpsじゃなくてもOK。\n\n aws elasticbeanstalk environment process https \n Port    \n Protocol  HTTP \n\n\nこれで、ロードバランサーにHTTPSでアクセスがきたら、その先にあるECの番ポートにつなぎに行くという設定ができた。\nつまり、以下の赤マルの部分の設定ができた。\n\n\n\n\nAuto Scalingを構成する\n\n aws autoscaling asg \n Availability Zones  Any\n MaxSize    \n MinSize     \n\n\nっこではaws autoscaling asgでAuto Scalingグループで使用するインスタンスの最小数と最大数を、それぞれに設定した。つまりこれはスケールしない設定。\n\n最大数を以上に設定した場合、Availability Zones  Anyになってるときは、使用可能なAvailability Zone間で均等にインスタンスを起動するようになる。\n\n\nHTTPSのみ有効で、AutoScaleしないalb secure listener configの内容\n\nここまでの内容まとめると\nalb secure listener configは以下のようにする\n\n\nファイルの置き場所\nelastic beantalk java app\n├ー ebextensions\n│└ーalb secure listener config\n・\n・その他のファイル達\n・\n\n\n\n\nHTTPSのみ有効で、AutoScaleしないalb secure listener configの内容\n\n\nalb secure listener config\noption settings \n aws elbv listener default \n ListenerEnabled   false  \n aws elbv listener  \n DefaultProcess  https\n ListenerEnabled   true \n Protocol  HTTPS\n SSLCertificateArns  arn aws acm ap northeast   certificate fabb f efe db eab\n aws elasticbeanstalk environment process https \n Port    \n Protocol  HTTP\n aws autoscaling asg \n Availability Zones  Any\n MaxSize    \n MinSize     \n\n\n\n\nアプリをデプロイする\n\nさて、ここまで設定したところでアプリをデプロイする\n\neb create my jetty app test   instance type t small   elb type application   vpc id vpc xxxxxxxxxxxxxxxxx   vpc elbpublic   vpc elbsubnets subnet xxxxxxxxxxxxxxxxx subnet yyyyyyyyyyyyyyyyy   vpc publicip   vpc ecsubnets subnet xxxxxxxxxxxxxxxxx subnet yyyyyyyyyyyyyyyyy\n\n\n\n独自ドメイン対応\n\nアプリのデプロイが終わったら、Elastic Beanstalkを独自ドメインに対応する\n\nWebコンソールからRoute に行き、対象ドメインを選択してレコードセットの作成をクリック\n\n\n\nエイリアスを選択し、エイリアス先からElastic Beanstalk環境グループのなかから、このドメインを割り当てたい環境を選択し作成をクリックすればOK\n\n\n\nこれで、ドメインがElastic Beanstalkに作った環境にひもづいた。\n\nドメイン名は仮にexample comとすると\n\n\n\nで無事アプリが公開できた。\nここまでで独自ドメイン HTTPS化が終了。\n\n\nHTTPアクセスのHTTPSへのリダイレクト対応\n\nALB Application Loadbalancer の機能アップにより、ALBの設定だけでHTTP→HTTPSへのダイレクトができるようになっている。\n\nさきほどは、alb secure listener configの設定で以下のようにHTTPを無効にしてしまったが、これを有効にする\n\n\nalb secure listener config HTTP無効 \noption settings \n aws elbv listener default \n ListenerEnabled   false  \n\n\n\n↓\n\n\nalb secure listener config HTTP有効 \noption settings \n aws elbv listener default \n ListenerEnabled   true  \n\n\n\n既にデプロイしているなら、Webコンソールから有効にしてもOK\n\nWebコンソールから有効にするには、Elastic Beanstalkにアクセスし、\n\n\n\nElasticBeanstalk 変更したい環境 設定 ロードバランサーとメニューを選択して、変更をクリック\n\n\n\nロードバランサーの変更画面で、無効になっているポートを有効にして、適用をクリックすればOK\n\n\n\n分程度まつと、構成の更新が終了する。\n\nポートのリスナーが有効になったらロードバランサーのリダイレクト設定をする\n\n LoadBalancers \n\nElastic Beanstalkが自動生成したロードバランサーを選択し、リスナータブを選択する。\n\nHTTP のリスナーのルールを表示をクリックする\n\n\n\nルール設定画面でボタンをクリックする\n\n\n\n次に、をクリック\n\n\n\nIFとTHENの条件式を設定できるので、\n  IFにはパスが・・・を選択し、値として  アスタリスク ワイルドカード を入力\n  THENには、アクションの追加でリダイレクト先を選択する\n\n\n\nリダイレクト先としてHTTPSを選択し、ポート番号としてと入力する\n\n\n\nできたら、保存をクリック。\n\nこれで、HTTPにアクセスが来たらHTTPSにリダイレクトする設定は完了。\n\n実際に、\n\nにアクセスしてみると、ちゃんとにリダイレクトされる。\n\n\nまとめ\n\n\nゼロから始めるJavaでAWS Elastic Beanstalk   EB CLIを使ったJava Webアプリ環境構築の続編として、Elastic BeanstalkのWebアプリを独自ドメイン、HTTPS、HTTPからのリダイレクトに対応するための方法について説明しました\nシンプルな設定・操作で実用に耐える環境を構築できるElastic Beanstalkは本番環境構築だけでなく、プロトタイピングなどのラピッドな開発にも向いており色々なシーンで重宝しそうです。\n\n\n説明した構成\n\n\n本稿で使用したソースコード\n",
      "link": "https://qiita.com/riversun/items/b36f207c354bd35bf35c",
      "updated": "2019-07-07 23:13:27"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ゼロから始めるJavaでAWS Elastic Beanstalk   EB CLIを使ったJava Webアプリ環境構築",
      "description": "\n概要\n\n\nAWS Elastic Beanstalkを使えば、ECでのOSやミドルウェアのセットアップ不要でコマンドつでWebアプリ実行環境を構築することができます。セキュリティパッチ等も自動適用できるため運用が軽くなるメリットもある上、ベースはEC等AWSのコンポーネントの組み合わせなので、その気になれば色々カスタマイズもできるという、とても使い勝手の良いサービスです。\n本稿ではEB CLIというツールをつかってコマンドつでJava Webアプリの実行環境を構築します。\nJava Webアプリは、Tomcatで動作するWARではなく、Java SEでつくったスッピンのWebアプリ JAR が対象です。\n つまり概念さえ理解できればWebアプリFrameworkはSpring BootでもPlay frameworkでもかまいませんし、アプリサーバーもTomcatでもJettyでもGlassfishでもOKです \nまた、続編で独自ドメイン対応、HTTPS対応、HTTP→HTTPSへのリダイレクト対応、Auto Scaling設定をします。\n\n\n\n環境・構成\n\n以下のような構成をコマンドラインから作ります\n コマンドラインから作るので環境の量産、再構築もカンタン \n\n\n\n\nアプリ・プラットフォーム\n\n\nAWS Elastic BeanstalkのJava プラットフォーム\n本稿ではJSP ServletのWebアプリをJettyで実行\n\n\nクラウド環境\n\n\nサービス AWS Elastic Beanstalk  EC   Application Loadbalancer on VPC \nリージョン 東京 ap northeast   →東京じゃなくてもOK\n\n\n\n\n\nソースコード\n\n本稿で紹介する全ソースコードはこちらにあります\n\n\n\n本編\n\n\nElastic Beanstalkコマンドラインインターフェイス EB CLI のインストール\n\nElastic BeanstalkアプリはWeb GUIをつかっても構築できるが、環境構築の自動化など本番運用を考えるとコマンドラインを使うと同じような操作を何度もせずにすみ手間もへるし、アプリ・環境の構築・変更がコマンド一発で済のでCIに組み込むなどすれば安定した運用が可能となる。\n\n\n EB CLIをインストールする\n\nEB CLIのインストールにはPythonが必要となる。\nもし、PCにPythonがインストールされていなければ、\n\nなどからインストールしておく。最新版をインストールしておけばOK\n\nPythonがインストールが完了していれば、以下のコマンドでEB CLIをインストールできる\n\npip install awsebcli   upgrade   user\n\n\n\n 以下のコマンドでインストール終了チェック\n\nEB CLIがインストールされ、コマンドラインで利用可能になっているかどうか、以下のコマンドで確認する\n\neb   version\n\nEB CLI    Python   \n\n\nちゃんとインストールできた模様\n\nTIPS\nWindows環境の場合、ebコマンドがみつからない場合がある。\nそのときは、\n\nC  Users  ユーザー名  AppData Roaming Python Python Scripts\n\n\nをパスに追加する\n\n\nJavaでWebアプリを作る\n\n\nElastic Beanstalkで作るJavaアプリはFAT JAR形式で作る\n\nElastic BeanstalkでJava Webアプリを実行するにはいくつかの方法がある。\n\n\nwarファイルをアップロード Tomcatで実行する \nJARファイルをアップロード Java SE環境で実行する \n\n\n本稿ではJARファイルをアップロードする方式を採用する。\n\nJARファイルとは、Javaで作ったアプリ Webアプリ のソースコード 依存ファイルをつのJARにまとめたモノの事を言う。全部つにしてサイズの大きなJARを作るので「FAT JAR」とも言う。\n\nJARを使うメリットは、作ったWebアプリをJARファイルにさえできれば、何でもOKということ。\n\nFrameworkはPlay Frameworkでも、Spring Bootでも、Strutsでも、JSFでも良いし もちろん素のJSP ServletでもOK 、APサーバーもTomcatでもJettyでもGlassfishでもUndertow Wildfly でもOK。\n\nただしJARをつくってElastic Beanstalkで動かすには、「Elastic Beanstalkのお作法」があるので、そちらをみていく。\n\nといっても、難しくはない。\n\n\nElastic Beanstalk用のJavaソースコード構成\n\nWeb APサーバーとしてJettyを使ったJava Webアプリを考える。\n\nまず、全体像は以下のとおり。「 」印がついているところがElastic Beanstalkのための構成\nその他はJavaのMavenプロジェクトの標準的なソースコード構成となる\n\n\nElasticBeanstalk用Javaソースコード構成\nelastic beantalk java app\n├ーsrc main java・・・Javaのソースコードのルートディレクトリ\n│└ーmyserver \n│└ーStartServer java\n├ーsrc main resources\n│└ーwebroot・・・静的WebコンテンツやJSPのルートディレクトリ\n│├ーindex html\n│└ーshow jsp \n├ーsrc main assembly\n│└ーzip xml・・・ elastic beanstalkにアップロードするZIPファイルの生成ルールを記述\n├ー elasticbeanstalk・・・ Elastic Beanstalkへのデプロイ情報を格納するディレクトリ\n│└ーconfig yml・・・ Elastic Beanstalkへのデプロイ情報のYAMLファイル\n├ーtarget・・・ビルドした結果 jarなど が生成されるディレクトリ\n├ーProcfile・・・ Elastic BeanstalkのEC上でJARファイルを実行するためのスクリプトを記述する\n└ーpom xml・・・Maven用設定ファイル\n\n\n\nソースコードや設定ファイル等の中身については後で説明するとして、\n先にこのソースコードがどのようにビルドされてElastic Beanstalkにアップロードされるかをみておく。\n\n\nElastic BeanstalkにアップロードするZIPファイルの構造\n\n本稿では、手元のPCでソースコードをビルドして、それをつのJARファイルにパッケージし、さらにJARファイル以外にもElastic Beanstalkに必要なファイル含めてZIPファイルを生成する。\n\nここで、JavaのソースコードをビルドしてできたFAT JARのファイル名をeb app jar with dependencies jarとする。 maven assembly pluginで生成する、後で説明 \nElasticBeanstalkにアップロードするためのZIPファイル名をmy jetty app eb zipとすると、そのZIPファイルの中身構造は以下となる。\n\n\nmy jetty app eb zipの中身\nmy jetty app eb zip\n├ーmy jetty app jar with dependencies jar・・・JavaのソースコードをつのJARにパッケージしたもの\n└ーProcfile・・・Elastic BeanstalkのEC内でJARを実行するためのスクリプト\n\n\n\nZIPファイルの中には、ソースをまとめたJARファイルと、実行スクリプトの書いてあるファイルであるProcfileのつとなる。\nということで、この形式のZIPファイルをEB CLI経由でElastic Beanstalkにアップロードすればデプロイできる。\nこれが「Elastic Beanstalkのお作法」のつということになる。\n\n次は、Javaアプリのソースをみていく。\nとくに「Elastic Beanstalkのお作法」に関連する部分を中心に説明する\n\n\nJavaアプリのソースコードの作り方\n\nこれからJavaアプリを動かそうとしているElastic Beanstalk環境は以下のようなもので、Java Webアプリとしてケアしておくべき点はポート番号をListenするところ\n\n\n\nつまり、Javaでポート番号をListenするサーバープログラムを作ってあげればひとまずOK\n\nサンプルコード\nということでサンプルコードを以下のリポジトリに置いた\n\n\nこのサンプルはServlet JSP プッシュ通知機能をJetty上動作させている\n\nこのコードをクローンすると\n\nclone \n\n\n前述したとおり以下のようなソースコード構成となっている。\n 一部ファイルは省略 \n\n\nElasticBeanstalk用Javaソースコード構成\nelastic beantalk java app\n├ーsrc main java・・・Javaのソースコードのルートディレクトリ\n│└ーmyserver \n│└ーStartServer java\n├ーsrc main resources\n│└ーwebroot・・・静的WebコンテンツやJSPのルートディレクトリ\n│├ーindex html\n│└ーshow jsp \n├ーsrc main assembly\n│└ーzip xml・・・ elastic beanstalkにアップロードするZIPファイルの生成ルールを記述\n├ー elasticbeanstalk・・・ Elastic Beanstalkへのデプロイ情報を格納するディレクトリ\n│└ーconfig yml・・・ Elastic Beanstalkへのデプロイ情報のYAMLファイル\n├ーtarget・・・ビルドした結果 jarなど が生成されるディレクトリ\n├ーProcfile・・・ Elastic BeanstalkのEC上でJARファイルを実行するためのスクリプトを記述する\n└ーpom xml・・・Maven用設定ファイル\n\n\n\n※「 」印がついているところがElastic Beanstalkのための構成\n\nJavaアプリとしてのメインクラス エントリーポイント はStartServer javaとなる。\nこれをローカル実行してhttp   localhost  にアクセスすれば、以下のようになり\nJSPやServletなどのシンプルな実装を試すことができるサンプルとなっている。\n\n\n\nここからは、ソースコードで重要なところを以下に説明する\n\n\nsrc main java myserver StartServer java\n\nメインクラス。\nJettyを起動してServlet JSPをホストしポートで待ち受ける\n\n\n pom xml\n\n\npom xml 抜粋 \n lt project xmlns    xmlns xsi   \n xsi schemaLocation     gt \n  lt modelVersion gt   lt  modelVersion gt \n  lt groupId gt org riversun lt  groupId gt \n  lt artifactId gt my jetty app lt  artifactId gt \n  lt version gt   lt  version gt \n  lt packaging gt jar lt  packaging gt \n  lt name gt my jetty app lt  name gt \n  lt description gt jetty app on elastic beanstalk lt  description gt \n\n\n\n\npom xml\n lt project xmlns    xmlns xsi   \n xsi schemaLocation     gt \n\n  lt modelVersion gt   lt  modelVersion gt \n\n  lt groupId gt org riversun lt  groupId gt \n  lt artifactId gt my jetty app lt  artifactId gt \n  lt version gt   lt  version gt \n  lt packaging gt jar lt  packaging gt \n  lt name gt my jetty app lt  name gt \n  lt description gt jetty app on elastic beanstalk lt  description gt \n\n  lt properties gt \n  lt project build sourceEncoding gt UTF  lt  project build sourceEncoding gt \n  lt jetty version gt   v lt  jetty version gt \n  lt  properties gt \n\n  lt dependencies gt \n  lt     for server    gt \n  lt dependency gt \n  lt groupId gt javax servlet lt  groupId gt \n  lt artifactId gt javax servlet api lt  artifactId gt \n  lt version gt   lt  version gt \n  lt  dependency gt \n  lt dependency gt \n  lt groupId gt org eclipse jetty lt  groupId gt \n  lt artifactId gt jetty annotations lt  artifactId gt \n  lt version gt   jetty version  lt  version gt \n  lt  dependency gt \n  lt dependency gt \n  lt groupId gt org eclipse jetty lt  groupId gt \n  lt artifactId gt jetty webapp lt  artifactId gt \n  lt version gt   jetty version  lt  version gt \n  lt  dependency gt \n  lt dependency gt \n  lt groupId gt org eclipse jetty lt  groupId gt \n  lt artifactId gt apache jsp lt  artifactId gt \n  lt version gt   jetty version  lt  version gt \n  lt type gt jar lt  type gt \n  lt  dependency gt \n\n  lt dependency gt \n  lt groupId gt org eclipse jetty lt  groupId gt \n  lt artifactId gt apache jstl lt  artifactId gt \n  lt version gt   jetty version  lt  version gt \n  lt type gt pom lt  type gt \n  lt  dependency gt \n  lt dependency gt \n  lt groupId gt com fasterxml jackson core lt  groupId gt \n  lt artifactId gt jackson databind lt  artifactId gt \n  lt version gt   lt  version gt \n  lt  dependency gt \n  lt  dependencies gt \n\n  lt build gt \n  lt plugins gt \n  lt plugin gt \n  lt groupId gt org apache maven plugins lt  groupId gt \n  lt artifactId gt maven compiler plugin lt  artifactId gt \n  lt version gt   lt  version gt \n  lt configuration gt \n  lt source gt  lt  source gt \n  lt target gt  lt  target gt \n  lt excludes gt \n  lt exclude gt examples      lt  exclude gt \n  lt  excludes gt \n  lt  configuration gt \n  lt  plugin gt \n  lt plugin gt \n  lt groupId gt org apache maven plugins lt  groupId gt \n  lt artifactId gt maven javadoc plugin lt  artifactId gt \n  lt version gt   lt  version gt \n  lt executions gt \n  lt execution gt \n  lt id gt attach javadocs lt  id gt \n  lt goals gt \n  lt goal gt jar lt  goal gt \n  lt  goals gt \n  lt  execution gt \n  lt  executions gt \n  lt configuration gt \n  lt author gt true lt  author gt \n  lt source gt  lt  source gt \n  lt show gt protected lt  show gt \n  lt encoding gt UTF  lt  encoding gt \n  lt charset gt UTF  lt  charset gt \n  lt docencoding gt UTF  lt  docencoding gt \n  lt doclint gt none lt  doclint gt \n  lt additionalJOption gt  J Duser language en lt  additionalJOption gt \n  lt  configuration gt \n  lt  plugin gt \n  lt     add source folders    gt \n  lt plugin gt \n  lt groupId gt org codehaus mojo lt  groupId gt \n  lt artifactId gt build helper maven plugin lt  artifactId gt \n  lt version gt   lt  version gt \n  lt executions gt \n  lt execution gt \n  lt phase gt generate sources lt  phase gt \n  lt goals gt \n  lt goal gt add source lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt sources gt \n  lt source gt src main java lt  source gt \n  lt  sources gt \n  lt  configuration gt \n  lt  execution gt \n  lt  executions gt \n  lt  plugin gt \n  lt plugin gt \n  lt groupId gt org apache maven plugins lt  groupId gt \n  lt artifactId gt maven assembly plugin lt  artifactId gt \n  lt executions gt \n  lt    ソースコードをfat jar化する   gt \n  lt execution gt \n  lt id gt package jar lt  id gt \n  lt phase gt package lt  phase gt \n  lt goals gt \n  lt goal gt single lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt appendAssemblyId gt true lt  appendAssemblyId gt \n  lt archive gt \n  lt manifest gt \n  lt mainClass gt myserver StartServer lt  mainClass gt \n  lt  manifest gt \n  lt  archive gt \n  lt finalName gt   project artifactId  lt  finalName gt \n  lt descriptorRefs gt \n  lt descriptorRef gt jar with dependencies lt  descriptorRef gt \n  lt  descriptorRefs gt \n  lt  configuration gt \n  lt  execution gt \n  lt     Elastic Beanstalkにアップロードするzip fat jarや関連ファイルを含む を作成する   gt \n  lt execution gt \n  lt id gt package zip lt  id gt \n  lt phase gt package lt  phase gt \n  lt goals gt \n  lt goal gt single lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt appendAssemblyId gt true lt  appendAssemblyId gt \n  lt finalName gt   project artifactId  lt  finalName gt \n  lt descriptors gt \n  lt descriptor gt src main assembly zip xml lt  descriptor gt \n  lt  descriptors gt \n  lt  configuration gt \n  lt  execution gt \n  lt  executions gt \n  lt  plugin gt \n  lt  plugins gt \n  lt resources gt \n  lt resource gt \n  lt directory gt src main resources lt  directory gt \n  lt  resource gt \n  lt  resources gt \n  lt  build gt \n lt  project gt \n\n\n\n以下、ポイントだけみていく。\n\n\nmavenのartifactId\n\n以下はartifactIdとしてmy jetty appとした。\nこの後のビルドで生成されるファイル名などもこのartifactIdの値が使われる\n\n  lt groupId gt org riversun lt  groupId gt \n  lt artifactId gt my jetty app lt  artifactId gt \n\n\n\nmaven assembly pluginの設定\n\n以下はmaven assembly pluginの設定を行っている。\nmaven assembly pluginとは、配布用のjarファイルやzipファイルを作るためのmavenプラグイン。\nこのmaven assembly pluginにはつのタスクをさせている。\n\n lt plugin gt \n  lt groupId gt org apache maven plugins lt  groupId gt \n  lt artifactId gt maven assembly plugin lt  artifactId gt \n  lt executions gt \n  lt    ソースコードをfat jar化する   gt \n  lt execution gt \n  lt id gt package jar lt  id gt \n  lt phase gt package lt  phase gt \n  lt goals gt \n  lt goal gt single lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt appendAssemblyId gt true lt  appendAssemblyId gt \n  lt archive gt \n  lt manifest gt \n  lt mainClass gt myserver StartServer lt  mainClass gt \n  lt  manifest gt \n  lt  archive gt \n  lt finalName gt   project artifactId  lt  finalName gt \n  lt descriptorRefs gt \n  lt descriptorRef gt jar with dependencies lt  descriptorRef gt \n  lt  descriptorRefs gt \n  lt  configuration gt \n  lt  execution gt \n  lt     Elastic Beanstalkにアップロードするzip fat jarや関連ファイルを含む を作成する   gt \n  lt execution gt \n  lt id gt package zip lt  id gt \n  lt phase gt package lt  phase gt \n  lt goals gt \n  lt goal gt single lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt appendAssemblyId gt true lt  appendAssemblyId gt \n  lt finalName gt   project artifactId  lt  finalName gt \n  lt descriptors gt \n  lt descriptor gt src main assembly zip xml lt  descriptor gt \n  lt  descriptors gt \n  lt  configuration gt \n  lt  execution gt \n  lt  executions gt \n lt  plugin gt \n\n\n\nその JARファイル生成\nまず前半の設定に注目。\n\n  lt execution gt \n  lt id gt package jar lt  id gt \n  lt phase gt package lt  phase gt \n  lt goals gt \n  lt goal gt single lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt appendAssemblyId gt true lt  appendAssemblyId gt \n  lt archive gt \n  lt manifest gt \n  lt mainClass gt myserver StartServer lt  mainClass gt \n  lt  manifest gt \n  lt  archive gt \n  lt finalName gt   project artifactId  lt  finalName gt \n  lt descriptorRefs gt \n  lt descriptorRef gt jar with dependencies lt  descriptorRef gt \n  lt  descriptorRefs gt \n  lt  configuration gt \n  lt  execution gt \n\n\nここで記述しているのはJavaソースコードをつのJARファイルにまとめること。\nつまり、FAT JARを作るためのタスク設定となる。\n\n lt mainClass gt myserver StartServer lt  mainClass gt はJARファイルを実行するときの起動クラス。\n\n lt finalName gt   project artifactId  lt  finalName gt は、最終的にできあがるJARファイルのファイル名がartifactIdで指定された値 my jetty app  AssemblyId  jarとなる。\n\n lt descriptorRef gt jar with dependencies lt  descriptorRef gt はmavenのdependenciesに記載した依存ライブラリも一緒にJARファイルとしてパッケージする。\n\n lt appendAssemblyId gt true lt  appendAssemblyId gt は、生成されるJARファイルのファイル名にAssemblyIdをつけるか否かをセットする。もしこれをtrueにすると、JARファイル名はmy jetty app jar with dependencies jarとなる。\n\nここで設定した条件でJARファイルを生成するには\n\nmvn package\n\n\nとすればよい。するとtargetディレクトリにmy jetty app jar with dependencies jarが生成される\n\nその ZIPファイルの生成\n\n次は後半の設定\n\n\n  lt execution gt \n  lt id gt package zip lt  id gt \n  lt phase gt package lt  phase gt \n  lt goals gt \n  lt goal gt single lt  goal gt \n  lt  goals gt \n  lt configuration gt \n  lt appendAssemblyId gt true lt  appendAssemblyId gt \n  lt finalName gt   project artifactId  lt  finalName gt \n  lt descriptors gt \n  lt descriptor gt src main assembly zip xml lt  descriptor gt \n  lt  descriptors gt \n  lt  configuration gt \n  lt  execution gt \n\n\nここで記述しているのは、上でつくったJARファイルとElastic Beanstalk関連ファイル Procfileなど をZIPファイルにまとめるタスクとなる。\n\n lt finalName gt   project artifactId  lt  finalName gt としているところはJARのときと同じく生成されるファイル名を指定している。\n\nZIPの生成ルールは外だしされた設定ファイルである lt descriptor gt src main assembly zip xml lt  descriptor gt で設定する。\n\n\nsrc main assembly zip xml\n\nさて、その外だしされたzip xmlをみてみる。\n\nこれは最終的にZIPファイルを生成するときの生成ルールとなる。\n\n lt include gt でJARファイルとProcfileなどを指定して、Elastic Beanstalkにアップロードする形式のZIPファイルの生成方法を指示している。\n\n\nzip xml\n lt  xml version    encoding  UTF    gt \n lt assembly\n xmlns   \n xmlns xsi   \n xsi schemaLocation     gt \n  lt id gt eb lt  id gt \n  lt baseDirectory gt   lt  baseDirectory gt \n  lt formats gt \n  lt format gt zip lt  format gt \n  lt  formats gt \n  lt fileSets gt \n  lt fileSet gt \n  lt directory gt   project basedir  lt  directory gt \n  lt outputDirectory gt   lt  outputDirectory gt \n  lt includes gt \n  lt include gt Procfile lt  include gt \n  lt include gt Buildfile lt  include gt \n  lt include gt  ebextensions   lt  include gt \n  lt  includes gt \n  lt  fileSet gt \n  lt fileSet gt \n  lt directory gt   project build directory  lt  directory gt \n  lt outputDirectory gt   lt  outputDirectory gt \n  lt includes gt \n  lt include gt my jetty app jar with dependencies jar lt  include gt \n  lt      lt include gt   jar lt  include gt     gt \n  lt  includes gt \n  lt  fileSet gt \n  lt  fileSets gt \n lt  assembly gt \n\n\n\n\n最終的に生成されるZIPファイルのファイル名はmy jetty app  id  zipとなる。\nzip xmlでは、 lt id gt eb lt  id gt と指定しているので、生成されるZIPファイル名はmy jetty app eb zipとなる。\n\nまとめると必要パッケージを生成するためのmavenコマンドmvn packageを実行するとmy jetty app jar with dependencies jarとmy jetty app eb zipの両方がtargetディレクトリ以下に生成されることになる。\n\nここは、あとで実際にdeployファイルを生成するところでもう一度確認する。\n\n\n Procfile\n\nProcfileはElastic BeanstalkのEC内でJARを実行するためのファイル。\n\nElastic BeanstalkのEC上でアプリを起動するためのコマンドをweb 以下に記載する。\n\n\nProcfile\nweb  java  jar my jetty app jar with dependencies jar\n\n\n\n↓のように起動オプションを記述してもOK\n\nweb  java  jar my jetty app jar with dependencies jar  Xmsm\n\n\n詳細は公式参照\n\n\n elasticbeanstalk config yml\n\nconfig ymlにはデプロイ情報を記述する\nここにはElastic Beansalkにデプロイするファイルtarget my jetty app eb zipを指定している。\n\n\nconfig yml\ndeploy \n artifact  target my jetty app eb zip\n\n\n\nElastic Beanstalkにデプロイする際に、このファイルが参照される。\n\nいまはdeploy artifact しか記述していないが、これからEB CLIをつかってElastic Beanstalkのデプロイ設定をしていく過程でこのconfig ymlに必要な値が追加されていく。\n\n\nEB CLIを使ってアプリをデプロイする\n\nファイルの意味をざっくり理解できたところで、さっそくElastic Beanstalkにアプリをデプロイする。\n\n\nEB CLIを使ってElastic BeanstalkアプリケーションをAWS上に作る\n\n アプリのディレクトリに移動する\n\ncd java jetty app on elasticbeanstalk\n\n\n Elastic BeanstalkにJava用の箱 を作る\n  箱 アプリケーション \n\nそのためのEB CLIのコマンドは以下の形式となる。\n\n\n\neb init アプリ名   region リージョン名   platform プラットフォーム\n\n\n\nここではアプリ名をmy eb app、リージョンは東京リージョン ap northeast  、プラットフォームをjava とする\n\nコマンドは以下のようになる。\n\neb init my eb app   region ap northeast    platform java \n\n\nすると\n\nApplication my eb app has been created \n\n\nというメッセージがでて、\nAWS Elasticbeanstalk上にアプリケーションの箱ができる\n\nさっそくWebコンソールで確認すると箱 アプリケーション ができている\n\n\n\n\n\nさて、ソースコードにある  elasticbeanstalk config ymlを見てみる。\n\n上記コマンドで、config ymlも更新され、以下のようになっている。\n\n\nconfig yml\n\nbranch defaults \n master \n environment  null\ndeploy \n artifact  target my jetty app eb zip\nglobal \n application name  my eb app\n branch  null\n default ec keyname  null\n default platform  java \n default region  ap northeast \n include git submodules  true\n instance profile  null\n platform name  null\n platform version  null\n profile  eb cli\n repository  null\n sc  git\n workspace type  Application\n\n\n\n\nTIPS\n            \n\nもし、Credential情報が登録されていなければ、以下のようにaws access idとaws secret keyを聞かれるので入力する。\n\neb init my eb app   region ap northeast    platform java \nYou have not yet set up your credentials or your credentials are incorrect\nYou must provide your credentials \n aws access id   xxxxx\n aws secret key   xxxxx\nApplication my eb app has been created \n\n\n回入力すれば、 user   awsディレクトリ以下にconfigファイルができるので、次からは聞かれない。\n\n            \n\n\nデプロイ用のパッケージを作る\n\nさきほどみてきたように、Elastic BeanstalkにデプロイするためのZIPファイルを生成する。\n\nZIP生成はMavenでやるのでmaven packageコマンドをたたく\n\n cleanもついでにやっておくと、コマンドは以下のとおり \n\nmvn clean package\n\n\n以下のようになり、無事target ディレクトリにmy jetty app eb zipが生成できた。\n\n INFO      maven assembly plugin  beta  single  package zip    my jetty app    \n INFO  Reading assembly descriptor  src main assembly zip xml\n INFO  Building zip  jetty app on eb target my jetty app eb zip\n INFO                                                                          \n INFO  BUILD SUCCESS\n INFO                                                                          \n INFO  Total time   s\n INFO  Finished at    T    \n INFO  Final Memory  M M\n INFO                                                                          \n\n\n\nElastic Beanstalkにデプロイし環境を構築する\n\nいまから、つくったZIPファイルをEB CLIをつかってElastic Beanstalkにデプロイする\n\nElastic Beanstalkにデプロイするには、eb createコマンドを使う\n\neb createコマンドの使い方\n\neb createコマンドは以下のように指定する。\n\n\neb create 環境の名前  オプション  オプション ・・・ オプション \n\n\nオプションは以下のとおり\n\n\n\nオプション\n説明\n\n\n  cname\nURLのCNAMEを指定。\n例CNAMEにmy jetty app testを指定すると\n\nとしてアクセスできる\n\n\n  instance type\nインスタンスタイプ。\ntシリーズを指定する場合はVPC必須\n\n\n  elb type\nロードバランサーのタイプ\n「application」を指定すると\nアプリケーションロードバランサーになる。\nあとで、独自ドメインやHTTPS対応するときにも\nロードバランサーあると楽\n\n\n  vpc id\nVPCのID\n\n\n  vpc elbpublic\nロードバランサーを\nパブリックサブネットに置く\n\n\n  vpc elbsubnets\nロードバランサーのサブネットIDを指定。\n複数指定するときはカンマ区切り\n\n\n  vpc publicip\nElastic BeanstalkのECを\nパブリックサブネットに置く\n\n\n  vpc ecsubnets\nElastic Beanstalkの\nECのサブネットIDを指定。\n複数指定するときはカンマ区切り\n\n\n\n詳しくは公式参照\n\nそれでは、\nコマンドラインから以下を実行する。\n\neb create my jetty app test   cname my jetty app test   instance type t small   elb type application   vpc id vpc xxxxxxxxxxxxxxxxx   vpc elbpublic   vpc elbsubnets subnet xxxxxxxxxxxxxxxxx subnet yyyyyyyyyyyyyyyyy   vpc publicip   vpc ecsubnets subnet xxxxxxxxxxxxxxxxx subnet yyyyyyyyyyyyyyyyy\n\n\n上記は実際のID等はダミーだが、環境名my jetty app test、cnameがmy jetty app testでECのインスタンスタイプがt small、ELB ロードバランサー がapplicationロードバランサー、VPCのIDがvpc xxxxxxxxxxxxxxxxxとしてさきほどのZIPファイルをデプロイするコマンドとなる。\n\n どのZIPファイルがアップロードされるかは、 elasticbeanstalk config ymlで指定されているのでそれが参照される \n\nまた、  vpc elbpublic、  vpc publicipはELB ロードバランサー とElastic BeanstalkのECが指定したVPC内のパブリックサブネットで実行されることを示している。  vpc elbsubnetsと  vpc ecsubnetsそれぞれおなじパブリック・サブネット つ を指定してある。ELBはパブリックにアクセスできないとアプリにアクセスできないのでパブリックサブネットに置く。EC側はパブリックサブネットに置く方法とプライベートサブネットにおく方法がある。本稿の例では、パブリックサブネットにおいているが、自動生成されたセキュリティグループによってELBからしかアクセスできないようになっている。ただし、パブリックIPは割り当てられる。\n よりセキュアにするにはECはプライベートサブネットに置くなど工夫ができるが、これはElastic BeanstalkというよりEC VPCまわりのセキュリティイシューなのでここでは割愛とする。 \n\nさて、上記コマンドを実行すると、以下のようにアップロードから環境構築までが自動的に実行される。\n\n\nUploading                                                         Done   \n\nEnvironment details for  my jetty app test\n Application name  my eb app\n Region  ap northeast \n Deployed Version  app df  \n Environment ID  e abc\n Platform  arn aws elasticbeanstalk ap northeast   platform Java  running on bit Amazon Linux  \n Tier  WebServer Standard \n CNAME  my jetty app test ap northeast  elasticbeanstalk com\n Updated         \n\nPrinting Status \n      INFO createEnvironment is starting \n      INFO Using elasticbeanstalk ap northeast   as Amazon S storage bucket for environment data \n      INFO Created target group named  arn aws elasticloadbalancing ap northeast   targetgroup awseb AWSEB LMAAAA \n      INFO Created security group named  sg \n      INFO Created security group named  sg \n      INFO Created Auto Scaling launch configuration named  awseb e xxxxxxxxxx stack AWSEBAutoScalingLaunchConfiguration V\n      INFO Created Auto Scaling group named  awseb e xxxxxxxxxx stack AWSEBAutoScalingGroup XXXXXXXXXXXX\n      INFO Waiting for EC instances to launch  This may take a few minutes \n      INFO Created Auto Scaling group policy named  arn aws autoscaling ap northeast   scalingPolicy e autoScalingGroupName awseb e \nxxxxxxxxxx stack AWSEBAutoScalingGroup XXXXXXXXXXXX policyName awseb e xxxxxxxxxx stack AWSEBAutoScalingScaleUpPolicy FA\n      INFO Created Auto Scaling group policy named  arn aws autoscaling ap northeast   scalingPolicy a autoScalingGroupName awseb e \nxxxxxxxxxx stack AWSEBAutoScalingGroup XXXXXXXXXXXX policyName awseb e xxxxxxxxxx stack AWSEBAutoScalingScaleDownPolicy SSZW\n      INFO Created CloudWatch alarm named  awseb e xxxxxxxxxx stack AWSEBCloudwatchAlarmHigh HN\n      INFO Created CloudWatch alarm named  awseb e xxxxxxxxxx stack AWSEBCloudwatchAlarmLow BX\n      INFO Created load balancer named  arn aws elasticloadbalancing ap northeast   loadbalancer app awseb AWSEB BQ\n      INFO Created Load Balancer listener named  arn aws elasticloadbalancing ap northeast   listener app awseb AWSEB BQ\n      INFO Application available at my jetty app test ap northeast  elasticbeanstalk com \n      INFO Successfully launched environment  my jetty app test\n\n\n\n環境構築が開始されると、Webコンソールでも状況を把握することができる。\n\nデプロイできた模様\n\n\n\n\n\n無事アプリがにデプロイされた模様\n\nアクセスすると、\n\n\n\nちゃんとJSPやServletも動いている模様\n\n\n\n\nまとめ\n\n\nJava SEをつかったWebアプリをEB CLIをつかって、AWS Elastic Beanstalkにデプロイするまでの手順をハンズオン方式で紹介しました\n結果、以下のような構成をコマンドから構築することができました\n\n\n\n\n\n紹介した全ソースコードはこちらです\n\n続編「ゼロから始めるAWS Elastic Beanstalk  」では、独自ドメイン対応、HTTPS対応、HTTP→HTTPSへのリダイレクト対応、Auto Scaling設定について、取り扱います。\n",
      "link": "https://qiita.com/riversun/items/9d53238fef611ce7d466",
      "updated": "2019-07-07 23:17:21"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ServerlessFrameworkでLambda ALBを使う",
      "description": "\nはじめに\n\nAPI Gatewayを利用するのであればfunctionsに記載するだけであったが\nALBをLambdaのトリガーとする場合別途ALBを作成する必要がありそうだったのでやってみた\n\n\nサービス構成\n\nALB   gt  Lambda\n\nLambdaは引数をそのままレスポンスとして返すだけのものを作成。\nALBはつのパスでのルーティングをそれぞれ同じLambdaへ、それ以外をとするものを作成。\n\n※事前に用意したもの\n  LambdaにつけるIAMロール\n  ALBにつけるVPC のサブネットIDつとセキュリティグループID \n\n\nフォルダ構成\n\n \n├ーhandler js\n└ーserverless yml\n\n\n\nLambdaソースコード\n\nコードは内容には特に関係ないので別になんでもよい\n\n\nhandler js\nmodule exports main   async  event    gt   \n return response    \n headers   \n  Content Type    application json \n   \n body  JSON stringify event  \n statusCode  \n  \n \n\n\n\n\n\nserverless yml\n\nAPI Gatewayと若干書き方は変わるが\n単純にALBをLambdaのAPIとしての口としたいだけなら\nALBとリスナーをresourcesで作成して\nfunctionsでそれに紐づけるだけでよさそう。\n\n\nserverless yml\nservice  alb lambda example\n\nprovider \n name  aws\n runtime  nodejs x\n region  ap northeast \n\nfunctions \n echo \n handler  handler main\n role  arn aws iam   role iam role   IAMロールのARN\n events \n   alb \n listenerArn  \n Ref  exampleLoadBalancerListener\n priority  \n conditions \n path   hello\n   alb \n listenerArn  \n Ref  exampleLoadBalancerListener\n priority  \n conditions \n path   world\n\nresources \n Resources \n exampleLoadBalancer \n Type  AWS  ElasticLoadBalancingV  LoadBalancer\n Properties \n Name  alb lambda example alb\n Scheme  internet facing\n Subnets \n   subnet XXXXXXXXXXXXX  サブネットID\n   subnet YYYYYYYYYYYYY  サブネットID\n SecurityGroups \n   sg ZZZZZZZZZZZZZZ  セキュリティグループID\n exampleLoadBalancerListener \n Type   AWS  ElasticLoadBalancingV  Listener \n Properties \n DefaultActions \n   Type  fixed response\n FixedResponseConfig  \n ContentType  text html\n MessageBody   lt h gt Not Found lt  h gt \n StatusCode  \n LoadBalancerArn  \n Ref  exampleLoadBalancer\n Port  \n Protocol  HTTP\n\n\n\n\n結果\n\n\nAWSコンソール上のALBの振り分けルール\n\n\n\n作成されたALBのDNSにブラウザでアクセスすると helloと worldではJSONがレスポンスされそれ以外ではNot Foundページが表示された。\n\n\nトラブルシュート\n\n\nsls deployに成功するがLambdaがALBと関連付けられてない\n\nserverlessをアップデートすると治るかも",
      "link": "https://qiita.com/aki_lua87/items/6cd4db59c45a6dd794bc",
      "updated": "2019-07-07 19:09:53"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【AWS】API Gateway経由でWebアプリにアクセスした際のレスポンスがBase形式となる",
      "description": "AWS Lambdaなどに構築したWebアプリにAPI Gateway経由でアクセスした際に、既定の設定ではレスポンスがHTMLコンテンツではなくBase形式となってしまう。\n\n\n\nこのような場合は、API Gatewayの設定でAPIの「バイナリメディアタイプ」を追加する必要がある。\n\n\n解決方法\n\n対象のAPIで 設定   バイナリメディアタイプ で バイナリメディアタイプの追加 をクリックし、text htmlまたは   を指定して、 変更の保存 をクリック。\n\n\n\nステージに反映させるためにAPIのデプロイを忘れずに行う。\n\n再度Webアプリにアクセスすると正常にHTMLコンテンツが表示された。\n\n\n\n以上",
      "link": "https://qiita.com/r-wakatsuki/items/de90155d26a7655f3d24",
      "updated": "2019-07-07 14:16:56"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Django   Heroku   AWS Sで画像表示させる方法",
      "description": "\n概要\n\nDjangoアプリで画像をSに保存し、アプリに表示させる方法を書いてみます。\n想定している処理は、①ユーザーが画像投稿→②Sに保存→③Sからアプリに画像表示です。\n\nなお、環境はPython  、Django になります。\n\n\n  AWS設定\n\n\n   AWS登録\n\nSを使用するのに、AWSアカウントが必要になりますので、お持ちでない方はAWS Signupからご登録ください。\n\n参照 \nAWSアカウント作成の流れ\nAmazon S\n\n\n  バケット作成\n\n\n①Sを検索してクリック\n\n\n\n\n②バケット作成\n\n\n\n\n③バケット名とリージョンを選択\n\n\n\n\n④オプションなし\n\nオプションですが、今回は設定しません\n\n\n\n\n⑤アクセス権限\n\nこちらの項目を外してください、初期設定ではブロックされてしまいます\n\n\n\n\n⑥内容を確認して作成\n\n\n\n\n⑦バケット完成\n\n\n\n\n   CORS設定\n\n\n①CORSの設定に移動\n\n作成したバケットをクリックし、「アクセス制限」の「CORSの設定」に移動します\n\n\n\n\n②CORS構成エディターに追加\n\n下記のように追加してください\n\n\nCORS\n lt  xml version    encoding  UTF    gt \n lt CORSConfiguration xmlns    gt \n  lt CORSRule gt \n  lt AllowedOrigin gt   lt  AllowedOrigin gt \n  lt AllowedMethod gt GET lt  AllowedMethod gt \n  lt AllowedMethod gt POST lt  AllowedMethod gt \n  lt AllowedMethod gt PUT lt  AllowedMethod gt \n  lt AllowedHeader gt   lt  AllowedHeader gt \n  lt  CORSRule gt \n lt  CORSConfiguration gt \n\n\n\n参照 \nCross Origin Resource Sharing  CORS \nDirect to S File Uploads in Python   S Setup\n\n\n   IAM設定\n\n\n①IAMを検索してクリック\n\n\n\n\n②ユーザーをクリック\n\n\n\n\n③ユーザー追加をクリック\n\n\n\n\n④ユーザー名を入力、プログラムによるアクセスをチェック\n\n\n\n\n⑤アクセス許可\n\nSを検索し、AmazonSFullAccessにチェック\n\n\n\n\n⑥オプションなし\n\nオプションですが、今回は設定しません\n\n\n\n\n⑦内容を確認して作成\n\n\n\n\n⑧IAM設定完了\n\n後ほど、アクセスキーIDとシークレットアクセスキーを使いますので、画面を閉じないでください\n\n\n\n参照 IAMとは\n\n\n アプリ設定\n\n\n  インストール\n\n\n①django storagesインストール\n\n\nterminal\n  pip install django storages\n\n\n\n参照 django storages   Amazon S\n\n\n②botoインストール\n\n\nterminal\n  pip install boto\n\n\n\n参照 \nBoto  Documentation\nAWS SDK for Python  Boto \n\n\n   settings py設定\n\n下記のように追加してください\n\n\nsettings py\n INSTALLED APPS    \n  django contrib admin  \n  django contrib auth  \n  django contrib contenttypes  \n  django contrib sessions  \n  django contrib messages  \n  django contrib staticfiles  \n  your app name  \n  storages    追加\n \n\n  追加\n AWS ACCESS KEY ID   os environ  AWS ACCESS KEY ID  \n AWS SECRET ACCESS KEY   os environ  AWS SECRET ACCESS KEY  \n AWS STORAGE BUCKET NAME   os environ  AWS STORAGE BUCKET NAME  \n\n DEFAULT FILE STORAGE    storages backends sboto SBotoStorage \n S URL    http    s s amazonaws com     AWS STORAGE BUCKET NAME\n MEDIA URL   S URL\n\n AWS S FILE OVERWRITE   False\n AWS DEFAULT ACL   None\n\n\n\n\n  環境変数\n\nHerokuの環境変数をつ設定します。\n\n①AWS ACCESS KEY IDと②AWS SECRET ACCESS KEYは、IAMユーザーを追加した際に表示された、アクセスキーIDとシークレットアクセスキーになります。\n\n③AWS STORAGE BUCKET NAMEは作成したバケット名です。\n\n\nterminal\n  heroku config set AWS ACCESS KEY ID  ご自身のアクセスキーIDを記入 \n  heroku config set AWS SECRET ACCESS KEY  ご自身のシークレットアクセスキーを記入 \n  heroku config set AWS STORAGE BUCKET NAME  ご自身のバケット名を記入 \n\n\n\n参照 \nConfiguration and Config Vars   Heroku\nDjango Docs   Deployment checklist\n\n\n   requirements txt\n\nインストールしたモジュールをrequirements txtに追加します。\n\n\nterminal\n  pip freeze  gt  requirements txt\n\n\n\n\n   local settings py設定\n\nもしlocal settings pyを使用している場合は、下記を参考にしてください。\n\nローカル環境ではMEDIA ROOTで指定したディレクトリから読み込み、HerokuではSから読み込むことができます。\n\n\nsettings py\n   \n省略\n   \n\nDEBUG   False\n\nALLOWED HOSTS        \n\nINSTALLED APPS    \n  django contrib admin  \n  django contrib auth  \n  django contrib contenttypes  \n  django contrib sessions  \n  django contrib messages  \n  django contrib staticfiles  \n  your app name  \n  storages  \n \n\n   \n省略\n   \n\nMEDIA ROOT   os path join BASE DIR   media  \nMEDIA URL     media  \n\ntry \n from  local settings import  \nexcept ImportError \n pass\n\nif not DEBUG \n SECRET KEY   os environ  SECRET KEY  \n\n AWS ACCESS KEY ID   os environ  AWS ACCESS KEY ID  \n AWS SECRET ACCESS KEY   os environ  AWS SECRET ACCESS KEY  \n AWS STORAGE BUCKET NAME   os environ  AWS STORAGE BUCKET NAME  \n\n DEFAULT FILE STORAGE    storages backends sboto SBotoStorage \n S URL    http    s s amazonaws com     AWS STORAGE BUCKET NAME\n MEDIA URL   S URL\n\n AWS S FILE OVERWRITE   False\n AWS DEFAULT ACL   None\n\n import django heroku\n django heroku settings locals   \n\ndb from env   dj database url config conn max age   ssl require True \nDATABASES  default   update db from env \n\n\n\n\nlocal settings py\nimport os\n\nSECRET KEY    ご自身のSECRET KEYを記入 \n\nBASE DIR   os path dirname os path dirname os path abspath   file     \n\nDATABASES    \n  default    \n  ENGINE    django db backends sqlite  \n  NAME   os path join BASE DIR   db sqlite   \n  \n \n\nDEBUG   True\n\n\n\n\n最後に\n\nHerokuでのデプロイが曖昧でしたら、下記の記事も参考にしてみてください。\n\nDjangoアプリをHerokuにデプロイする方法\n\n以上",
      "link": "https://qiita.com/frosty/items/e793da61f9525d7afbe6",
      "updated": "2019-07-07 13:41:23"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "【参加レポート】AWS Summit Tokyo  Dayにいってきた ",
      "description": "\n\nめちゃくちゃ出遅れましたが、AWS Summit Tokyo  Dayに参加してきましたので、参加レポートを書きたいと思います。\n\n結論から書くと、去年より遥かにグレードアップしている部分が多数あると感じられました。\n具体的に何がグレードアップしていたかについては、記事の中でご紹介させていただこうと思います。\n\n\nAWS Summit Tokyoについて\n\nまずはじめに、AWS Summit Tokyoを知らないという人のために、AWS Summit Tokyoのご説明を致します。\n\nAWS Summit Tokyoは、Amazon Web Serviceが公式で開催しているAWSの祭典です。\nIT系のカンファレンスの中では日本最大級の規模になっており、合計以上セッションやデモンストレーション、ハンズオンワークショップを実施しています。\n\n年々、来場者数が増加しているらしく、AWS Summit Tokyo では、初の幕張メッセでの開催となりました。\n\nセッション以外でも、AWSのエキスパートに質問や相談ができるブースや、スポンサー企業のブースなど、様々なコンテンツが用意されています。\n\nより詳しい情報は、公式ページに掲載されておりますので、ご興味のある方はご確認ください。\n\n\n\nAWS Summit Tokyo のグレードアップポイント \n\n今回のAWS Summit Tokyo では今までのものと比較しても、大規模にグレードアップしているポイントがいくつもありました。\nその中でも、よりすぐりのグレードアップポイントについて、ピックアップしてご紹介したいと思います。\n\n\n 会場がグレードアップ \n\nまず、大きな変化としては、会場が幕張メッセになったことです。\n\n去年までは、品川プリンスホテルでの開催でした。\n品川ということで、職場からのアクセスは大変良かったのですが、セッション会場間の移動が大変だったり、企業ブースがセッション会場と別部屋になっていました。\n\nしかし、幕張メッセでの開催になったことで、企業ブースとセッション会場がシームレスに同じ空間に存在する形になりました。\n一部のセッション会場を除いた殆どのセッション会場が同じフロアにあるので、セッション会場の移動がとても楽になりました。\n\n\n AWSSummitの会場全体の様子です 今年は幕張メッセとなり、昨年より広い会場となりました AWSSummit pic twitter com hpfIGaNdーAWS  アマゾンウェブサービス  awscloud jp  年月日\n\n\n\n\nまた、DeepRacerのレース会場があったり、\n\n\nこちらはAWS EXPOホールの AWSDeepRacerリーグ会場です。日本で初開催となった AWSDeepRacerリーグは、初日から大盛況です   AWSSummit pic twitter com FSbwngTmーAWS  アマゾンウェブサービス  awscloud jp  June   \n\n\n\n\n空中にNewRelicさんの広告付きの電車が走るなど、豪華さに拍車がかかっていました \n 写真を取り忘れました     \n\n\n アメニティが豪華\n\nスポンサーブースやExpoブースで貰えるアメニティがとても豪華になっていたように感じました。\n\n\n AWSSummitのスポンサブースでは、様々なノベルティをお配りしています。 AWSSummitは最終日となりましたが、ブースにもぜひお立ち寄りくださいpic twitter com hAYKLQpーAWS  アマゾンウェブサービス  awscloud jp  年月日\n\n\n\n\n去年はアンケートに答えると、PCのカメラ部分を隠すウェブカメラカバーがもらえたのですが、今年は冷感タオルと、アイスがもらえました \n\n\nお帰りの際は、アンケートへ回答して、かき氷かアイスクリームを食べるのをお忘れなく AWSSummit pic twitter com pMArQOijーAWS  アマゾンウェブサービス  awscloud jp  June   \n\n\n\n\n認定者ラウンジも、去年は認定者シールを貰えましたが、今年はステンレスボトルと、認定者バッチを貰えました。\n\n\n認定資格者の方が利用できる認定者ラウンジです。お席と電源をご用意しておりますので、お立ち寄り下さい  AWSSummit pic twitter com pEzaLdRnーAWS  アマゾンウェブサービス  awscloud jp  June   \n\n\n\n\n僕は毎年チャレンジしていたSIOSさんのくじで、ようやくCoatiマグカップをGetすることができました \n\n\nCoatiマグカップが当たったw AWSSummit pic twitter com QYnaIbHYーurmot   urmot  June   \n\n\n\n\n\n セッション\n\nセッションもかなり豪華になっていました \n\n特に基調講演が始まる際にはダンサーによるオープニングアクトがあるなど、本当にテックカンファレンスなのかを疑うほどの豪華さでした。\n\n\n AWSSummit基調講演のオープニングアクトが始まりました   pic twitter com vIRiajZKvーAWS  アマゾンウェブサービス  awscloud jp  年月日\n\n\n\n\n基調講演の内容を簡単にまとめると、\n\n\n前年同期比で の成長\n\n\nクラウドを使う がAWSを選択\n\n\nAWSのIT人材の育成\n\n\nAWS Loft Tokyo\nAWS Dev Day\nAWS Innvocate\nAWS Educate\n\n\nクラウドジャーニー\n\n\nWindows対応\nRedshiftのパフォーマンス倍\n\n\n「全ての開発者に機械学習を」\n\n\nSageMaker\nDeepRacer\nトレーニングマテリアルの日本語化\n\n\n\n\nという感じです。\n相変わらず、成長率が凄まじいですね   。\n\n去年に引き続き機械学習に力を入れていき、IT人材の育成にも力を入れていくみたいですね ありがたい    。\n\n\nまとめ\n\n去年より遥かにグレードアップし、益々成長を遂げているAWS Summitでした \n\n次は、 にAWS Summit Osaka が開催されます。\n\nそして、来年のAWS Summit Tokyoの会場は横浜だそうです \n 今回もだけど、もはやTokyoじゃないw \n\n\n AWSSummitが終了しました 本日のまとめビデオをご覧下さい。今年も多くの出会いがありました。来年は会場をパシフィコ横浜に移し、さらに AWSSummit Tokyoを盛り上げていければと思います。そして初開催の AWSSummit Osakaは月日 大阪で皆さまとお会いできるのを、楽しみにしています  pic twitter com gBnCtAOZーAWS  アマゾンウェブサービス  awscloud jp  June   \n\n\n",
      "link": "https://qiita.com/uramotot/items/27ce5290a561979ac670",
      "updated": "2019-07-07 11:46:16"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ECとELBで、URLごとに別のサーバにルーティングする",
      "description": "\nイントロ\n\nECで構築したUiPath Orchestratorサーバを、自己署名じゃない証明書でSSLで公開する Certificate ManagerとELBを使用  のつづきです。\n自己署名証明書で動いているIISのOrchestratorサーバを、ELBを用いて正式な証明書を使ったサーバとして公開するところまでをやりました。\n\n残りは、URLが「」の時は、Elasticsearchサーバへルーティングする設定の追加です。\n\n\nもろもろ前提などは、前回の記事をご確認ください。\n\n\nやってみる\n\nロードバランサの設定画面で「ルールの表示 編集」をクリック。\n\n\nルーティングを追加するために上部の「プラス」をクリック。\n\n\nルールを追加するために「ルールの挿入」をクリック。\n\n\nURLが「」の時だったらを追加するため、プルダウンから「ホストヘッダー   」を選択。\n\n\n「ela example xyz」と入力しチェックマークをクリック。\n\n\nIF 左側 が確定しました。続いてTHEN 右側です ですが、プルダウンから「転送先」を選択し、転送先のターゲットグループとして前回作成した「ela」を選択。チェックマークをクリックすると、、\n\n\n「アクセス先がela example xyzだったら、ターゲットグループelaに転送する」という設定が追加されました。右上の「保存」をクリックします。\n\n\n追加されました \n\n\nさてへアクセスしてみると、、表示されましたねー。。\n\n\nこのApplication Load Balancerのホスト名ベースのルーティング機能によって、一台のロードバランサを使って、様々なサーバにリバースプロキシできるわけですね。とても便利です。\nUiPath Orchestratorの構築の観点からも、メインのOrchestratorサーバとElasticsearchサーバを、一台のロードバランサでURLで振り分けできることが確認出来ましたね。\n\nおつかれさまでした。\n\n\n関連リンク\n\n\n\nECで構築したUiPath Orchestratorサーバを、自己署名じゃない証明書でSSLで公開する Certificate ManagerとELBを使用  前回記事\nALBのHost based routingを試してみた\n",
      "link": "https://qiita.com/masatomix/items/b311eaac67440406d0e5",
      "updated": "2019-07-07 11:17:56"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Terraform  リポジトリ構造と活用範囲を考える",
      "description": "\nはじめに\n\nTerraformに入門すると、最初に簡単なVPCを作成するまでは早いですが、実運用を見越してterraform tfstateの管理方法について考えたり、効率的なディレクトリ構成について考えたりすると手が止まってしまいます。\n\n入門時期を終えて、書籍『Pragmatic Terraform on AWS』を読んでTerraformのお作法について学び直したところなので、これまで得た知見を整理するために記事を書いてみます。\n\n\n書くこと\n\n\nTerraformの使い所\nTerraformで実装したリポジトリの例とサンプルコード 一部 \n\n\n\n書かないこと\n\n\nTerraformの使い方・インストール方法\n\n\n\nTerraformの運用方法\n\nTerraformでクラウド環境を構築するだけなら直ぐですが、「構築した環境の上で動作するアプリケーションのライフサイクルについて」や、「AWSアカウント直後から、対象の環境でTerraformを初めて実行するまで」のことを考えると、色々と考えることが多くなります。\n\n\n実際の構築順序\n\n今回の記事は「Terraformで全ては構築しない」という前提で記載しています。\nこれを踏まえて、「AWSアカウントを取得して基本設定を終わらせた状態」から「アプリケーションをデプロイする」までをまとめると次のつのステップが必要になります。\n\n\nアカウントの基本設定とtfstate保存領域の確保\nTerraformによるインフラの構築\nTerraform対応範囲外のインフラ構築 GUI、CUI \n\n\nこのうち、 のみがTerraformの実行によって環境を構築するフェーズです。\n\n\n\n\n アカウントの基本設定とtfstate保存領域の確保\n\nAWSアカウントを取得して、本当の直後は次のようなサイトを参考にして設定を行います。\n\n\nAWSアカウントを取得したら速攻でやっておくべき初期設定まとめ  Qiita\n\n\nこの設定が終わったら、Terraformで作業を行うために次のような設定を行います。\n\n\n作業ユーザの作成\n\naccess key  secret keyの発行\nbackend用のS Bucketの作成\n\n\nまた、状況に応じてTerraform実行前にやることとして、以下のようなこともあるかもしれません。\n\n\nElastic IPの発行 外部IFとして固定が必要なものなど \nドメインの取得とRoute の設定\nACMを使用したSSL証明書の登録\n\n\ncf  Multi account AWS Architecture   Terraform by HashiCorp\n\n\nideally the infrastructure that is used by Terraform should exist outside of the infrastructure that Terraform manages \n\n\n\n  Terraformによるインフラの構築\n\n詳細については、「Moduleのお作法を整理する」以降の節で記載します。\n\n\n  Terraform対応範囲外のインフラ構築 GUI、CUI \n\nアプリケーションのデプロイなど、Terraformが構築したシステム・インフラの上にのるリソースのデプロイを行います。\n\n\nElastic Beanstalkアプリケーション\nAWS Lambdaアプリケーション\nAPI Gatewayアプリケーション\n\n\nただし、この については、自分の中でも悩みの多いところであり、どのように役割分担してゆくか戸惑っている箇所です。\n\n\nModuleのお作法を整理する\n\nまず最初に、Terraformでインフラを構築するにあたって避けては通ることの出来ないmoduleについて整理します。\n\n\nStandard Module Structure\n\nTerraformの公式サイトではStandard Module Structure  標準モジュール構造 として次のつの構成が紹介されています。\ncf  Creating Modules   Terraform by HashiCorp\n\n\nminimal recommended module\n complete example of a module\n\n\n\n構造のルール\n\n構造のルールとして次のようなことが記載されています。\n\n\n\nRoot module  リポジトリのルートディレクトリに「Terraform files」を配置しなければならない。これがモジュールのprimary entrypointとなる。\n\nREADME   Root moduleとネストされたmoduleはREADMEを配置した方が良い。inputやoutputはツールで自動生成できるため、記載する必要はない。\n\nLICENSE   Publicにモジュール公開するなら、あった方が良い。\n\nmain tf  variables tf  outputs tf  最小モジュールとして推奨されるファイル名。空でもあったほうが良い。\n\nVariables and outputs should have descriptions   全てのvariableとoutputに行の説明を記載しましょう。\n\nNested modules    moduleはmodules 配下に配置しましょう。moduleは可能な限り複雑さを排除した振る舞いを記載し、READMEに用法を記載しましょう。Root moduleがNested modulesを呼ぶ場合、個別のリポジトリにせずに全てつのリポジトリで管理するようにしましょう。\n\nExamples    moduleの使用方法を記載したexampleはexamples 配下に配置しましょう。exampleにはREADMEを配置し、用法のゴールを記載しましょう。\n\n\n\nA minimal recommended module\n\n\n例 最小構成\n  tree minimal module \n \n├ーREADME md\n├ーmain tf\n├ーvariables tf\n├ーoutputs tf\n\n\n\n\nA complete example of a module\n\n\n例 全体構成\n  tree complete module \n \n├ーREADME md\n├ーmain tf\n├ーvariables tf\n├ーoutputs tf\n├ー   \n├ーmodules \n│├ーnestedA \n││├ーREADME md\n││├ーvariables tf\n││├ーmain tf\n││├ーoutputs tf\n│├ーnestedB \n│├ー    \n├ーexamples \n│├ーexampleA \n││├ーmain tf\n│├ーexampleB \n│├ー    \n\n\n\n\nTerraformリポジトリの構造を検討する\n\nTerraformによってシステムのインフラを構築する際に実装するリポジトリの構造について検討します。\n\n\n基本コンセプト\n\n検討にあたっての基本コンセプトは次の通りです。\n\n\nコンポーネント分割\n環境分割\nローカル・モジュール\nBackendはAWS Sで管理\n\n\n\n コンポーネント分割\n\n書籍『Pragmatic Terraform on AWS』に「コンポーネント分割」という節があります。\n\nここには次のような記載記載があります。\n\n\n環境は分かりやすい境界です。ひとつの環境につき、ひとつのtfstateファイルというのは\n素直な考え⽅です。しかし、この考え⽅にはデメリットがあります。ひとつのtfstateファイ\nルでその環境のすべてのリソースを管理すると、ひとつのミスが全体に影響を与えてしまいま\nす。また、Terraformの実⾏にも時間がかかります。そこで、環境を複数のコンポーネントに\n分割しましょう。\n引用 「KOS MOS   『Pragmatic Terraform on AWS』v    」\n\n\nまた、具体的な例として次のような指針が示されています。\n\n\n安定度が高いコンポーネントとそれ以外の分離\nステートフルなリソース ストレージやデータストア の隔離\n エンドユーザへの 影響範囲が異なるものの分割\n組織のライフサイクルに関わるリソースの分離\n関心事の分離\n\n\nこれらの記載を踏まえると、例えば「VPC」「RDS」「EC」「IAM」といった異なる役割を持つリソースをつのtfstateファイルで保持することが好ましくないことがわかります。\n\nそのため、今回は記事「terraformはどの単位で分割すべきか  Qiita」を参考にしてコンポーネントの分割を行うことにしました。\n\n\n 環境分割\n\nここで言う環境とは、Terraformによって構築する対象のサービスが「本番」「検証」「開発」のどの用途で使用されるといった意味で使用します。\nこの環境の定義の方法は次のつの方法があります。\n\n\nディレクトリ分割型\nworkspace型\n\n\nインターネット上には「ディレクトリ分割型」で環境を定義する例が多く、記事「Terraform運用ベストプラクティス workspaceをやめてみた等諸々 長生村本郷Engineers Blog」では「workspace型」のデメリットによって「ディレクトリ分割型」に戻したという旨の記載があります。\n\nまた、書籍『Pragmatic Terraform on AWS』には、年 ⽉に⾏われたHashiCorpJapan Meetupにおいて、workspace型の利用者は少数派で、多くのユーザがディレクトリ分割を行っていたとの記載があります。\n\n一方で、workspace型の活用例も確かに存在し、そちらの実例の方が魅力に感じたため、今回はworkspace型を採用することにします。\n\n\n ローカル・モジュール\n\n「Module Sources   Terraform by HashiCorp」によると、module blockがsourceとして指定出来るものとして次の方法が紹介されています。\n\n\nLocal paths\nTerraform Registry\nGitHub\nBitbucket\nGeneric Git  Mercurial repositories\nHTTP URLs\nS buckets\nGCS buckets\n\n\n今回、この中で使用するのは「Local paths」です。\n\n\n  BackendはAWS Sで管理\n\nこれはもう当然の選択ですが、BackendにAWS Sを指定してterraform tfsateを管理する方式を採用します。\n\n\nプロジェクト構成\n\nStandard Module Structureをベースに、前述の基本コンセプトを考慮して設計したディレクトリ構成が以下です。\n\n\n構成例\n  tree sample project \n \n├ーREADME md\n├ーLICENSE\n├ー secret\n├ー gitignore\n├ー   \n├ーbin  lt    リポジトリの初期化\n│├ーinit components sh\n│├ーinit s sh\n│├ーinit s bat\n│├ーconfig \n├ーenvironments   lt    環境ごとの変数を定義\n│├ーcommon\n││├ーterraform tfvars\n│├ーproduct\n││├ーterraform tfvars\n│├ーstaging\n│├ーdevelop\n│├ーdefault\n├ーmodules   lt     Local Module\n│├ーnestedA \n││├ーREADME md\n││├ーvariables tf\n││├ーmain tf\n││├ーoutputs tf\n│├ーnestedB \n│├ー    \n├ーcomponents   lt    コンポーネント分割した設定群\n│├ーnetwork \n││├ーREADME md\n││├ーmain tf  lt    基本となる処理を記載\n││├ーvariables tf  lt     variablesを記載\n││├ーoutputs tf  lt     outputブロックを記載\n││├ーbackend tf  lt    リモートステートを記載 terraformブロック、dataブロック \n││├ーprovider tf  lt     providerを記載\n││├ー terraform\n│├ーfirewall \n│├ーiam \n│├ーs \n│├ーdatastore \n│├ーapplication \n│├ーoperation \n│├ー    \n\n\n\nTerraformを実際に実行するのは、components配下にある任意のコンポーネント・ディレクトリの下で実施します。\n\n大雑把な構成図は以下の通りです。\n\n\n\n\nSample Code\n\nfirewallコンポーネントを例に、コンポーネント配下のコード例を記載します。\n\n\nbackend\n\nリモートステート情報を定義するcomponents firewall backend tfの例は次の通りです。\n\n\ncomponents firewall backend tf\nterraform  \n required version      \n backend  s   \n region    ap northeast  \n encrypt   true\n\n bucket     lt unique bucket name gt  \n key    firewall terraform tfstate \n\n profile    profile name \n  \n \n\ndata  terraform remote state   network   \n backend    s \n\n config    \n bucket     lt unique bucket name gt  \n key    env    terraform workspace  network terraform tfstate \n region    ap northeast  \n\n profile    profile name \n  \n \n\n\n\n\nvariables\n\n変数情報を定義するcomponents firewall variables tfの例は次の通りです。\n\n\ncomponents firewall variables tf\nvariable  common   \n type   map string \n\n default    \n  default region     ap northeast  \n  default project     project name \n  \n \n\n\n\n\nmain\n\n基本となる処理を定義するcomponents firewall main tfの例は次の通りです。\n\n\ncomponents firewall main tf\nmodule  firewall   \n source          modules nestedB \n\n common   var common\n vpc   data terraform remote state network outputs vpc\n \n\n\n\n\nprovider\n\nproviderを定義するcomponents firewall provider tfの例は次の通りです。\n\n\ncomponents firewall provider tf\nvariable  aws access key   \n \n\nvariable  aws secret key   \n \n\nprovider  aws   \n version        \n access key   var aws access key\n secret key   var aws secret key\n region    ap northeast  \n \n\n\n\n\nterraform tfvars\n\n環境ごとに定義する変数ファイルenvironments common terraform tfvarsの例は次の通りです。\n\n\nenvironments common terraform tfvars\nregion    ap northeast  \ncidrs                 \namis    \n  ap northeast a     ami abc \n  ap northeast c     ami def \n \n\n\n\n\nworkspace\n\n各コンポーネント・ディレクトリ配下に次のつのworkspaceを作成します。\n\n\ncommand\nterraform workspace list\n  default\n develop\n product\n staging\n\n\n\n\nAWS CLI profile\n\nbackendで指定するAWS Sにアクセスするためのprofile情報を設定します。\n\n\n aws credentials\n profile name \naws access key id   AKIAXXXXXXXXXXXXXXXXXX\naws secret access key   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n\n\n\nBackend用AWS S Bucket作成スクリプト\n\nAWS CLIを使用してS Bucketを作成します。\n\n\nbin init s bat\n echo off\n\nset profile name  \nset bucket name  \n\naws s mb s    bucket name   \n   profile  profile name \n\naws sapi put bucket versioning  \n   profile  profile name   \n   bucket  bucket name   \n   versioning configuration Status Enabled\n\naws sapi put bucket encryption  \n   profile  profile name   \n   bucket  bucket name   \n   server side encryption configuration file   config config public access block json\n\naws sapi put public access block  \n   profile  profile name   \n   bucket  bucket name   \n   public access block configuration file   config config public access block json\n\n\n\n\nbin config config public access block json\n \n  Rules     \n  ApplyServerSideEncryptionByDefault    \n  SSEAlgorithm    AES \n  \n   \n \n\n\n\n\nbin config config bucket encryption json\n \n  BlockPublicAcls   true \n  IgnorePublicAcls   true \n  BlockPublicPolicy   true \n  RestrictPublicBuckets   true\n \n\n\n\n\n実行\n\nterraformコマンドを実行する際は以下のようにします。\nコマンドを実装するフォルダに注意してください。\n\n\ncommand bash\n  環境変数の設定を確認\nexport TF VAR aws access key  AKIAXXXXXXXXXXXXXXXXXX \nexport TF VAR aws secret key  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX \necho  TF VAR aws access key\necho  TF VAR aws secret key\n\n   workspaceの選択\nterraform workspace select   default   product   staging   develop  \n\n  初期化\nterraform init\n\n   Dry run\nterraform plan  \n var file        environments common terraform tfvars   \n var file        environments   terraform workspace show  terraform tfvars \n\n\n\n\ncommand Powershell\n  環境変数の設定を確認\n env TF VAR aws access key  AKIAXXXXXXXXXXXXXXXXXX \n env TF VAR aws secret key  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX \n env TF VAR public key path     secret public \n env TF VAR aws access key \n env TF VAR aws secret key \n env TF VAR public key path \nGet ChildItem env \n\n   workspaceの選択\nterraform workspace select   default   product   staging   develop  \n\n  初期化\nterraform init\n\n   Dry run\nterraform plan  \n var file        environments common terraform tfvars   \n var file        environments   terraform workspace show  terraform tfvars \n\n\n\n\nまとめ\n\nこの記事のまとめです。\n\n\n実現したこと\n\n\nコンポーネント分割によって、tfstateファイルで管理するリソースを分割\nWorkspacesを使用して環境 本番、検証、開発 分割\nローカル・モジュールしてリソースを定義\nAWS Sを使用したterraform tfstateの管理とバージョニング\n\n\n\n課題\n\n\n一つのリポジトリに複数のコンポーネントを管理する形になっており、terraform initの度に、実行ディレクトリにproviderツールがインストールされる。\nModule Repogitoryが良くわからない。\n全体的に手探り\n\n\n\n対策\n\n\nコンポーネントごとにリポジトリ設計した方が良いのかもしれない。\nこれは使ってみるしかない。\n 悩ましい \n\n\n\nおわりに\n\n記事を書いている途中にv xからv xにTerraformのバージョンを上げたら、書いていたコードが全然動かなくなってかなり焦りました。terraform upgradeコマンドを実行しましたが、万能ではないようです。\n\nさすがにメジャーバージョンがまだ系だけあって、破壊的な変更というのがあるのですね。\n\nTerraformが持つコアの機能は素晴らしくイメージも付きやすいものですが、実際に触ってみるとHCLの記法や各構築対象サービスのお作法を押さえておく必要があり、最初の壁を乗り越えるまでが大変だという印象を受けます。\nまた、まだツール自体が成長段階ということもあり、次々の新しい記法やお作法が生まれており、そういった情報を把握して追従することにも一手間が生まれそうです。\n\nただ、そのコスト以上に、Terraformから得られるメリットの方が大きいため、もっとTerraformについて知って行きたいと思います。\n\n\n参考\n\n今回の記事を作成するにあたって参考にした情報です。\n\n\nディレクトリ構造\n\n\nTerraform Best Practices in    Qiita\nTerraformにおけるディレクトリ構造のベストプラクティス  Developers IO\nshogomuranushi oreno terraform   GitHub\nDevOpsを支える今話題のHashiCorpツール群についてに登壇してきました てっくぼっと \nTerraform workspaceを利用する。環境ごとのリソース名の分岐など  Goldstine研究所\nTerraformのModuleソースとしてTerraform Enterprise s Private Module Registryを利用する  GMOメディアエンジニアブログ\n\n\n\nTerraformの分割単位\n\n\nterraformはどの単位で分割すべきか  Qiita\nTerraformと変数 variable の話  CUPSULE CLOUD\nFeature  Conditionally load tfvars tf file based on Workspace     GitHub\nVariable Definitions   tfvars  Files   Input Variables   Configuration Language   Terraform by HashiCorp\n\n\n\nStateの管理\n\n\nBackendのSやDynamoDB自体をterraformで管理するセットアップ方法  Qiita\n\n\n\nサンプル\n\n\ncloudposse terraform aws elastic beanstalk application   GitHub\nterraform aws modules terraform aws vpc   GitHub\n\n\n\nTerraform v x\n\n\nterraform vアップデートterraform upgrade terraform checklistサブコマンド実行結果と、ファイルの変更例  Qiita\n\n\n\nAWS S for terraform tfstate\n\n\n独りTerraform研究所   Backendについてドキュメントを読んだり チュートリアルしたり ようへいの日々精進XP\nSの暗号化についてまとめてみた 年月版   本日も乙\n",
      "link": "https://qiita.com/anfangd/items/1b84f69fa2a4f8a29fbc",
      "updated": "2019-07-07 14:26:14"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "TerraformでFargateを扱う際にはまったポイントまとめ",
      "description": "\nはじめに\n\nTerraformでインフラ構築 AWS を勉強していて、Fargateを扱う際にはまったポイントをまとめました。\n\n\nバージョン\n\n\nTerraform v \n\n\n\nはまったポイント\n\n\nエラー内容\n\nError  ClientException  No Fargate configuration exists for given values \n\n\n\n原因\n\nタスク定義でFargate起動タイプを選択している場合、CPU  Memoryは以下の組み合わせから選ばなくてはならないが、\n\n cpu   \n memory   \n\n\nのように設定していた。\n\n\n\n\nCPU value\nMemory value  MiB \n\n\n\n\n    vCPU \n  GB     GB     GB \n\n\n    vCPU \n  GB     GB     GB     GB \n\n\n   vCPU \n  GB     GB     GB     GB     GB     GB     GB \n\n\n   vCPU \nBetween   GB  and   GB  in increments of   GB \n\n\n   vCPU \nBetween   GB  and   GB  in increments of   GB \n\n\n\n\n\n参考\n\nNo Fargate configuration exists for given values\n\n\n指定されたCPUまたはメモリの値が無効\n\n\n\n\n\nエラー内容\n\nError  ClientException  Fargate requires task definition to have execution role ARN to support ECR images \n\n\n\n原因\n\n\n\nexecution role arnを指定する必要があった\n\n\n\n参考\n\nterraformのmoduleで定義したresouseにアクセスするにはoutputしないとダメ\n\n\n\n\n\n\n\nエラー内容\n\nError  InvalidParameterException  Network Configuration must be provided when networkMode  awsvpc  is specified \n\n\n\n原因\n\n\n\nnetwork configurationを設定する必要があった\n\n\n\n参考\n\n\n\n\n\n\nエラー内容\n\nError  Incorrect attribute value type\nInappropriate value for attribute  subnets   element   string required \n\n\n\n解決方法\n\nflattenを用いる\n\n\n参考\n\nTerraform    vpc module v  Inappropriate value for attribute  subnet ids   element   string required    \n\n\n\nその他はまったポイント\n\n\nとでの記述の仕方の違い\n\n\n主にブロックタイプ  attr   … という波括弧で記述する属性 とマップ  attr     key   value  という記述をする属性 ではまりました\n参考\n\n\nTerraformをからに移行するときの注意点とは\n i \n\n\n\n\n\nタイポ\n\n\n疲れていると謎のタイポをしています\n",
      "link": "https://qiita.com/suaaa7/items/5daa014420fdc596eab5",
      "updated": "2019-07-07 10:03:21"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "AWS IoT AnalyticsでCSVインポートの処理を作る",
      "description": "AWS IoT Analyticsをあまり利用したことがなかったので、利用した備忘録。ただ色々といじってみたかったので本来のIoT Analyticsの用途からは離れていることは受け入れる感じで。\n\n\nこんな感じのことをやりたい\n\nこういうのを実現するためにAWS IoT Analyticsは使えるんじゃないかな、と。 向いているとは言っていない \n\n\n\nとあるバケットにCSVファイルが置かれる\nCSVファイル配置をトリガとしてLambdaがキックされ、中身をAWS IoT AnalyticsのChannelに送付する。 この際渡す量をAPI側で調整してあげれば、Pipeline内の処理性能はある一定以上は不要になるので、サイジングをシビアに考える必要がない \nあとはStream処理でPipelineの中でやりたい処理を実施する。 今回はDynamoDBへのデータ格納 \n\n\nPipeline AWS IoT Analytics はVPCの中でLambda処理を動かすことはできないようなので、PipelineのLambdaからVPC内に配置したデータストアへのデータ格納はできないようですが、とりあえずこれができるだけでも色々とメリットを得られるので良さそうです。\n\n\nChannelはデータの受信したデータをバックアップとして保持しておくことが可能。それを用いてPipeline処理を再度実施することもできる。 例えばPipeline処理を更新したので再度同じデータを通しておく、と言うことも可能。この際データストアに保持されるデータは追加されるのではなく上書きされる仕組みも個人的には嬉しい \nPipelineを通したデータの一覧をデータセットとして確認することができる。なんならデータセットを利用して他の処理 AWS Batchなどを使った定期的な重めの処理 を実行することも可能。\n\n\n\n全体の成果物は以下\n\nとりあえず以降に説明する内容の成果物は以下に保存\n\n\n\n\nデータ配置後トリガで起動するLambdaの実装\n\n早速実装に取り掛かります。図で言うところの②のトリガが発生した以降に起動されるLambdaの実装はこんな感じになります。\n\n\ndata sensor py\ndef convert from csv to json file path  bucket name  header False  \n df   pd read csv file path \n tmp json    \n\n if header \n tmp json   df to json orient  records  \n else \n tmp json   df to json orient  values  \n\n result json array     \n for ele json in json loads tmp json  \n logger info ele json \n result json     \n result tmp json     \n result tmp json  s bucket     bucket name\n result tmp json  data     ele json\n\n result json  messageId     str ele json  account number   \n result json  payload     json dumps result tmp json \n result json array append result json \n\n return result json array\n\n\ndef send data lambda event  context  \n\n bucket str   event  Records      s    bucket    name  \n bucket   s client Bucket bucket str \n key   urllib parse unquote plus event  Records      s    object    key    encoding  utf   \n file path   TMP PATH   key\n\n os makedirs os path dirname file path   exist ok True \n bucket download file key  file path \n\n json array   convert from csv to json file path  bucket str  True \n\n for json sub in chunked json array  DATA BATCH SIZE  \n response   iot analytics client batch put message \n channelName    csv import sample channel  \n messages   json sub\n  \n\n return  All data sending finished  \n\n\n\nだいぶ雑なつくりですが、これでChannelに対してCSVデータを送付することができます。今回扱っているデータはElasticsearchのサンプルで使われるデータのaccounts csvを利用しています。こんな感じのデータです。\n\n\naccounts csv\naccount number address age balance city email employer firstname gender lastname state\n Holmes Lane  Brogan amberduke pyrami com Pyrami Amber M Duke IL\n Bristol Street  Dante hattiebond netagy com Netagy Hattie M Bond TN\n Madison Street  Nogal nanettebates quility com Quility Nanette F Bates VA\n Hutchinson Court  Orick daleadams boink com Boink Dale M Adams MD\n\n\n\nこれをそれぞれIoT Analyticsに流し込むことになりますが、IoT Analytics側はデータずつ送付されるとそれだけでPipelineの起動数が多すぎになってしまうため、Batch Sizeを調整してまとめてStream処理を走らせられるようにIoT AnalyticsのPipeline部分に設定をしています。\n\n\n\nこれでデータが送られてきた際、Pipelineにデータずつ流し込んでくれるようになります。\n\n\nPipelineで呼び出されるLambda処理\n\n無事Channelへのデータ送付がされたら、次にPipeline処理が呼ばれます。このようなコードを書きました。\n\n\ncsv importer store py\ndef store data lambda event  context  \n logger info  Start store data      format event  \n\n bucket   s client Bucket event    s bucket   \n key   CONF PATH\n file path   TMP PATH   key\n os makedirs os path dirname file path   exist ok True \n\n bucket download file key  file path \n conf file   open file path   r  \n conf json   json load conf file \n\n logger info conf json \n result json array     \n tmp json     \n\n for ev in event \n tmp json     \n for key  value in ev  data   items   \n if key in conf json keys   \n tmp json conf json key     value\n result json array append tmp json \n\n ddb table   ddb client Table DYNAMODB TABLE \n\n with ddb table batch writer   as batch \n for item json in result json array \n batch put item \n Item item json\n  \n\n return result json array\n\n\n\nこれで実行結果がDynamoDBに格納され、かつIoT Analyticsの時系列DBにも格納されるようになります。\n\n\n実際に実行してみた結果\n\n実際にCSVファイルをSに配置して、この処理を通してみました。IoT Analyticsのデータセット部でクエリを発行することで実施結果が確認できました。 もちろんDynamoDBにもデータは入っていました。 \n\n\n\nDynamoDBにも問題なくデータが格納されています。\n\n\n\nこれで無事に処理を通すことができました。\n\n\nPipelineの再処理を実施したくなったら \n\nもしPippeline内の処理を更新した、などで再度データを流したい場合は、AWS IoT Analyticsでポチポチするだけで実現できます。 もちろんCLIで実施することも可能です \n\n\n\nChannelの右側から「メッセージの再処理」を選択します。\n\n\n\n期間でしか絞れないのが惜しいところですが、ここで期間を指定して、その期間でChannelが受信したメッセージをPipelineに再度流すことができます。\n\nちなみにCLIの場合はstart pipeline reprocessingで実施できます。\n\n\nまとめ\n\n今回はAWS IoT Analyticsが本来期待されている目的とは少し異なる利用法を試してみましたが、IoT ANayticsが持っているメッセージ再処理などの特徴をうまく利用することで、CSVインポート処理を任せてみることもできそうだ、と言うこともわかりました。様々な活用法があるとは思うので、模索していきたいところです。",
      "link": "https://qiita.com/kojiisd/items/fc8ee0b252dc38c53541",
      "updated": "2019-07-07 08:15:40"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Zappa利用時におけるトラブルシューティング集",
      "description": "\nそもそもZappaとは\n\nGithub 英語   \n日本語訳  \n\n\nZappa makes it super easy to build and deploy server less  event driven Python applications  including  but not limited to  WSGI web apps  on AWS Lambda   API Gateway  Think of it as  serverless  web hosting for your Python apps \n\n\nとのこと。FlaskやDjangoなどのWebアプリケーションフレームワークを利用したPythonアプリをAWS Lambdaへデプロイする際にとても役に立つ。\n\n\nZappaによるサーバーレスアプリ開発でのトラブルシューティング集\n\n以下、Zappaによるサーバーレスアプリ開発時に遭遇したトラブル事象とそのシューティング方法を記載していく。\n\n\n  zappaコマンド templateなど を実行するとNoRegionErrorとなる\n\n\n事象\n\n  zappa template dev  l   your lambda arn   r   your role arn \nCalling template for stage dev  \nWarning  AWS Lambda may not be available in this AWS Region \nWarning  AWS API Gateway may not be available in this AWS Region \nOh no  An error occurred    \n\n              \n\nTraceback  most recent call last  \n 中略 \nbotocore exceptions NoRegionError  You must specify a region \n\n              \n\nNeed help  Found a bug  Let us know   D\nFile bug reports on GitHub here  \nAnd join our Slack channel here  \nLove  \n Team Zappa \n\n\n\n対処方法\n\nzappa settings jsonに aws region    aws region name を記載する。\n\n\nzappa settings json\n \n  dev    \n  app function    server app  \n  aws region    ap northeast   \n    \n  \n \n\n\n\n\n  zappa templateコマンドを実行するとAttributeError   ZappaCLI  object has no attribute  apigateway policy となる\n\n\n事象\n\n  zappa template dev  l   your lambda arn   r   your role arn  \nCalling template for stage dev  \nOh no  An error occurred    \n\n              \n\nTraceback  most recent call last  \n File   root  local share virtualenvs application NgYdrrUH lib python site packages zappa cli py   line   in handle\n sys exit cli handle   \n File   root  local share virtualenvs application NgYdrrUH lib python site packages zappa cli py   line   in handle\n self dispatch command self command  stage \n File   root  local share virtualenvs application NgYdrrUH lib python site packages zappa cli py   line   in dispatch command\n json self vargs  json  \n File   root  local share virtualenvs application NgYdrrUH lib python site packages zappa cli py   line   in template\n policy self apigateway policy \nAttributeError   ZappaCLI  object has no attribute  apigateway policy \n\n              \n\nNeed help  Found a bug  Let us know   D\nFile bug reports on GitHub here  \nAnd join our Slack channel here  \nLove  \n Team Zappa \n\n\n\n対処方法\n\nzappaのバージョンを に下げる。\n\n  zappa  v\n \n  pipenv install zappa      skip lock\nInstalling zappa   …\nAdding zappa to Pipfile s  packages …\nInstallation Succeeded \nInstalling dependencies from Pipfile…\n ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉  ー\n\n\n\n参考\n\n\n\n\n  zappa deployやupdateコマンドを実行するとStatus check on the deployed lambda failed  A GET request to     yielded a  response code となる\n\n\n事象\n\n  zappa deploy dev\nCalling deploy for stage dev  \nDownloading and installing dependencies  \n   markupsafe     Using locally cached manylinux wheel\n   sqlite  python  Using precompiled lambda package\nPackaging project as zip \nUploading app dev  zip  MiB   \n  ████████████████████████████████████████████████████████████████████████████████████████████████████████  M M    lt    MB s \nScheduling  \nScheduled app dev zappa keep warm handler keep warm callback with expression rate  minutes  \nUploading app dev template  json  KiB   \n  ████████████████████████████████████████████████████████████████████████████████████████████████████████  K K    lt    KB s \nWaiting for stack app dev to create  this can take a bit   \n  ██████████████████████████████████████████████████████████████████████████████████████████████████████████████       lt    s res \nDeploying API Gateway  \nError  Warning  Status check on the deployed lambda failed  A GET request to     yielded a  response code \n\n\n\n対処方法\n\nコマンドでのデプロイ時に動作確認としてAPI Gatewayの リソースに対してGETリクエストが行われるが、そのリクエストに対するLambdaからのレスポンスがエラーとなっている。\nLambda側の実行ロールの権限やソースコード自体に問題がないか確認をする。\n\n\n  zappa deployやupdateコマンドを実行するとProvided role  role arn  cannot be assumed by principal  events amazonaws com  となる\n\n\n事象\n\n  zappa update dev\nCalling update for stage dev  \nDownloading and installing dependencies  \n   markupsafe     Using locally cached manylinux wheel\n   sqlite  python  Using precompiled lambda package\nPackaging project as zip \nUploading app dev  zip  MiB   \n  ████████████████████████████████████████████████████████████████████████████████████████████████████████  M M    lt    MB s \nUpdating Lambda function code  \nUpdating Lambda function configuration  \nUploading app dev template  json  KiB   \n  ████████████████████████████████████████████████████████████████████████████████████████████████████████  K K    lt    KB s \nWaiting for stack app dev to update  \nres     s res  \nDeploying API Gateway  \nScheduling  \nUnscheduled app dev zappa keep warm handler keep warm callback \nOh no  An error occurred    \n\n              \n\nTraceback  most recent call last  \n 中略 \ncannot be assumed by principal  events amazonaws com  \n\n              \n\nNeed help  Found a bug  Let us know   D\nFile bug reports on GitHub here  \nAnd join our Slack channel here  \nLove  \n Team Zappa \n\n\n\n対処方法\n\nLambda関数の実行ロールとしたいIAMロールの信頼されたエンティティにevents amazonaws comを追加する必要がある。\n\nより、対象のIAMロールで 信頼関係 タブ  信頼関係の編集 から、ポリシードキュメントのevents amazonaws comを追記し、 信頼ポリシーの更新 をクリック。\n\n以下はポリシードキュメントの編集例。\n\n\nPolicyDocument example json\n \n  Version        \n  Statement    \n  \n  Effect    Allow  \n  Principal    \n  Service    \n  lambda amazonaws com  \n  events amazonaws com \n  \n   \n  Action    sts AssumeRole \n  \n  \n \n\n\n\n\n参考\n\n\n\n以上",
      "link": "https://qiita.com/r-wakatsuki/items/fb4e23344a5a89113b72",
      "updated": "2019-07-07 16:09:07"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Sでパブリックアクセス可能なファイルを投稿する",
      "description": "\nはじめに\n\n久しぶりにAWSで新規アプリケーション開発したらハマった。\n\n からSのパブリックアクセス機能がリリースされ、一工夫必要になったらしい。\n\n\nではどうすればいいのかという話。\n\nパブリックアクセス機能についてはこちらを参照。\n\n\n\n設定手順\n\n下記のつのいずれかの設定でパブリックアクセス可能なオブジェクトにできる。\n\n\n公開権限を委任しアップロード時にパブリックアクセスを許可する\n\n権限の委任以外は従来と同じ。\n\n\nパブリックアクセス設定\n\n\n新しいアクセスコントロールリスト ACL を介して許可されたバケットとオブジェクトへのパブリックアクセスをブロックする\n任意のアクセスコントロールリスト ACL を介して許可されたバケットとオブジェクトへのパブリックアクセスをブロックする\n\n\nのつをオフにする。\n\n\n\nあとは、ファイルアップロード時にaclでパブリックアクセスにすればOK。\n\naws s cp  任意のファイル  s   s public access test    acl public read\n\n\n\nバケットポリシーで許可\n\n下記のポリシーで、s public access testバケットのpublicフォルダ以下についてパブリックアクセスを許可している。\n\n\nパブリックアクセス設定\n\n下記のつを許可しておく。\n\n\n新しいパブリックバケットポリシーを介して許可されたバケットとオブジェクトへのパブリックアクセスをブロックする\n任意のパブリックバケットポリシーを介して、バケットとオブジェクトへのパブリックアクセスとクロスアカウントアクセスをブロックする\n\n\n\n\n\nポリシー\n\n下記のポリシーを「アクセス権限」タブ内のバケットポリシーで設定する。\n\n\n \n  Version       \n  Statement   \n  \n  Sid   AddPerm  \n  Effect   Allow  \n  Principal       \n  Action    s GetObject   \n  Resource    arn aws s   s public access test public    \n  \n  \n \n\n\npublicディレクトリ以下にアップロードされたファイルは常にパブリックアクセス許可された状態になる。\n\naws s cp  任意のファイル  s   s public access test public \n\n\n\nバケットポリシー amp タグで制御\n\n特定のタグが付与されたオブジェクトについて、パブリックアクセス許可のポリシーを適用する方法。\n一括で権限を変更できるので一番スマートかもしれない。\n\nタグの利用には別途料金がかかる。\n S Storage Management pricing\n\n\nタグの追加\n\nバケット内の「プロパティ」タブから、タグを追加\n\nkey  public object\nvalue  yes\n\n\n\nパブリックアクセス設定\n\n バケットポリシーで許可 と同じ。\n\n\nポリシー\n\n設定方法は バケットポリシーで許可 と同じ。\n\n \n  Version        \n  Statement    \n  \n  Effect    Allow  \n  Principal       \n  Action    s GetObject  \n  Resource    arn aws s   s public access test    \n  Condition    \n  StringEquals    \n  s ExistingObjectTag public object    yes \n  \n  \n  \n  \n \n\n\n\nコマンド\n\ncp時に付与できないようでちょっと面倒。プログラムでアップロードするなら関係ないかも。\n\n ファイルアップロード\naws s cp  Downloads c jpg s   s public access test public    profile kol \n タグを付与\naws sapi put object tagging   bucket s public access test   key  public  任意のファイル     tagging  TagSet  Key public object Value yes   \n\n\n\n終わりに\n\nAWS Sでパブリックアクセス可能なファイルを投稿するための設定方法を記述した。\nTechブログを書くなら数ヶ月前の自分に向けて書くと良いとどこかで読んだが、マジで自分に届け。",
      "link": "https://qiita.com/R_H7GCH/items/5574776862785269d5aa",
      "updated": "2019-07-07 03:52:52"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "ECで構築したUiPath Orchestratorサーバを、自己署名じゃない証明書でSSLで公開する Certificate ManagerとELBを使用 ",
      "description": "\nイントロ\n\nUiPath Orchestratorを構築するにあたって、久しぶりにECでSSL証明書を取って環境構築したので、そのときの備忘。\n\nタイトルはUiPath OrchestratorサーバをSSLで公開するってなってますが、実際はOrchestratorに特化した話ではなく一般的なECサーバのSSL公開の話って感じです。\n\n\n前提\n\n\nドメインを持っている example xyzとしました \nそのドメインは、AWSのRoute で管理している\nUiPath Orchestratorが、AWS上のECで WindowsサーバのIIS上で ポートですでに動いている\n今時点ではOrchestratorサーバ 以下Orchサーバ へはhttps    IPアドレス  でアクセスしている\n保守のためそのECサーバへはの他RDP ポート やKibana ポート も許可している\nOrchestratorが動くIISサーバは、SSL証明書が自己署名証明書なので、ちゃんとした証明書を用いるようにしたい\n監視とか冗長構成とかは、とりあえず今回は省略\n\n\n記事開始時点を図示するとこんな感じ。\n\n\nさてさて今回、UiPath Orchestratorを自己署名証明書で構築するところは省略していて、すでに構築済みとしています。そのOrchサーバを自己署名じゃない証明書で公開するのに\n\n\n証明書を正規に取得して自己署名証明書から置き換えOrchサーバをそのまま外部に「」などで公開\n\n\nしてもよいのですが、今回は\n\n\nOrchサーバの手前にELB Elastic Load Balancer を置いて、AWS上で無料で取得できるSSL証明書を用いる\n\n\nことにしました。ELBは特定のネットワークからのポートへの接続のみ許可するモノとします。\n図示するとこうです。\n\n\n\nほんとはSSLをほどいてルーティングしたい、、\n\n     追記  \nちなみに本当は、ELBでSSLをほどいてIISにはHTTPのポートで流したかったのですが、UiPath OrchestratorのインストーラがIIS上にSSL付きで構築してくれてしまうのと、さらにはURL Rewrite機能を用いてHTTPアクセスをHTTPSにリダイレクトまでしてくれていて、、、、これらをオフってIIS上にHTTPのOrchestratorを構築する手順が明確ではなかったからです、、、。バインドをに替えて、URL RewriteをすべてDisableにすれば行けるのかもしれませんが未検証ですorz。\n     追記以上  \n\n     追記   \nバインドをポートを追加して、URL RewriteをOFFにしてみたのですが、\n\n\n\n\nおお、Web画面も動くしなんかうまくいってそうな感じ、、、とおもったら、PC上からのロボットトレイからの接続・切断がNGに、、┐     ┌。。\n\n\n検証はまた後日かな、、、。\n     追記以上  \n\n\nやってみる\n\nさて構築の流れとしては、\n\n\nAWSのCertificate ManagerでSSL証明書をあらかじめ取得しておき\nELBが使用するターゲットグループを作成\nELBを構築してSSL証明書を配置\n最後にELBのサーバ名をAWS Route  DNSサービス で名前解決\n\n\nすればよさそうです。ついでに同じELBを用いて\n\n\nURL違いでElasticsearchサーバに処理を転送する設定 別記事 \n\n\nも入れてみます。\n\n\nCertificate ManagerでSSL証明書をあらかじめ取得\n\nAWS Certificate Managerを使って無料でSSL証明書を発行するにまとめました。証明書取得まではそれで行けると思います。\n\n\nELBの構築 の前のターゲットグループ作成 \n\nELBは処理を転送するのに、ターゲットグループというECインスタンスのグルーピングを用います。今回Orchサーバのインスタンスをもつグループ「orch」と、あとで使うElasticsearchサーバのインスタンスをもつ「ela」というグループを作成します。先の図のココです。\n\n\n\nEC  gt  gt ターゲットグループを表示して「ターゲットグループの作成」をクリック。\n\n\n今回はELBから、そのままHTTPS ポート で待ち受けているIISサーバに転送するので、プロトコルはHTTPS、ポートはとします。IISサーバではHTTPで待ち受けている場合は、プロトコルはHTTP、ポートはIISのポート 通常かな を設定します。\n\n\n作成が完了したら「閉じる」をクリック。\n\n\nつづいてターゲットグループにOrchサーバを登録するため「編集」をクリックします。\n\n\nOrchestratorサーバを選択して「登録済みに追加」をクリック。待ち受けるポートをインスタンスごとに個別に指定出来ますが、今回はターゲットグループに指定したポートと同じなのでそのままでOK。\n\n\n上に追加されたので、「保存」をクリック。\n\n\n保存されました。\n\n\n同様に、elaのターゲットグループも作ります。違うところは、ELBがHTTPSで受けた後、こちらのターゲットグループにはHTTPでポートでルーティングしますので、そのように設定します。\n\n\nターゲットグループへのElasticsearchサーバの登録も忘れずに。。\n\n\n保存できました。\n\n\n以上で、各ターゲットグループの作成は完了です。\n\n\nELBの構築 Application Load Balancerの作成 \n\nつづいて、主役のロードバランサの作成です。\n\n\nApplication Load Balancerを選択。\n\n\n今回はHTTPSのみを受け付けるので、HTTPS を選択。アベイラビリティゾーンはとりあえず、自分が持ってるサブネットを指定しておきましょう。\n\n\nSSLの設定です。すでにさきほど作成済みであれば、ココに証明書として出てくるはずなのでそれを選択して次へ。\n\n\nELBが属するセキュリティグループについて、グループはELB用に新規に作成します。このグループは外部からアクセスされるところになるので慎重に。今回はOrchestratorサーバを構築する記事なのであるIPアドレスたちからの接続のみを許可する設定としていますが、広く公開する場合は「任意の場所」を選択しましょう。\n\n\nさて、ルーティングするターゲットグループを指定します。まずはシンプルに、全てのリクエストを「orch」グループにルーティングするように設定します。\nのちに「」へのリクエストはSSLをほどいて別のグループ「ela」にルーティングさせますが、あと回しで。\n\n\nインスタンスとポート番号を確認します。\n\n\n最終確認。よければ「作成」をクリック。\n\n\nおお、作成できたようですね。\n\n\n一覧に戻ってみると、確かにELBというロードバランサが作成できています。下の詳細にDNS名が表示されていますが、このサーバ名がロードバランサに割り振られたサーバ名になります。あとでもっとわかりやすいエイリアスをつけますが、これでアクセスできるようになったということですね。\n\n\nここまででほぼ、ELBの構築は完了です。\n\n\nELBのセキュリティグループ→ECのグループへの通信許可\n\nつづいて上記のセキュリティグループのリンクをクリックして、ELB側でなくECインスタンス達側のセキュリティグループの設定を変更します。\n\n\nこのままではELBからの接続が許可されていないことになるので、ELBのセキュリティグループからEC側のセキュリティグループへ、HTTPSのためのポートと、Elasticsearch Kibanaのためのポートへの通信を許可します。また下記のイメージの下からつめ、元々あったHTTPS への直接通信はホントは削除した方がよいでしょう。ECインスタンス直接アクセスは遮断し、ELBからのアクセスに限定するほうがよいからですね。\n\n\n\nいったん疎通確認\n\nさてさてELBからのルーティングやセキュリティグループによる通信設定もできたので、いったんブラウザでhttps    長いELBのDNS名  へアクセスしてみましょう。\n\n\n\nおなじみのUiPath Orchestrator画面が表示されましたねー。。今のところ、サーバ名が証明書のCommon Nameと不一致のためSSL的にはエラーになってますが、アクセス自体はできるようになりましたね。\n\nいやー記事にすると長くなりました。。\n\n\nELBのサーバ名をAWS Route  DNSサービス で名前解決\n\nさてさて、あとすこし。\n先ほどの長ーいELBのサーバ名を、たとえばorch example xyzとかela example xyzなどでアクセスできるよう、DNSサービスで名前解決します。\n\nRoute にアクセスして、該当ホストゾーンの設定画面を開き「レコードセットの作成」をクリック。\n\n\n名前に「orch」、エイリアスを「はい」にするとエイリアス先に、ELBのレコードが選択出来るようになるので、それを選択し「作成」をクリック。\n\n\nレコードが追加されました。簡単ですね。\n\n\nさてさて「」でアクセスしてみましょう。\n\n\n表示されましたねー。。\nサーバ名がSSL証明書のCommon Name    example xyzと一致しているため、先ほどのELBのサーバ名の時は発生していた証明書が不正っていうエラーも、今回は出ていない状態になりました \n\n最後に、ついでに同じ設定で「ela」というサーバ名も追加しておきます。\n\n\n\n\nElasticsearchサーバのルーティングと疎通\n\nほぼ環境構築は完了です。まだ「へのアクセスは、Elasticsearch KibanaサーバにHTTPでポートへルーティングする」という設定が入ってないのですが、改めて別記事で書こうと思います。\n\n     追記  \n別記事書きました   \nECとELBで、URLごとに別のサーバにルーティングする\n     追記以上  \n\nおつかれさまでした。\n\n\n関連リンク\n\n\nALBのHost based routingを試してみた\nAWS Certificate Managerを使って無料でSSL証明書を発行する\nUiPath Orchestrator向けのElasticsearch Kibanaの構築メモ\nUiPath社ナレッジベース\n【AWS】とっても便利な Amazon Route のエイリアス機能を利用してロードバランサ ELB を設置してみました \nApplication Load Balancerで設定する種類のポート番号の意味を理解しよう\n\nECとELBで、URLごとに別のサーバにルーティングする次の記事\n",
      "link": "https://qiita.com/masatomix/items/e220d09f5de1341e0494",
      "updated": "2019-07-07 13:46:02"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "Datadog入門 AWS IAMロール作成とIntegrationのインストール",
      "description": "\nAWS IAMロールの作成\n\n\n  DatadogのAWS accuntIDを確認する\n\n\n公式のDatadog DocsからAWS accuntIDを控える\n\n\n\n\n\n\n\n\n\n\n\n  AWS External IDを確認する\n\n\nDatadogのマイページから Integrations     Integrations  \n\nAWSアイコンをクリック\n\n configration をクリックし、 AWS External ID を控える\n\n\n\n\n AWS IAM管理画面を開く\n\n\n ロール    ロールの作成 をクリック\n\n 別のAWSアカウント をクリック\n\n\n アカウントID に、上記で控えた「DatadogのAWS accuntID」を入力\nオプション「外部IDが必要」にチェック\n 外部ID に、上記で控えた「AWS External ID」を入力\n 次のステップ アクセス権限 をクリック\n\n\n\n\n ポリシーの作成 をクリック\n\n\n\n  JSON をクリックし、JSONでポリシーを定義。 ポリシーの確認 をクリック\n\n\n\nポリシーの参考\n\n\n\n\n\n\n\n  名前 を入力 例 DatadogAWSIntegrationPolicy し、 ポリシーの作成 をクリック\n\n\n ポリシーの作成が完了\n\n\n 先程作成したポリシー名でフィルタしてチェックする。 次のステップ タグ をクリック\n\n\n  次のステップ 確認 をクリック\n\n\n  名前 を入力 例 DatadogAWSIntegrationRole し、 ロールの作成 をクリック\n\n\n ロールが作成されたことを確認\n\n\n\nDatadog AWS Integrationの設定\n\n\nDatadogのマイページから Integrations     Integrations  \n\n\nAWS accuntIDを入力\nロール名を入力 例 DatadogAWSIntegrationRole \n Install Configuration をクリック\n\n\n\n\n\n反映まで分程かかるまで待つこと。\n\n\n以上。",
      "link": "https://qiita.com/ytayta/items/1c5161c260a42798f7ff",
      "updated": "2019-07-06 22:57:53"
    },
    {
      "name": "AWSタグが付けられた新着投稿 - Qiita",
      "category": "Program",
      "title": "SにPutObjectするPythonプログラムをFargateに乗せるまでの道のり",
      "description": "簡単なPythonプログラムをFargateで実行するまでの道のりです。\n\n\napp py\nimport os\n\nimport boto\n\nbucket   os getenv  BUCKET NAME      \nkey    HelloWorld txt \nbody    Hello  World  \n\ns   boto client  s  \ns put object Bucket bucket  Key key  Body body \n\n\n\n環境変数で渡したバケットに固定文字列のファイルを出力するだけです。\n\n試した環境は\n\n\nWindows環境で実行\nDocker環境で実行\nFargate環境で実行 回だけ実行 \nFargate環境で実行 定期実行 \n\n\n\n\n 更新\nSエンドポイントは不要でしたので記述を削除しました。\n\n\n\n\nWindows環境で実行\n\n\n環境\n\n\nWindows \nPython \n\n\n\nPython仮想環境の作成\n\npython  m venv  venv\n venv Scripts activate\n\n\n\nライブラリーの取得\n\npip install boto awscli\n\n\n\nAWS CLIの設定 認証情報の設定 \n\naws configure\nAWS Access Key ID  None    アクセスキー \nAWS Secret Access Key  None    シークレットアクセスキー \nDefault region name  None    ap northeast などのリージョン \nDefault output format  None  \n\n\n\nバケット名の指定 環境変数 \n\nバケット名は環境変数から取得するようにしたので、環境変数にバケット名をセットします\n\nset BUCKET NAME  バケット名 \n\n\n\n実行 \n\npython  m app\n\n\n無事、Sにファイルが出力されました。\n\n\nDocker環境で実行\n\n\nファイルの用意\n\nrequirements txtとDockerfileを用意し、app pyと同じフォルダーに格納します。\n\n\nrequirements txt\nboto   \n\n\n\n\nDockerfile\nFROM python alpine\n\nWORKDIR  app\nADD    app\n\nRUN python  m pip install  r requirements txt\nCMD   python     m    app  \n\n\n\n\nDockerイメージのビルド\n\ndocker build  t  Dockerイメージのタグ名   \n\n\n\n実行  \n\nコンテナ内にはAWSの認証情報がないので、環境変数で渡します。バケット名も同様です。\n\ndocker run  e AWS ACCESS KEY ID  アクセスキー   e AWS SECRET ACCESS KEY  シークレットアクセスキー   e BUCKET NAME  バケット名   Dockerイメージのタグ名 \n\n\n無事、Sにファイルが出力されました。\n\n\nFargate環境で実行の準備\n\n\n準備 ECRへの登録\n\nFargateで実行するため、DockerイメージをECR Elastic Container Registry に登録します。\n\n\nリポジトリの作成\n\naws ecr create repository   repository name  リポジトリ名 \n\n\n\ndocker loginコマンドの取得と実行\n\naws ecr get login   no include email\n\n\nコンソールに出力されるコマンドを実行します。\n\n\nリモートリポジトリに登録するローカルDockerイメージを指定する\n\ndocker image tag  ローカルのDockerイメージのタグ名   リポジトリ名   リモートのDockerイメージ名 \n\n\n\nDockerイメージをリポジトリにプッシュする\n\ndocker image push  リポジトリ名   リモートのDockerイメージ名 \n\n\n\n準備 ECSのクラスターの作成\n\nマネジメントコンソールでECSの画面を表示。クラスターを作成する。\n\n\n\n\n設定項目\n設定内容\n\n\n\n\nクラスターテンプレートの選択\nネットワーキングのみ\n\n\nクラスターの設定\nこのクラスター用の新しいVPCを作る\n\n\n\n\n\n準備 ECSのタスク定義の作成\n\nマネジメントコンソールでECSの画面を表示。タスク定義を作成する。\n\n\n\n\n設定項目\n設定内容\n\n\n\n\n起動タイプの互換性の選択\nFARGATE\n\n\nタスクとコンテナの定義の設定\n↓↓↓\n\n\nタスク実行ロール\n\nAmazonECSTaskExecutionRolePolicyの他にSへのPutObject権限も必要\n\n\nコンテナの定義\n↓↓↓\n\n\nイメージ\nECRのイメージURI マネジメントコンソールで確認する \n\n\n環境変数\n\nBUCKET NAMEにバケット名を指定\n\n\n\n\n\nFargate環境で実行 回だけ実行 \n\nタスク定義から作成したものを選び、アクションのタスクの実行を行います。\n\n\n\n\n設定項目\n設定内容\n\n\n\n\n起動タイプ\nFARGATE\n\n\nクラスター\n作成したもの\n\n\nタスクの数\n\n\n\nVPC\n作成したもの\n\n\n\n\n\n実行   \n\nウィザードの最後のタスクの実行を押すと、実行されます。\n\n\nFargate環境で実行 定期実行 \n\nクラスターから作成したものを選び、画面下のタブにあるタスクのスケジューリングの先の作成を選びます。\n\n\n\n\n設定項目\n設定内容\n\n\n\n\nスケジュールルールタイプ\nお好みで\n\n\n起動タイプ\nFARGATE\n\n\nタスク定義\n作成したもの\n\n\nタスクの数\n\n\n\nクラスターVPC\n作成したもの\n\n\n\n\n\n実行    \n\nウィザードの最後の作成を押すと、指定したスケジュールに従って実行されます。\n固定された間隔で実行の場合、初回起動は作成を押してから固定された間隔が経過したあとになるようです。\n\n\nまとめ\n\n後半がかなり手抜きになりましたが、一応Fargateで動作するところまでできました。\n\n\n参考サイト\n\nAWS ECS Fargate操作覚え書き",
      "link": "https://qiita.com/moritalous/items/75ab963ec783d58cf685",
      "updated": "2019-07-06 23:17:40"
    }
  ],
  [
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "ダウンロード違法化の審議について一言言っておく",
      "description": "ライセンス違反の静止画像のダウンロードを違法化しようという法律の審議が行われているらしい。\n\n海賊版静止画のDL規制を文化審議会が意見まとめる 朝日新聞デジタル\n\nはっきり言って、これは由々しき事態である。インターネットの利用に大きな制約をかけ、日本の文化を破壊するであろう、最悪の法案であると言える。はっきり言って、このような低レベルな話し合いが行われていること自体に私は憤怒している。最近は多忙のため筆を置いていたのだが、久々に筆をとることにした。この法案の問題をしてきしておかねばならないからだ。\n\n技術的に取り締まりは難しい一技術者としてこれだけは言っておきたい。そもそも、ダウンロード側を意図したものだけ上手く取り締まるような技術は存在しない。\n\nまず、ファイルのダウンロードというが、それは範囲が大きすぎる。多くのウェブページには画像が多数埋め込まれているが、基本的にそれらは画像",
      "link": "http://nippondanji.blogspot.com/feeds/8479308991486700388/comments/default",
      "updated": "2018-12-09 08:47:10"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "そろそろTempTableストレージエンジンについて一言言っておくか",
      "description": "MySQL で内部的に作成されるテンポラリテーブルが、HEAPストレージエンジンからTempTableストレージエンジンへと変更されたことは、皆さんもご存知だろう。このストレージエンジンはテンポラリテーブル専用として設計されたもので、実体を持ったテーブルとしての利用は想定していない。一応、internal tmp mem storage engineオプションを指定することで、従来のHEAPストレージエンジンも選択は可能であるが、個人的にはそれはお勧めしない。\n\n\nTempTableストレージエンジンは、メモリとディスクの両方を自ら使い分ける。これは、従来型のテンポラリテーブルとは違う。HEAPストレージエンジンはインメモリ専用で、tmp table sizeあるいはmax heap table sizeを超えるサイズが必要になると、ディスク上のテーブルへと自動的に変換が行われる",
      "link": "http://nippondanji.blogspot.com/feeds/8108375046647946312/comments/default",
      "updated": "2018-06-20 09:58:25"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "MySQLのZero Dateへの対処法",
      "description": "MySQLのZero Dateへの対処法\n\nMySQLの     は使ってはならない そーだいなるらくがき帳\n\nこのエントリで、MySQLのゼロが含まれる日付け、いわゆるZero Dateについての問題点が色々挙げられているのを見かけたので、手短に対処法を述べておきたい。\n\nZero Dateが存在する理由なぜそんな厄介なデータが存在するのかというのは、開発の経緯や互換性といった深淵な理由からなので気にしないで欲しい。まあ、人間は完璧ではないので、人間が作るプログラムも完璧ではないということだ。\n\n当然ながらSQL標準から外れているものは、例外的な使い方をしたい場合を除き、使うべきではない。アンチパターンも使い方次第という話もあるが、例外的な使い方は基本的に苦労が増えるので使うべきではない。\n\nSQLモード実は、Zero DateはSQLモードで禁止できる",
      "link": "http://nippondanji.blogspot.com/feeds/19789855702330831/comments/default",
      "updated": "2018-05-14 08:50:16"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "MySQL 登場 立ち止まることを知らない進化はこれからも続く。",
      "description": "ゴールデンウィークはいかがお過ごしされただろうか。今年は天気も良く、行楽日和が続いたように思う。\n\nさて、先日MySQL が正式にリリースされた。少し時間が経ってしまったが、今回はMySQL の新機能について紹介したい。コミュニティ版のダウンロードはこちらから可能だ。\n\nひとつ前の正式バージョンはMySQL だったのだが、MySQL は非常に大きなリファクタリングが含まれており、 x台のバージョン番号を捨て去ろうという話があった。そこで、次のメジャーバージョンは最初の桁を増やすということになったのだが、MySQL は過去に既に存在し、買収などの騒ぎで開発が頓挫してしまった経緯がある。 xはMySQL NDB Clusterと被っている。というわけで、のの部分の次という意味合いもあって、というバージョン番号を引っさげ、満を持しての登場となった",
      "link": "http://nippondanji.blogspot.com/feeds/3867520447779580286/comments/default",
      "updated": "2018-05-07 00:01:03"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "労働時間を 減らしてもGDPが 減らない理由",
      "description": "先日、下記のような噴飯物の記事を見かけたので一言言っておきたくなり、久々に筆を執った。\n\n竹中平蔵氏「労働時間の削減ばかりに目を向けているが、労働時間を 減らしてもGDPが 減るだけです」  IT速報\n\n断言するが、労働時間を 減らしてもGDPが 減るということなどあり得ない。世の中こんな考え方をしている経営者が多いんだろうか だとしたら世も末だ。\n労働時間≠GDP労働時間が 減ってもGDPが 減らない最大の理由は、両者が異なるものだということだ。異なるものなのだから同じように変化しないのは当たり前だ。\n\nGDPは経済規模の指標だ。平たく言えば、どれだけ金が動いたのかということだ。例えば小売業を考えてみると、労働時間が長かろうが短かろうが、同じだけの売り上げがあればGDPへの寄与は変化しない。労働者が 短く働くことで売り上げが 減るというような企業は",
      "link": "http://nippondanji.blogspot.com/feeds/176892504078051633/comments/default",
      "updated": "2018-04-09 23:58:43"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "新書籍販売のお知らせ Pro MySQL NDB Cluster",
      "description": "表題の通り、新しい書籍が販売される運びとなった。現在はまだ電子版のみで、紙の書籍は後から遅れて販売される予定である。\n\n\n\nMySQL Clusterは、現在も製品名としてはMySQL Clusterのままであるが、InnoDB Clusterとの区別をはっきりさせるため、マニュアル上ではMySQL NDB Clusterという表記になっている。今回の本のタイトルもその点を考慮し、Pro MySQL NDB Clusterとした。以前書いたMySQL Cluster構築・運用バイブルと比べると、ページ数も格段に増え、さらに対象としているバージョンもからへとアップグレードしているので、現場でMySQL NDB Clusterを扱っている、あるいはこれから扱ってみようと考えている人には、必携の",
      "link": "http://nippondanji.blogspot.com/feeds/8308587442107318754/comments/default",
      "updated": "2017-11-08 00:05:39"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "db tech showcase  tokyoで発表してきました。MySQL NDB Cluster と現在開発中の新機能について dbts",
      "description": "ブログを更新するのは久しぶりである。ここのところ、ずっと缶詰めになってとある書籍を書いていたからだ。書籍は大詰めを迎えており、レビューワー 出版社とのやり取りが続いている。\n\nそんな中、先日db tech showcase tokyoでMySQL NDB Cluster の新機能、そして今後予定されている新機能の中で、すでにDMRにおいて明らかになっているものについて、発表する場を頂いた。新機能の詳細については、以下のスライドを参照して欲しい。\n MySQL Cluster新機能解説 and beyond from Mikiya Okuno \n正直なところ、今回紹介した新機能の中には、とてもワクワクするような派手目のものはない。でも堅実かつ着実に、既存のソフトウェアを改良する内容となっている。とりわけ、MySQL NDB Cluster は、MySQL ",
      "link": "http://nippondanji.blogspot.com/feeds/7331366715705961911/comments/default",
      "updated": "2017-09-07 10:47:41"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "そろそろ「リカンベントそのものが直接の事故原因ではなかったという事実」について一言言っておくか",
      "description": "昨年、漫画家の方がリカンベント型の自転車を運転中に亡くなってしまったということを書いた。私はリカンベントを趣味としているのだが、この事故によってあまりにも「リカンベントは危険だ」というような論調でリカンベント型の自転車を批判するコメントが多かったため、それを弁護したい気持ちがあった。当然、どのような乗り物でも事故は生じるわけであり、リカンベントも例外ではない。だが、先月になって「司法解剖が行われた結果、死因は心筋梗塞だったということが分かった」というニュースが出ている。リカンベント型の自転車による事故ではなく、心筋梗塞の結果事故が起きたというわけなのだ。\n\n酷かった偏見リカンベント型の自転車というのは確かに珍しい。その奇抜なフォルムから注目も集めやすいのだが、至って真っ当な乗り物である。少なくとも普通のアップライト型の自転車と安全性の点で、そう大きな差はないと言える。 その構造上、",
      "link": "http://nippondanji.blogspot.com/feeds/3158295498769289626/comments/default",
      "updated": "2017-04-16 23:00:03"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "フォントは自由に変えられる。だから絵文字で何かを伝えるのはナンセンス。そんなことも分からなかったのか、Googleよ。",
      "description": "Unicodeに絵文字が多数追加されたことは、以前から批判していたのだが、やはりというか何と言うか、しょっぱい問題が起こりつつある。\n\nmacOS SierraやiOS でピストル絵文字が水鉄砲に変わることで起こる問題。  AAPL Ch \n\n絵文字はフォント次第で形が変わる。故にフォントが変わればニュアンスも変わる。自分と相手、あるいは今使っている機種と将来使う機種が同じフォントを使っているとは限らない。だからフォントを変更することで様々な問題が起きるわけである。\n\n根本的な問題 性質が異なるものを混ぜてしまった文字と絵は本質的に性質が違う。\n\n文字はその見た目ではなく、文字を組み合わせた単語、単語を並べた文章によって意味を持つ。フォントが違っても、見た目の違いはあれど、文章そのものの意味は変わらない。どのようなフォントで読んでも意味は通じるのである。\n\nところが、絵文字は",
      "link": "http://nippondanji.blogspot.com/feeds/9013156726230365029/comments/default",
      "updated": "2016-10-26 23:18:55"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "リカンベントは速いのか",
      "description": "先日のエントリの続きだが、リカンベントは速度が出るからスピードが出すぎて危ないというような意見を多く見かけるので、ひとこと行っておく。\n\n言いたいことはひとつだけ。「リカンベントとひとことで言っても、色々なモデルがある。だから必ずしも速いとは限らない。」ということだ。\n\nアップライトの自転車にママチャリからロードバイク、マウンテンバイク、クロスバイクなど色んな種類があるように、リカンベントと言っても様々なタイプのものが存在する。もちろんレース用にチューニングされたものから、街乗り用のコンフォートモデルまで様々だ。前者は最高速度は速いが、後者はそうでもない。むしろアップライトのスポーツ車両よりも遅い場合が多い。\n\nなぜか。\n\nリカンベントが速いとされる理由は、空気抵抗が少ないからだ。空気抵抗を減らすには、身体の進行方向に対する投影面積を小さくしなければならない。そこで重要になるのがシート角",
      "link": "http://nippondanji.blogspot.com/feeds/3535086818556408392/comments/default",
      "updated": "2016-10-25 14:07:36"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "リカンベントは危険な乗り物なのか ",
      "description": "先日、漫画家の方がリカンベント自転車を運転中に亡くなるという事故があった。まずは亡くなった方のご冥福をお祈りしたい。リカンベントでなおかつ漫画家の方の事故という組み合わせは、非常に珍しく、センセーショナルな内容だと思う。この事故を受けて、根拠もなくリカンベントが危険だという印象を持つ方が多数いらっしゃるようなので、警鐘を鳴らしておくことにする。\n\nリカンベントは危険であるともないとも言えないまず最初に私の立場を明確にしておく。私は数年前からリカンベントに乗ることを趣味にしているので、ある程度リカンベントに慣れ親しんでいる。\n\nどのような乗り物についても言えることだが、一件の事故や、見た目の印象をもって危険であるかどうかを判断することは、ただの主観であって何の根拠もない。危険性を科学的に正しく評価するには、例えば少なくとも次のようなことを考慮する必要があるだろう。\n\n事故の発生件数と",
      "link": "http://nippondanji.blogspot.com/feeds/4477572045623121893/comments/default",
      "updated": "2016-10-25 02:20:05"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "私は如何にして詳解MySQL を執筆するに至ったか",
      "description": "先日・・・といっても既に先月のことになるのだが、詳解MySQL 出版記念交流会には多くの方に来て頂いた。台風にも関わらず、とても多くの 名ほど の方に足を運んで頂いたのは、本当に嬉しかった。ここで改めてお礼申し上げる。\n\nありがとうございました  \n\nさて、交流会のときに使用したスライドを公開したので、興味のある方は見ていただきたい。\n\n私は如何にして詳解MySQL を執筆するに至ったかfrom Mikiya Okuno \n少しスライドを補足すると、MySQL の新機能にスポットを当てた本というのは、やはりニッチだと思う。しかし、MySQL を使いこなす上で、このようにまとまった情報は必須・・・というか、無いと話にならないだろうと思っていたので、出版するに至ったことは非常にありがたい。よくこんな企画をよく通してくれたなと・・・。G社さんの反応は、",
      "link": "http://nippondanji.blogspot.com/feeds/7721028280065416365/comments/default",
      "updated": "2016-10-05 23:50:39"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "データベースについてのそもそも論",
      "description": "先月のはじめのほうで、「リレーショナルデータベースとの上手な付き合い方」というタイトルで、回発表をした。ひとつは「まべ てっくVol 」であり、もうひとつは「Hacker Tackle ハカタクル  」である。\n\n「リレーショナルデータベースの開発・運用に纏わるもろもろの話をして欲しい」というような内容の話をしてくれないかという同じような依頼を、ちょうど日違いのイベントで頂いた。 のまべ てっくと、 のHacker Tackleである。そうなると必然的に話す内容も、同じようなものになってくる。同じ人物  私 が話すのだから、テーマも同じで時期も同じであれば、内容が同じようなものになるのが自然である。もし違うものになってしまっているのであれば、片方はウソをついているということになるはずだ。今日は発表に使用したスライドを紹介しつつ、なぜデータベースを使うべきなのか あるいは",
      "link": "http://nippondanji.blogspot.com/feeds/7164574271192485168/comments/default",
      "updated": "2016-10-05 23:28:27"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "MySQL 新機能解説 レプリケーション、セキュリティ編",
      "description": "今月の頭、詳解MySQL の出版記念第一弾として、MyNA 日本MySQLユーザ会 名義でイベントを行ったので、その際使用したスライドを紹介しておこう。今回紹介した新機能のカテゴリはつ。レプリケーションとセキュリティである。レプリケーションはMySQLの運用と切っても切り離せないほど重要なものであり、そしてデータベースサーバーにとってセキュリティが重要であることは、言うまでもないだろう。\n\nレプリケーションWhat s New in MySQL  Replication from Mikiya Okuno \n今回のバージョンでは、レプリケーションが大きく進化している。\n\nまず、MySQL では、レプリケーションのトポロジの限界を打ち破った  MySQL までのバージョンでは、スレーブはひとつのマスターしか持つことができないという制約があったのだが、それが",
      "link": "http://nippondanji.blogspot.com/feeds/6546145693916598569/comments/default",
      "updated": "2016-09-29 08:20:53"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "キーボードの理想形を追い求めて、キーボードについて談義してきた話。",
      "description": "少し古い話になるが、先月LLoT Lightweight Language of Things というイベントでキーボードに関するパネルディスカッションに参加してきたので、ブログでも報告しておこうと思う。\n\nキーボードにこだわろう「キーボードにこだわろう」と題したそのひとコマでは、日本におけるこだわりキーボードメーカーの巨匠であるPFUと東プレから、それぞれパネリストを招待するという豪華な顔ぶれである。これにダイヤテックも加われば完璧だっただろうか。\n\nそんな豪華な顔ぶれの中に、私はErgoDox派として参戦してきた。もう一人のパネリストである前田氏が司会進行を兼ねる形でディスカッションが行われた。前田氏はKinesis Advantageの使い手である。\n\n以下は私が自己紹介とキーボードに対するこだわりなどを語るときに使用したスライドである。興味があれば見てみて欲しい。\n\nとある",
      "link": "http://nippondanji.blogspot.com/feeds/3586293218981627568/comments/default",
      "updated": "2016-09-28 14:10:32"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "MySQL   Development Milestone Release登場  ",
      "description": "先月、詳解MySQL を発刊したばかりであるが、MySQL 自体は去年の月にリリースされたバージョンである。それから約年弱、MySQLは開発の手を緩めること無く日々改良を重ねている。\n\nそう、MySQL の登場である。\n\n現在はDevelopment Milestone Release 通称DMR という状態なので、まだ正式版における機能が固まっている段階ではないという点には注意して欲しい。MySQLの開発プロセスでは、DMRをリリースするごとにその段階で成熟した機能をマージする。DMRを何度かリリースした後に、キリの良いところでリリース候補版となって正式版で追加される機能が一応確定し、その後バグ修正を経て正式版 GA版 がリリースされる予定となっている。詳しくはMySQLのマニュアルを参照して欲しい。\n\nバージョン  の次は誰もがだと思ってい",
      "link": "http://nippondanji.blogspot.com/feeds/209592343395161887/comments/default",
      "updated": "2016-09-22 05:33:41"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "自転車による平地での最高速度の世界記録が更新  Km h  ",
      "description": "先日開催されたWHPSC World Human Powered Speed Challenge において、自転車による平地での最高速度の世界記録が更新されたそうだ。実に時速Kmにも達したのである  高速道路を走る自動車をゆうに凌ぐ速度であり、もしこんなものが公道を走ろうものなら、お巡りさんも真っ青のスピード違反である。以下の動画はKm hのものではないが、WHPSCにおいて、同じ人が日前に走ったときのものであり、記録は Km hとのことだ。\n\n\n\nいかがだろう。風を切る音が既に我々が普段想像する自転車とは程遠いのだが、これは間違いなく、まごうことなき人力 動力を人力に頼った 自転車なのであって、ロケットか何かの類ではない。\n\nなぜこの自転車がこんな形をしているのか。その理由はズバリ、空気抵抗を極限まで減らすためである。空気中を移動する物体は、その",
      "link": "http://nippondanji.blogspot.com/feeds/4807470498765500826/comments/default",
      "updated": "2016-09-19 14:06:21"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "【告知】詳解MySQL 出版記念交流会",
      "description": "来る月日、先月発売した書籍、「詳解MySQL 」の出版記念交流会を開催して頂くことになった。株式会社ネクストビート様より有り難い申し出があり、開催の運びとなった。会場は株式会社ネクストビート様にて。月日 より開催である。詳細は以下のイベントページから確認してもらいたい。\n\n【詳解MySQL 出版記念交流会】一人前になりたい  『超』熟練者から学ぶデータベース運用ノウハウ\n\n現時点ではまだ空きがあるので、都合のつく方はぜひ来ていただきたいと思う。開催日時はシルバーウィーク真っ最中なので行楽に出かける人も多いと思う。リア充には楽しい季節である。そんな中、予定がなくて退屈していた人には朗報だ  ぜひ交流会でギークな人たちと出会って充実した時間を過ごして欲しいと思う。\n\nまだ本書を買っていないという方も、当日は会場で即売も行われるので、安心して欲しい。皆様の",
      "link": "http://nippondanji.blogspot.com/feeds/6775076583569908221/comments/default",
      "updated": "2016-09-07 23:59:55"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "【告知】書籍の販売開始やイベントの登壇予定等",
      "description": "先日ブログで紹介した新書籍が発売されたのと、当面のイベントに登場する予定などについて告知しておこうと思う。まだ空きがあるイベントもあるので、都合がつけばぜひ参加して頂きたい。\n\n詳解MySQL 販売開始\nお待たせしたが、詳解MySQL の販売が開始された。現場でMySQLを使っている人、これからも使い続ける予定のある人は、ぜひ手にとって読んでみて欲しい。\n\n本書はMySQL の新機能を網羅的に解説したものであり、使い方や構造のポイントをそれなりに詳しくは書いているが、ページ数や時間の関係上、徹底的に詳しく解説をしたというわけではない。恐らくは、「こんな上辺だけの解説では満足できない  」という方も多くいらっしゃることだろう。そんな方たちのために、巻末に新機能の一覧をもうけ、新機能の元になったWorkLogやBugのIDの一覧を掲載しておいた。さらに詳しく知りたい人",
      "link": "http://nippondanji.blogspot.com/feeds/2541803556962734586/comments/default",
      "updated": "2016-08-26 09:23:16"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "書籍発行のお知らせ 詳解MySQL 進化したMySQLをよく知るためのテクニカルガイド",
      "description": "既に昨日のdb tech showcaseのスライドでご存じの方も多いだろうが、この度MySQL の新機能を解説するための書籍を発行させていただくこととなった。月日発売予定である。\n\nMySQL の新機能については、これまでブログでは紹介してこなかった。というのも、あまりにもボリュームが多すぎて、ブログという媒体でカジュアルに紹介するには向いていないと思ったからだ。とはいえ、MySQL を皆さんに使っていただくには、誰かが新機能をしっかりと解説しなければならない。どうするべきか考えた結果、書籍としてまとめて出させていただくことになった。\n\n新機能について真面目に解説しようとすると、新しいポイントがどこなのかということを言及するために、結局のところ元々の機能についてもある程度解説が必要になってしまう。そういうわけで、この書籍では、MySQLが持つ機能の基本的な",
      "link": "http://nippondanji.blogspot.com/feeds/5481236634478173979/comments/default",
      "updated": "2016-07-15 08:05:42"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "DB Tech Showcase Tokyo で発表しました。MySQL の新機能InnoDB編",
      "description": "表題の通り、db tech showcase Tokyo にて、MySQL の新機能についての解説を行った。スライドをアップロードしたので、セッションに来てくれた方も、見逃したという方もぜひ見て頂きたい。\n\n What s New in MySQL  InnoDB from Mikiya Okuno \n思えば、年前のdb tech showcaseでMySQL の新機能について解説したときは、回のセッションですべての機能を詳解することができた。ところが、MySQL に至っては、昨年MyNA会でオプティマイザ関連の新機能についての解説を行ったのに続き、今回はInnoDBの新機能だけに的を絞った解説となった。このように小出しにしているのにはワケがある。いや、そもそも小出しにしているというつもりはない。単にMySQL の新機能が多すぎて、一回では",
      "link": "http://nippondanji.blogspot.com/feeds/4534965991198586153/comments/default",
      "updated": "2016-07-14 11:15:10"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "「PCをWinのままにしておきたいのに強制的にWinにするMSが嫌だ Linuxに行く 」という方へLinuxユーザーとして言っておきたいこと",
      "description": "「PCをWinのままにしておきたいのに強制的にWinにするMSが嫌だ Macに行く 」という方へMacユーザーとして言っておきたいこと\n\nという記事を見かけたので、Linuxデスクトップユーザーからも一言だけ言っておく。\n\n結論から。\n\n「悪いことは言わないからやめておけ 」\n\n以前、いますぐWindowsを捨ててデスクトップでGNU Linuxを使う の理由というエントリを書いたことがあるので、使っちゃいけないみたいなことを書くと、「おいおい、いまさら何言ってんだよ」と思われる方も居るかも知れない。だが、以前のエントリの主旨は「GNUのWindows移植版であるCygwinを使うぐらいだったらGNU Linuxはいかが 」という提案をするためのものであり、いわばCygwinを使うようなIT技術者向けのメッセージである。Cygwinが必要だということは、UNIXライクなツール群",
      "link": "http://nippondanji.blogspot.com/feeds/8980995344806081386/comments/default",
      "updated": "2016-03-14 22:00:15"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "企業が導入すべき非喫煙者のための休憩",
      "description": "最近、下記のような記事を目にした。\n\n星野リゾートの喫煙者を採用しない合理性がすごい。社長は見習うべき。 あしみの日記\n\n星野リゾートでは喫煙者を採用しておらず、その理由として作業効率、施設効率、職場環境が挙げられているそうな。 参照 あなたはたばこを吸いますか  星野リゾート採用サイト 理由の正当性については議論しないが、説明文の中に気になる文言があった。\n\n喫煙習慣のある社員には喫煙のための場所が設置され、より頻繁に休憩が認められるということは、喫煙習慣のない社員から見ると不公平に感じる問題です。\n\n予想通りではあるが、このことに対する反応として、非喫煙者にも同等の休憩を認めれば良いというコメントが多数挙がっていた。そうすれば確かに不公平感はない。その意見には納得である。そもそも、人は連続して動き続けるには限界があるため、当然ながら喫煙していようがいまいが、誰にとっても休憩は",
      "link": "http://nippondanji.blogspot.com/feeds/5644661119965985400/comments/default",
      "updated": "2016-03-04 14:49:34"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "書評 ヘルシープログラマすべてのIT屋が全力で反省して読むべき一冊",
      "description": "オライリージャパンより献本御礼。先日、新沼氏とお会いした際に頂いた。本書は実にに素晴らしい内容であり、私を含む、すべてのIT屋が反省して読むべきである。いや、正確には私はもう読んだので、後は反省するだけである。\n\n\n一生現役で居るには\nプログラマに限った話ではないが、ずっと現役で働くには健康でなければならない。身体を壊して入院生活を送らなければならないようでは、労働することはもっての外である。プログラマは高い集中力が求められる職種であるが、目の前のことに集中するという点でも、健康は欠かせない要素である。\n\n以前、アレルギー紫斑病を患ったとき、ヶ月ほど出歩くことが出来なくなってしまった。病気自体も辛かったのだが、それよりも危機感を覚えたのが、出歩かなかったことによる身体の衰えである。筋肉は、動かさないとあっという間に衰えてしまう。一説によると、週間寝たきりになると、 も筋力が低下し",
      "link": "http://nippondanji.blogspot.com/feeds/2388294329375818985/comments/default",
      "updated": "2016-03-02 15:21:33"
    },
    {
      "name": "漢(オトコ)のコンピュータ道",
      "category": "技術系ブログ等",
      "title": "あの超オスもセパレート式キーボードを使ってるらしい ErgoDoxじゃないけど ",
      "description": "超オス。それは漫画、バキシリーズで登場する単語であり、常人離れした、規格外の体格を持った格闘家を指すときに使われる。そんな超オスがIT業界にも存在する。いや、あまりにも有名なので、恐らく業界人であればその名を知らぬ人は居ないだろう。そう、ウェブ魚拓の開発者、新沼大樹氏である。はっきり言って、IT業界で新沼氏を知らなかったらモグリだと言って差し支えない。それどころか、その名はIT業界だけで収まらず、アスリートたちの間でも広まっている。なんせ、握力日本一である。CoCのNo  kg相当 をコンスタントに閉じられるということなので、もしかすると世界一かも知れない。 参考 IRONMAN BLOG 新沼大樹氏、未公開写真、握力王新沼大樹氏テレビ出演  YouTube \n\n実は、そんな新沼氏から衝撃の発言を聞いた。\n\n「私もセパレート式のキーボードを使っています。」\n\n・・・\n\n",
      "link": "http://nippondanji.blogspot.com/feeds/3424927400834969052/comments/default",
      "updated": "2016-02-01 00:02:03"
    }
  ],
  [
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "An eco friendly internet of disposable things is coming",
      "description": "\n\t\nGet ready for a future of disposable of internet of things  IoT  devices  one that will mean everything is connected to networks  It will be particularly useful in logistics  being used in single use plastics in retail packaging and throw away shippers  carboard boxes How it will happen  The answer is when non hazardous  disposable bio batteries make it possible  And that moment might be approaching  Researchers say they re closer to commercializing a bacteria powered miniature battery that they say will propel the IoDT   Learn more  Download a PDF bundle of five essential articles about IoT in the enterprise  \nThe “internet of disposable things is a new paradigm for the rapid evolution of wireless sensor networks   says Seokheun Choi  an associate professor at Binghamton University  in an article on the school s website To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3406462/an-eco-friendly-internet-of-disposable-things-is-coming.html#tk.rss_all",
      "updated": "2019-07-03 12:54:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  “Shift Left  to Push Customer Support into Overdrive",
      "description": "\n\t\nThe “Shift Left  concept is all about efficiency and quality  In software development  shifting left means performing testing early and often in the project lifecycle instead of waiting until the end  By discovering and addressing errors and bugs earlier  teams can ultimately deliver a higher quality product  one that is better aligned with addressing customers  needs In support  it means shifting requests as close to the customer as possible   which includes offering the ability self serve  Moving solutions closer to the operational frontline and to the point of the first issue allows customers get answers quicker and organizations to close tickets faster  There are  big benefits of taking this approach To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405573/shift-left-to-push-customer-support-into-overdrive.html#tk.rss_all",
      "updated": "2019-07-02 13:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  SD WAN Buyers Should Think Application Performance as well as Resiliency",
      "description": "\n\t\nAs an industry analyst  not since the days of WAN Optimization have I seen a technology gain as much interest as I am seeing with SD WANs today  Although full deployments are still limited  nearly every network manager  and often many IT leaders I talk to  are interested in it  The reason for this is two fold   the WAN has grown in importance for cloud first enterprises and is badly in need of an overhaul  This hasn t gone unnoticed by the vendor community as there has been an explosion of companies bringing a broad range of SD WAN offerings to market  The great news for buyers is that there is no shortage of choices  The bad news is there are too many choices and making the right decision difficult To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3406456/sd-wan-buyers-should-think-application-performance-as-well-as-resiliency.html#tk.rss_all",
      "updated": "2019-07-02 10:33:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  How does David battle Goliath  With great strategy and the technology to implement it",
      "description": "\n\t\nThe classic story of David battling Goliath resonates with any successful entrepreneur  At some point  small companies must confront large  entrenched rivals  Those big companies possess clear advantages  brand recognition  economies of scale  financial leverage and many others  Customers need a compelling reason to switch providers How do would be Davids compete  They need to develop their own modernized slingshot  Technology provides virtually endless possibilities for competitive advantage  Like David  though  you need to size up your opponent and adopt the right strategy before choosing your weapon In the United Kingdom  a company called Ocado did just that in the exotic  sophisticated market of …grocery stores To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3406320/how-does-david-battle-goliath-with-great-strategy-and-the-technology-to-implement-it.html#tk.rss_all",
      "updated": "2019-07-01 19:12:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  Three Support Trends to Fuel Business Advantage",
      "description": "\n\t\nWho hasn t had an infuriating support experience as a customer  We ve all been there  Waiting on the phone  Repeating details  Being passed like a hot potato from one agent to the next  Has no one in the universe ever had or dealt with our same issue before  It can be frustrating  to say the least  And we re shell shocked the next time we need to reach out for customer support There s a light at the end of the tunnel  though  Before we get there  let s first consider why this is happening  If we look at the support landscape  we can see that it s changing \nThe technology we use is changing  Think about how technology has changed over the last few years  Ten years ago  we didn t have iPads  Now we have universal connectivity  And behind simple interfaces  underlying technologies are increasingly complex \nThe people we support are changing  ​End users are more mobile  People can work from anywhere  and they take their digital lives with them  ​And with nearly unlimited   and searchable   data  people are consuming information in a variety of ways \nCustomer expectations are changing  The more immediate and available our technology becomes  the more immediate and available we expect support to be  We expect to be able to reach out on any channel we choose   email  phone  twitter  snapchat  and so on  And we expect it to be easy \n\nThese changes are impacting help desks  Fifty five percent of organizations saw increased ticket volume in   Unfortunately  budgets aren t increasing accordingly To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405855/three-support-trends-to-fuel-business-advantage.html#tk.rss_all",
      "updated": "2019-07-01 14:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Tempered Networks simplifies secure network connectivity and microsegmentation",
      "description": "\n\t\nThe TCP IP protocol is the foundation of the internet and pretty much every single network out there  The protocol was designed  years ago and was originally only created for connectivity  There s nothing in the protocol for security  mobility  or trusted authentication The fundamental problem with TCP IP is that the IP address within the protocol represents both the device location and the device identity on a network  This dual functionality of the address lacks the basic mechanisms for security and mobility of devices on a network This is one of the reasons networks are so complicated today  To connect to things on a network or over the internet  you need VPNs  firewalls  routers  cell modems  etc  and you have all the configurations that come with ACLs  VLANs  certificates  and so on  The nightmare grows exponentially when you factor in internet of things  IoT  device connectivity and security  It s all unsustainable at scale To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405853/tempered-networks-simplifies-secure-network-connectivity-and-microsegmentation.html#tk.rss_all",
      "updated": "2019-07-01 11:52:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  Mobile Support  Are Your Employees Getting What They Need ",
      "description": "\n\t\nWe re going to go out on a limb and say most employees bring their own mobile devices to work  You probably do too  and it s not a bad idea  Using a smartphone or tablet you re already familiar with   and to which you re already attached at the hip   often helps you to be more productive as an employee  For companies  embracing the bring your own device trend isn t a bad idea either  That individual boost in productivity compounds across the enterprise But that doesn t mean it s as easy as turning on the green light and letting employees have at it  Those mobile devices need to be supported   and at a high service level end users have come to expect   or companies risk taking a hit on employee productivity and job satisfaction  You can thank places like Apple s Genius Bar for raising the bar on support expectations  That s the same level of first class support employees expect at work and the standard IT support teams are being held to today To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405881/mobile-support-are-your-employees-getting-what-they-need.html#tk.rss_all",
      "updated": "2019-06-28 15:45:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Cisco sounds warning on  critical security patches for DNA Center",
      "description": "\n\t\nCisco issued three “critical  security warnings for its DNA Center users   two having a Common Vulnerability Scoring System rating of  out of  The two worst problems involve Cisco Data Center Network Manager  DCNM   Cisco DNA Center controls access through policies using Software Defined Access  automatically provision through Cisco DNA Automation  virtualize devices through Cisco Network Functions Virtualization  NFV   and lower security risks through segmentation and Encrypted Traffic Analysis \nMore about SD WAN\n How to buy SD WAN technology  Key questions to consider when selecting a supplier \n How to pick an off site data backup method \n SD Branch  What it is and why you ll need it \n What are the options for security SD WAN  \n\n\nIn one advisory Cisco said a vulnerability in the web based management interface of DCNM could let an attacker obtain a valid session cookie without knowing the administrative user password by sending a specially crafted HTTP request to a specific web servlet that is available on affected devices  The vulnerability is due to improper session management on affected DCNM software To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405901/cisco-sounds-warning-on-3-critical-security-patches-for-dna-center.html#tk.rss_all",
      "updated": "2019-06-28 14:36:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Undo releases Live Recorder  for Linux debugging",
      "description": "\n\t\nLinux debugging has taken a giant step forward with the release of Live Recorder  from Undo  Just released on Wednesday  this product makes debugging on multi process systems significantly easier  Based on flight recorder technology  it delves more deeply into processes to provide insight into what s going on within each process  This includes memory  threads  program flow  service calls and more  To make this possible  Live Recorder  s record  replay and debugging capabilities have been enhanced with the ability to \nRecord the exact order in which processes altered shared memory variables  It is even possible to zero in on specific variables and skip backward to the last line of code in any process that altered the variable \nExpose potential defects by randomizing thread execution to help reveal race conditions  crashes and other multi threading defects \nRecord and replay the execution of individual Kubernetes and Docker containers to help resolve defects faster in microservices environments \n\nThe Undo Live Recorder enables engineering teams to record and replay the execution of any software program    no matter how complex    and to diagnose and fix the root cause of any issue in test or production To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405584/undo-releases-live-recorder-50-for-linux-debugging.html#tk.rss_all",
      "updated": "2019-06-28 13:58:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Seagate  Cloudian partner for high density storage as a service",
      "description": "\n\t\nData storage software vendor Cloudian has teamed up with Seagate Technology to offer a private cloud storage platform aimed at artificial intelligence  AI  and network edge workloads  The two companies said they plan to deliver exabyte scale private cloud storage on premises while still compatible with Amazon Web Services  S storage The new product is a mouthful and one only lawyers could have come up with  Cloudian HyperStore Xtreme  Powered by Seagate  Cloudian specializes in object storage platforms  which are already compatible with AWS S  and Seagate is a major provider of hard disk technology along with Western Digital  In announcing the deal  Seagate said S was the motivator for making the alliance To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405877/seagate-cloudian-partner-for-high-density-storage-as-a-service.html#tk.rss_all",
      "updated": "2019-06-28 10:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "IoT roundup  Robot boats  AT T makes IoT partner deals",
      "description": "\n\t\nThere s plenty of IoT technology coming into the automotive sector   sophisticated fleet management systems  in car entertainment and connectivity   but the real pot of gold is fully autonomous transport  which is inching closer all the time One piece of news on that front comes out of MIT  where researchers announced earlier this month that they are collaborating with the Amsterdam Institute for Advanced Metropolitan Solutions to create a “roboat   which leverages GPS  cameras and other sensors  alongside on board connectivity and compute  to create autonomous boats for travel along the Dutch capital s  canals To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405521/iot-roundup-robot-boats-atandt-makes-iot-partner-deals.html#tk.rss_all",
      "updated": "2019-06-28 04:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "HPE promises   reliability with its new storage system",
      "description": "\n\t\nHewlett Packard Enterprise has multiple enterprise class storage choices  offering products under the Nimble  PAR  and ProLiant brands  plus the enterprise storage software of InfoSight  developed by Nimble  You can add Primera to that list  a new high end storage array that s billed as a self managing platform that uses AI techniques to deliver   reliability guaranteed The Primera offering borrows some technology from the company s existing products  including the InfoSight AI capabilities developed by Nimble and the underlying distributed storage architecture of PAR   Read also  What is NVMe  and how is it changing enterprise storage   Get regularly scheduled insights  Sign up for Network World newsletters  \nPrimera was announced last week at HPE s Discover event in Las Vegas  Phil Davis  chief sales officer for HPE  said in the announcement keynote  “If you think about traditional storage  it s full of compromises and complexity  Do I want fast or reliable  Do I want agility or simplicity  But not any more  We re going to combine the simplicity of Nimble with the intelligence of Infosight and mission critical heritage of Par and we ve created a new class of storage that eliminates the traditional compromises and truly redefines what is possible with storage  To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405583/hpe-promises-100-reliability-with-its-new-storage-system.html#tk.rss_all",
      "updated": "2019-06-27 19:45:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "IDG Contributor Network  With net neutrality repealed  how blockchain based networking can protect online privacy",
      "description": "\n\t\nIn today s digital everything world where so much of our personal and professional lives are online  most people seem to have less of an expectation of data privacy than ever before  They know that companies  and governments  are mining their data  but after all of serious breaches of trust  it s no wonder that a majority are now becoming wary of both their data s security and how it is used In fact  a study by Harris and Finn Partners found that Americans are more concerned with data privacy than they are with job creation  That s a shocking finding  but maybe it shouldn t come as that big of a surprise  From the Facebook data scandal  to the horrendous number of retail data breaches to the serious security threats that even come with using public WiFi  nothing seems safe  And now with the repeal of net neutrality  even our internet service providers are free to collect and sell our browsing data To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405520/with-net-neutrality-repealed-how-blockchain-based-networking-can-protect-online-privacy.html#tk.rss_all",
      "updated": "2019-06-27 17:55:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  Disruption vs integration  The case against “rip and replace ",
      "description": "\n\t\nHow do you deal with change  It s one of the questions of our era We can see technology disruption play out in a number of industries like computing where the mainframe gave way to the PC that gave way to the smartphone  Change sneaks up on you and before you can craft a response  those new technologies disrupt your business Networking has seen plenty of changes over the years and when they arrive they often look disruptive at the outset  When I talk to customers about SD WAN  some ask whether SD WAN is so disruptive they can essentially replace their MPLS backbone with a mix of broadband providers plus a commodity box  and essentially rely on the public Internet as their backbone  Some of them even have one of those boxes they are experimenting with To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405836/disruption-vs-integration-the-case-against-rip-and-replace.html#tk.rss_all",
      "updated": "2019-06-27 17:18:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Mist s AI driven virtual assistant speeds up network troubleshooting",
      "description": "\n\t\nThe use of digital or virtual assistants and chatbots has picked up momentum with the rise of artificial intelligence  AI   These automated assistants have been around for years  but they haven t been all that useful  as they required a significant amount of programming to look for certain keywords and then the responses were based on logical guesses  The infusion of AI  however  has made these systems much smarter and now natural language queries can be made and responses are in plain English  or other languages  Many businesses have adopted digital assistants and chatbots to improve customer service  For example  Atom Tickets uses conversational AI to enable people to book movie tickets and even dinner with just a short sentence instead of having to go through the rigmarole of going back and forth with discrete commands To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405437/mists-ai-driven-virtual-assistant-speeds-up-network-troublshooting.html#tk.rss_all",
      "updated": "2019-06-27 13:59:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "SoftBank plans drone delivered IoT and internet by ",
      "description": "\n\t\nA Japanese telecommunications giant and a California based drone builder intend to start a drone delivered internet service by   The two companies  Softbank and AeroVironment  say they ve assembled the first one already  according to materials  pdf  on SoftBank s website in April The HAWK drone has a wingspan of  feet and is powered by solar panels mounted on the wings that drive  electric motors  The unmanned aerial vehicle  UAV  will fly in the stratosphere at  feet above sea level   miles   AeroVironment says in a press release  That s around twice the altitude that many passenger planes fly at To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405170/softbank-plans-drone-delivered-iot-and-internet-by-2023.html#tk.rss_all",
      "updated": "2019-06-27 11:53:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Oracle does in Dyn  resets DNS services to cloud",
      "description": "\n\t\nSome may call it a normal  even boring course of vendor business operations but others find it a pain the rump or worse That about sums up the reaction to news this week that Oracle will end its Dyn Domain Name System enterprise services by  and try to get customers to move to DNS services provided through Oracle Cloud \nMore about DNS \n DNS in the cloud  Why and why not \n DNS over HTTPS seeks to make internet use more private \n How to protect your infrastructure from DNS cache poisoning\n ICANN housecleaning revokes old DNS security key \n\n\nOracle said that since its acquisition of Dyn in  and the ensuing acquisition of Zenedge  its engineering teams have been working to integrate Dyn s products and services into the Oracle Cloud Infrastructure platform  “Enterprises can now leverage the best in class DNS  web application security  and email delivery services within Oracle Cloud Infrastructure and enhance their applications with a comprehensive platform to build  scale  and operate their cloud infrastructure   according to Oracle s FAQ on the move   As a result  Dyn legacy Enterprise services are targeted to be retired on May    with the exception of Internet Intelligence  To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405181/oracle-does-in-dyn-resets-dns-services-to-cloud.html#tk.rss_all",
      "updated": "2019-06-26 17:41:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Where are all the IoT experts going to come from ",
      "description": "\n\t\nIf the internet of things  IoT  is going to fulfill its enormous promise  it s going to need legions of smart  skilled  trained workers to make everything happen  And right now  it s not entirely clear where those people are going to come from That s why I was interested in trading emails with Keith Flynn  senior director of product management  R amp D at asset optimization software company AspenTech  who says that when dealing with the slew of new technologies that fall under the IoT umbrella  you need people who can understand how to configure the technology and interpret the data  Flynn sees a growing need for existing educational institutions to house IoT specific programs  as well as an opportunity for new IoT focused private colleges  offering a well  ounded curriculumTo read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3404489/where-are-all-the-iot-experts-going-to-come-from.html#tk.rss_all",
      "updated": "2019-06-26 15:41:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Tracking down library injections on Linux",
      "description": "\n\t\nWhile not nearly commonly seen on Linux systems  library  shared object files on Linux  injections are still a serious threat  On interviewing Jaime Blasco from AT amp T s Alien Labs  I ve become more aware of how easily some of these attacks are conducted In this post  I ll cover one method of attack and some ways that it can be detected  I ll also provide some links that will provide more details on both attack methods and detection tools  First  a little background    Two Minute Linux Tips  Learn how to master a host of Linux commands in these  minute video tutorials   \nShared library vulnerability\nBoth DLL and  so files are shared library files that allow code  and sometimes data  to be shared by various processes  Commonly used code might be put into one of these files so that it can be reused rather than rewritten many times over for each process that requires it  This also facilitates management of commonly used code To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3404621/tracking-down-library-injections-on-linux.html#tk.rss_all",
      "updated": "2019-06-26 15:08:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Juniper s Mist adds WiFi   AI based cloud services to enterprise edge",
      "description": "\n\t\nMist  now a Juniper Networks company  has rolled out an artificial intelligence  cloud based appliance and a WiFi  access point that together aim at helping users deploy smart  high density wireless networks Leading the rollout is the Mist Edge appliance that extends Mist s cloud services to the branch and lets enterprises manage the distributed Wi Fi infrastructure from a central location  \nMore about ax  Wi Fi  \n Why ax is the next big thing in wireless \n FAQ  ax Wi Fi \n Wi Fi   ax  is coming to a router near you\n Wi Fi  with OFDMA opens a world of new wireless possibilities \n ax preview  Access points and routers that support Wi Fi  are on tap\n\n\nThe Mist Edge device features the company s artificial intelligence engine that helps automate tasks such as adjusting Wi Fi signal strength and troubleshooting  According to Mist  some other potential use cases for Mist Edge include To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405123/juniper-s-mist-adds-wifi-6-ai-based-cloud-services-to-enterprise-edge.html#tk.rss_all",
      "updated": "2019-06-26 13:30:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": " HCI startups bucking the consolidation trend",
      "description": "The rapidly growing hyperconverged infrastructure industry   IDC says converged systems revenue grew  percent year over year in the last quarter of    is starting to consolidate  with tech giants HPE  Juniper Networks  Cisco and Red Hat all buying promising HCI startups To read this article in full  please click here Insider Story ",
      "link": "https://www.networkworld.com/article/3405162/10-hci-startups-bucking-the-consolidation-trend.html#tk.rss_all",
      "updated": "2019-06-26 10:23:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Extreme targets cloud services  SD WAN  WiFi  with  M Aerohive grab",
      "description": "\n\t\nExtreme Networks opened the checkbook again this week to the tune of   million for wireless networking vendor Aerohive The move will bring to Extreme Aerohive s wireless networking technology   including its new WiFi  gear  SD WAN software and cloud management services  \nSee reviews of free  open source network monitoring tools\n Icinga  Enterprise grade  open source network monitoring that scales \n Nagios Core  Network monitoring software with lots of plugins  steep learning curve \n Observium open source network monitoring tool  Won t run on Windows but has a great user interface \n Zabbix delivers effective no frills network monitoring \n\n\nGartner wrote   Aerohive s wireless focused access network portfolio comprises stand alone and stackable campus switches  access points and branch office routers  with a cloud managed distributed control architecture  Organizations typically employ Aerohive s HiveManager network management platform  which manages Wi Fi  Switching  SD WAN  and NAC  as a public or private cloud  although it also may be deployed on premises  To manage a multivendor unified access network  the enterprise can use HiveManager to configure  provision and monitor Aerohive APs in conjunction with switches from Aerohive or with N Series switches from OEM partner Dell EMC   The company also has a relationship with Juniper to managed wired devices To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3405440/extreme-targets-cloud-services-sd-wan-wifi-6-with-210m-aerohive-grab.html#tk.rss_all",
      "updated": "2019-06-26 09:34:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "BrandPost  You Can t Afford Not to Use a Business Driven SD WAN",
      "description": "\n\t\nDigital transformation and cloud initiatives are changing the way IT organizations are thinking about and architecting the wide area network  It is estimated that over  percent of applications have already moved to the cloud  Yet  the transformational promise of the cloud is falling short as conventional networks can t keep pace with demands of the cloud  Why  Because today s router centric and basic SD WAN architectures have either hit the wall or can t keep up with traffic pattern shifts  distributed applications and the open security perimeters inherent to the cloud  This blog will explore the limitations of today s WAN approaches  offering a better way forward with a business first networking model To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3404618/you-can-t-afford-not-to-use-a-business-driven-sd-wan.html#tk.rss_all",
      "updated": "2019-06-25 10:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": " steps to enhance IoT security",
      "description": "\n\t\nOne of the biggest concerns with the Internet of Things  IoT  is making sure networks  data  and devices are secure  IoT related security incidents have already occurred  and the worries among IT  security and networking managers that similar events will take place are justified “In all but the most restrictive environments  you re going to have IoT devices in your midst   says Jason Taule  vice president of standards and CISO at security standards and assurance company HITRUST   The question then isn t if  but how you are going to allow such devices to connect to and interact with your networks  systems and data  To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3404198/7-things-you-can-do-to-enhance-iot-security.html#tk.rss_all",
      "updated": "2019-06-25 04:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "IDG Contributor Network  Software Defined Perimeter  SDP   The deployment",
      "description": "\n\t\nDeploying zero trust software defined perimeter  SDP  architecture is not about completely replacing virtual private network  VPN  technologies and firewalls  By and large  the firewall demarcation points that mark the inside and outside are not going away anytime soon  The VPN concentrator will also have its position for the foreseeable future A rip and replace is a very aggressive deployment approach regardless of the age of technology  And while SDP is new  it should be approached with care when choosing the correct vendor  An SDP adoption should be a slow migration process as opposed to the once off rip and replace  As I wrote about on Network Insight  Disclaimer  the author is employed by Network Insight   while SDP is a disruptive technology  after discussing with numerous SDP vendors  I have discovered that the current SDP landscape tends to be based on specific use cases and projects  as opposed to a technology that has to be implemented globally  To start with  you should be able to implement SDP for only certain user segments To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3404477/software-defined-perimeter-sdp-the-deployment.html#tk.rss_all",
      "updated": "2019-06-21 17:11:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Carrier services help expand healthcare  with G in the offing",
      "description": "There are connectivity options aplenty for most types of IoT deployment  but the idea of simply handing the networking part of the equation off to a national licensed wireless carrier could be the best one for certain kinds of deployments in the medical field To read this article in full  please click here Insider Story ",
      "link": "https://www.networkworld.com/article/3403366/carrier-services-help-expand-healthcare-with-5g-in-the-offing.html#tk.rss_all",
      "updated": "2019-06-21 04:00:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Several deals solidify the hybrid cloud s status as the cloud of choice",
      "description": "\n\t\nThe hybrid cloud market is expected to grow from   billion in  to   billion by   at a Compound Annual Growth Rate  CAGR  of   during the forecast period  according to Markets and Markets The research firm said the hybrid cloud is rapidly becoming a leading cloud solution  as it provides various benefits  such as cost  efficiency  agility  mobility  and elasticity  One of the many reasons is the need for interoperability standards between cloud services and existing systems Unless you are a startup company and can be born in the cloud  you have legacy data systems that need to be bridged  which is where the hybrid cloud comes in To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3403354/several-deals-solidify-the-hybrid-clouds-status-as-the-cloud-of-choice.html#tk.rss_all",
      "updated": "2019-06-20 18:06:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Disposable   IoT satellites could swarm Earth s orbit",
      "description": "\n\t\nTiny cheap satellites  self organizing and communicating as a group  could shift the internet of things  IoT  to space  The postage stamp sized devices  acting as sensors  just like the ones we see in traditional IoT networks could perform tasks such as mapping or studying Earth  say scientists involved in a recent successful launch of the disposable nanosatellites The test satellites  essentially just sensors  were deployed in a batch in March  They captured data  communicated with one another  and then after a couple of days in orbit  as was planned  burned up as they reentered the atmosphere   IoT in the enterprise  Download a PDF bundle of five essential articles about IoT in the enterprise  \n“This is like the PC revolution for space   says Zac Manchester  an assistant professor at Stanford University  in an article on the school s website  Manchester invented the ChipSats  years ago  It has taken until now  and after a failed attempt in   to get the constellation operationalーif just for those few days To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3403364/disposable-100-iot-satellites-could-swarm-earths-orbit.html#tk.rss_all",
      "updated": "2019-06-20 15:29:00"
    },
    {
      "name": "Network World",
      "category": "海外TECH",
      "title": "Cracks appear in Intel s grip on supercomputing",
      "description": "\n\t\nIt s June  so it s that time again for the twice yearly Top  supercomputer list  where bragging rights are established or  in most cases  reaffirmed  The list constantly shifts as new trends appear  and one of them might be a break in Intel s dominance Supercomputers in the top  list include a lot of IBM Power based systems  and almost all run Nvidia GPUs  But there s more going on than that For starters  an ARM supercomputer has shown up  at    Astra at Sandia National Laboratories is an HPE system running Cavium  now Marvell  ThunderX processors  It debuted on the list at   last November  but thanks to upgrades  it has moved up the list  It won t be the last ARM server to show up  either To read this article in full  please click here",
      "link": "https://www.networkworld.com/article/3403443/cracks-appear-in-intels-grip-on-supercomputing.html#tk.rss_all",
      "updated": "2019-06-20 10:00:00"
    }
  ],
  [
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Ultra small nanoprobes could be a leap forward in high resolution human machine interfaces",
      "description": "Machine enhanced humans    or cyborgs as they are known in science fiction    could be one step closer to becoming a reality  thanks to new research \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/oPw-10OwALk/190703121358.htm",
      "updated": "2019-07-03 16:13:58"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Neural nets to simulate molecular motion cast",
      "description": "New work is showing that artificial neural nets can be trained to encode quantum mechanical laws to describe the motions of molecules  supercharging simulations potentially across a broad range of fields \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/Y6MnbYs-gas/190702184603.htm",
      "updated": "2019-07-02 22:46:03"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "A short bout of exercise enhances brain function",
      "description": "Neuroscientists  working with mice  have discovered that a short burst of exercise directly boosts the function of a gene that increases connections between neurons in the hippocampus  the region of the brain associated with learning and memory \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/ZOpvqN9c3_g/190702184555.htm",
      "updated": "2019-07-02 22:45:55"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Atmosphere of midsize planet revealed by Hubble  Spitzer",
      "description": "Two NASA space telescopes have identified the detailed chemical  fingerprint  of a planet between the sizes of Earth and Neptune  No planets like this can be found in our own solar system  but they are common around other stars \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/LwrWkNyPjN4/190702164603.htm",
      "updated": "2019-07-02 20:46:03"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Using artificial intelligence to better predict severe weather",
      "description": "When forecasting weather  meteorologists use a number of models and data sources to track shapes and movements of clouds that could indicate severe storms  However  with increasingly expanding weather data sets and looming deadlines  it is nearly impossible for them to monitor all storm formations    especially smaller scale ones    in real time \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/oCCE8B_pqEg/190702160115.htm",
      "updated": "2019-07-02 20:01:15"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "HIV eliminated from the genomes of living animals",
      "description": "Researchers have for the first time eliminated replication competent HIV  DNA    the virus responsible for AIDS    from the genomes of living animals  The study marks a critical step toward the development of a possible cure for human HIV infection \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/UHq2hv5-sL0/190702112844.htm",
      "updated": "2019-07-02 15:28:44"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "The neuroscience of autism  New clues for how condition begins",
      "description": "Scientists have uncovered details of a key cellular mechanism crucial for proper brain development  It involves a gene that  when mutated  had previously been linked to the development of autism \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/Aye-9kTp9m8/190702112842.htm",
      "updated": "2019-07-02 15:28:42"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Wood products mitigate less than one percent of global carbon emissions",
      "description": "The world s wood products    all the paper  lumber  furniture and more    offset just one percent of annual global carbon emissions by locking away carbon in woody forms  according to new research \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/tBUreZbbfgY/190701163837.htm",
      "updated": "2019-07-01 20:38:37"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "How you and your friends can play a video game together using only your minds",
      "description": "Researchers have created a method for two people to help a third person solve a task using only their minds \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/-rW29xZtqsk/190701163827.htm",
      "updated": "2019-07-01 20:38:27"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Physicists use light waves to accelerate supercurrents  enable ultrafast quantum computing",
      "description": "Scientists have discovered that terahertz light    light at trillions of cycles per second    can act as a control knob to accelerate supercurrents  That can help open up the quantum world of matter and energy at atomic and subatomic scales to practical applications such as ultrafast computing \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/BoQ5jI6ydkk/190701144322.htm",
      "updated": "2019-07-01 18:43:22"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Evolution of life in the ocean changed  million years ago",
      "description": "New research identifies a previously overlooked global event which changed the course of the evolution of life in the oceans  It coincided with a rise in calcium carbonate secreting plankton and their subsequent deposition on the ocean floor \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/JpZ58oCrHJg/190701143804.htm",
      "updated": "2019-07-01 18:38:04"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Pig Pen effect  Mixing skin oil and ozone can produce a personal pollution cloud",
      "description": "When ozone and skin oils meet  the resulting reaction may help remove ozone from an indoor environment  but it can also produce a personal cloud of pollutants that affects indoor air quality  according to a team of researchers \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/zgdIzz8vSB0/190628120533.htm",
      "updated": "2019-06-28 16:05:33"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "When the dinosaurs died  lichens thrived",
      "description": "When the asteroid hit  dinosaurs weren t the only ones that suffered  Clouds of ash blocked the sun and cooled the planet s temperature  devastating plant life  But fungi  which decompose dead stuff  did well  So what happened to the lichens  which are made of a plant and fungus living together as one organism \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/vH4DQia6tcE/190628120432.htm",
      "updated": "2019-06-28 16:04:32"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Researchers decipher the history of supermassive black holes in the early universe",
      "description": "Astrophysicists have found evidence for the direct formation of black holes that do not need to emerge from a star remnant  The production of black holes in the early universe  formed in this manner  may provide scientists with an explanation for the presence of extremely massive black holes at a very early stage in the history of our universe \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/OkVB7mM70EI/190628120422.htm",
      "updated": "2019-06-28 16:04:22"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Space station mold survives high doses of ionizing radiation",
      "description": "The International Space Station  like all human habitats in space  has a nagging mold problem  Astronauts on the ISS spend hours every week cleaning the inside of the station s walls to prevent mold from becoming a health problem  New research finds mold spores may also survive on the outside walls of spacecraft \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/MmrpAAHQhm0/190627121252.htm",
      "updated": "2019-06-27 16:12:52"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "NASA s TESS mission finds its smallest planet yet",
      "description": "NASA s Transiting Exoplanet Survey Satellite  TESS  has discovered a world between the sizes of Mars and Earth orbiting a bright  cool  nearby star  The planet  called L  b  marks the tiniest discovered by TESS to date \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/tYBZZ4CnaPI/190627114113.htm",
      "updated": "2019-06-27 15:41:13"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Some extinct crocs were vegetarians",
      "description": "Based on careful study of fossilized teeth  scientists have found that multiple ancient groups of crocodyliforms    the group including living and extinct relatives of crocodiles and alligators    were not the carnivores we know today  Evidence suggests that a veggie diet arose in the distant cousins of modern crocodylians at least three times \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/jVMIPPzdOAE/190627113958.htm",
      "updated": "2019-06-27 15:39:58"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Scientists discover how plants breathe    and how humans shaped their  lungs ",
      "description": "Experts reveal how plants provide a steady flow of air to every cell  Study shows humans have bred wheat plants to have fewer pores on their leaves and use less water  Findings pave the way to develop more drought resistant crops \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/SItoeMVYuh0/190627113953.htm",
      "updated": "2019-06-27 15:39:53"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": " Mystical  psychedelic compound found in normal brains of rats",
      "description": "A study in rats has revealed the presence of naturally occurring dimethyltryptamine  a hallucinogen \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/Avt3kDydbGQ/190627113951.htm",
      "updated": "2019-06-27 15:39:51"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Researchers grow active mini brain networks",
      "description": "Cerebral organoids are artificially grown  D tissue cultures that resemble the human brain  Now  researchers report success with functional neural networks derived from these organoids  Although the organoids aren t actually  thinking   the researchers  new tool    which detects neural activity using organoids    could provide a method for understanding human brain function \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/gEYnAZ86fPY/190627113945.htm",
      "updated": "2019-06-27 15:39:45"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Some corals can survive in acidified ocean conditions  but have lower density skeletons",
      "description": "Coral reefs face many challenges to their survival  including the global acidification of seawater as a result of rising carbon dioxide levels in the atmosphere  A new study shows that at least three Caribbean coral species can survive and grow under conditions of ocean acidification more severe than those expected to occur during this century  although the density of their skeletons was lower than normal \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/8KOzs77Oops/190626171230.htm",
      "updated": "2019-06-26 21:12:30"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "A new normal  Study explains universal pattern in fossil record",
      "description": "Instead of the typical bell shaped curve  the fossil record shows a fat tailed distribution  with extreme  outlier events occurring with higher than expected probability  Using the same mathematical tools that describe stock market crashes  scientists explain the evolutionary dynamics that give rise to universal patterns in the fossil record \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/aUp9q5bUW6k/190626160341.htm",
      "updated": "2019-06-26 20:03:41"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Honeybees infect wild bumblebees through shared flowers",
      "description": "Viruses in managed honeybees are spilling over to wild bumblebee populations though the shared use of flowers  a first of its kind study reveals  This research suggests commercial apiaries may need to be kept away from areas where there are vulnerable native pollinator species  like the endangered rusty patched bumblebee \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/z-q9xq-kGro/190626160339.htm",
      "updated": "2019-06-26 20:03:39"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "What made humans  the fat primate  ",
      "description": "How did humans get to be so much fatter than our closest primate relatives  despite sharing   of the same DNA  A new study suggests that part of the answer may have to do with an ancient molecular shift in how DNA is packaged inside fat cells  which curbed our body s ability to turn  bad  white fat into  good  brown fat \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/zpizOEEcXPY/190626160337.htm",
      "updated": "2019-06-26 20:03:37"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Translating proteins into music  and back",
      "description": "In a surprising marriage of science and art  researchers have developed a system for converting the molecular structures of proteins  the basic building blocks of all living beings  into audible sound that resembles musical passages  Then  reversing the process  they can introduce some variations into the music and convert it back into new proteins never before seen in nature \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/OoXv7l99qOs/190626125052.htm",
      "updated": "2019-06-26 16:50:52"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Thunderbolts of lightning  gamma rays exciting",
      "description": "Scientists have discovered a connection between lightning strikes and two kinds of gamma ray phenomena in thunderclouds  The research suggests that in certain conditions  weak gamma ray glows from thunderclouds may precede lightning bolts and their accompanying gamma ray flashes \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/wi-FTMifCT4/190626125004.htm",
      "updated": "2019-06-26 16:50:04"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "ALMA pinpoints the formation site of planet around nearest young star",
      "description": "Researchers found a small dust concentration in the disk around TW Hydrae  the nearest young star  It is highly possible that a planet is growing or about to be formed in this concentration  This is the first time that the exact place where cold materials are forming the seed of a planet has been pinpointed in the disk around a young star \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/p-SgcukVFTo/190626124952.htm",
      "updated": "2019-06-26 16:49:52"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "How octopus arms make decisions",
      "description": "Researchers studying the behavior and neuroscience of octopuses have long suspected that the animals  arms may have minds of their own  A new model is the first attempt at a comprehensive representation of information flow between the octopus s suckers  arms and brain  based on previous research in octopus neuroscience and behavior  and new video observations conducted in the lab \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/QmdXsCZBD_Y/190625102420.htm",
      "updated": "2019-06-25 14:24:20"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Could coffee be the secret to fighting obesity ",
      "description": "Scientists have discovered that drinking a cup of coffee can stimulate  brown fat   the body s own fat fighting defenses  which could be the key to tackling obesity and diabetes \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/FQePbvutKco/190624111622.htm",
      "updated": "2019-06-24 15:16:22"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "New therapy targets gut bacteria to prevent and reverse food allergies",
      "description": "A new study identifies the species of bacteria in the human infant gut that protect against food allergies  finding changes associated with the development of food allergies and an altered immune response \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/cZjahm7miYo/190624111545.htm",
      "updated": "2019-06-24 15:15:45"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Damage to the ozone layer and climate change forming feedback loop",
      "description": "Increased solar radiation penetrating through the damaged ozone layer is interacting with the changing climate  and the consequences are rippling through the Earth s natural systems  effecting everything from weather to the health and abundance of sea mammals like seals and penguins \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/mM4qeAGk2_E/190624111536.htm",
      "updated": "2019-06-24 15:15:36"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": " Bathtub rings  around Titan s lakes might be made of alien crystals",
      "description": "The frigid lakeshores of Saturn s moon Titan might be encrusted with strange  unearthly minerals  according to new research \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/jvWradhPC2M/190624111528.htm",
      "updated": "2019-06-24 15:15:28"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Scientists map huge undersea fresh water aquifer off U S  Northeast",
      "description": "In a new survey of the sub seafloor off the U S  Northeast coast  scientists have made a surprising discovery  a gigantic aquifer of relatively fresh water trapped in porous sediments lying below the salty ocean  It appears to be the largest such formation yet found in the world \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/T4FKvu7xyog/190621140341.htm",
      "updated": "2019-06-21 18:03:41"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Astronomers see  warm  glow of Uranus s rings",
      "description": "Two telescopes have measured the faint heat from the main  or epsilon ring  of Uranus  enabling astronomers for the first time to determine its temperature  a cool  Kelvin  Earlier images of the rings came from reflected light only  The observations also show that the rings lack dust  which is common in the rings of other planets  and are composed of centimeter sized particles and larger \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/wjg2PBAQgKk/190620153544.htm",
      "updated": "2019-06-20 19:35:44"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Animals  brain activity  syncs  during social interactions",
      "description": "Egyptian fruit bats and mice  respectively  can  sync  brainwaves in social situations  The synchronization of neural activity in the brains of human conversation partners has been shown previously  as a result of one person picking up social cues from the other and modulating their own behavior based on those cues  These studies suggest that something similar occurs when animals engage in natural social interactions \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/mIRUzutcVnQ/190620153538.htm",
      "updated": "2019-06-20 19:35:38"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Discovery of a  holy grail  with the invention of universal computer memory",
      "description": "A new type of computer memory to solve the digital technology energy crisis has been invented and patented by scientists  The device is the realization of the decades long search for a  Universal Memory  to replace the   billion market for Dynamic Random Access Memory  DRAM  and flash drives  It promises to transform daily life with its ultra low energy consumption  allowing computers which do not need to boot up and which could sleep between key strokes \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/c-Sv2fkDBmU/190620100015.htm",
      "updated": "2019-06-20 14:00:15"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Narwhals and belugas can interbreed",
      "description": "A team of researchers has compiled the first and only evidence that narwhals and beluga whales can breed successfully  DNA and stable isotope analysis of an anomalous skull from the Natural History Museum of Denmark has allowed researchers to confirm the existence of a narwhal beluga hybrid \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/MepsSK0h4Bw/190620100011.htm",
      "updated": "2019-06-20 14:00:11"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Fresh look at mysterious Nasca lines in Peru",
      "description": "Using a taxonomic approach  scientists have re identified the huge birds drawn on the desert plains of Peru as hermits or pelicans \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/1F4GpIx5h6M/190619232324.htm",
      "updated": "2019-06-20 03:23:24"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "First ever successful mind controlled robotic arm without brain implants",
      "description": "Researchers have made a breakthrough in the field of noninvasive robotic device control  Using a noninvasive brain computer interface  BCI   researchers have developed the first ever successful mind controlled robotic arm exhibiting the ability to continuously track and follow a computer cursor \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/LFhTKkxM0MM/190619142542.htm",
      "updated": "2019-06-19 18:25:42"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Melting of Himalayan glaciers has doubled in recent years",
      "description": "A newly comprehensive study shows that melting of Himalayan glaciers caused by rising temperatures has accelerated dramatically since the start of the st century \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/-TaNjh2cGSU/190619142538.htm",
      "updated": "2019-06-19 18:25:38"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "New research shows an iceless Greenland may be in our future",
      "description": "New research shows that Greenland may be ice free by the year   This research uses new data on the landscape under the ice to make breakthroughs in modeling the island s future  The findings show if greenhouse gas concentrations remain on their current path  the melting ice from Greenland alone could contribute as much as  feet to global sea level rise by the time it disappears \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/XA0dBrD73-o/190619142536.htm",
      "updated": "2019-06-19 18:25:36"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "U S  beekeepers lost over  percent of colonies last year  highest winter losses ever recorded",
      "description": "Beekeepers across the United States lost  percent of their honey bee colonies from April  to April   according to preliminary results of the latest annual nationwide survey conducted by the University of Maryland led nonprofit Bee Informed Partnership  The survey results indicate winter losses of  percent  which is the highest winter loss reported since the survey began  years ago and  percentage points higher than the survey average \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/SbAA6Rq14qA/190619142532.htm",
      "updated": "2019-06-19 18:25:32"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Plate tectonics may have driven  Cambrian Explosion ",
      "description": "The quest to discover what drove one of the most important evolutionary events in the history of life on Earth has taken a new  fascinating twist \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/5ckWiSSJuiA/190619130315.htm",
      "updated": "2019-06-19 17:03:15"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Antarctic marine life recovery following the dinosaurs  extinction",
      "description": "A new study shows how marine life around Antarctica returned after the extinction event that wiped out the dinosaurs  A team studied just under  marine fossils collected from Antarctica to understand how life on the sea floor recovered after the Cretaceous Paleogene  K Pg  mass extinction  million years ago  They reveal it took one million years for the marine ecosystem to return to pre extinction levels \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/bSRtwQIc_MU/190619104642.htm",
      "updated": "2019-06-19 14:46:42"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Astronomers make first detection of polarized radio waves in Gamma Ray Burst jets",
      "description": "Astronomers detect polarized radio waves from a gamma ray burst for the first time  Polarization signature reveals magnetic fields in explosions to be much more patchy and tangled than first thought  Combining the observations with data from X ray and visible light telescopes is helping unravel the mysteries of the universe s most powerful explosions \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/u0yZPqy6U5k/190619103207.htm",
      "updated": "2019-06-19 14:32:07"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "The fellowship of the wing  Pigeons flap faster to fly together",
      "description": "Homing pigeons fit in one extra wingbeat per second when flying in pairs compared to flying solo  new research reveals \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/R-xDH1J0oys/190618140201.htm",
      "updated": "2019-06-18 18:02:01"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Two new Earth like planets discovered near Teegarden s Star",
      "description": "An international research team has discovered two new Earth like planets near one of our closest stars  Teegarden s Star is about  light years away and is one of the smallest known stars \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/XC7Jk7y5Hnc/190618123509.htm",
      "updated": "2019-06-18 16:35:09"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Dark centers of chromosomes reveal ancient DNA",
      "description": "Geneticists exploring the dark heart of the human genome have discovered big chunks of Neanderthal and other ancient DNA  The results open new ways to study both how chromosomes behave during cell division and how they have changed during human evolution \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/SdDiwhN4acc/190618105024.htm",
      "updated": "2019-06-18 14:50:24"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Origin of life  A prebiotic route to DNA",
      "description": "DNA  the hereditary material  may have appeared on Earth earlier than has been assumed hitherto  Chemists now show that a simple reaction pathway could have given rise to DNA subunits on the early Earth \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/xK8V2AmZLdc/190618103721.htm",
      "updated": "2019-06-18 14:37:21"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "The evolution of puppy dog eyes",
      "description": "Dogs have evolved new muscles around the eyes to better communicate with humans  New research comparing the anatomy and behavior of dogs and wolves suggests dogs  facial anatomy has changed over thousands of years specifically to allow them to better communicate with humans \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/BvZq7VJT-go/190617175625.htm",
      "updated": "2019-06-17 21:56:25"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "NASA s Cassini reveals New Sculpting in Saturn Rings",
      "description": "As NASA s Cassini dove close to Saturn in its final year  the spacecraft provided intricate detail on the workings of Saturn s complex rings  new analysis shows \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/EAkf78ivDEQ/190617164658.htm",
      "updated": "2019-06-17 20:46:58"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Sun s history found buried in Moon s crust",
      "description": "The Sun s rotation rate in its first billion years is unknown  Yet  this spin rate affected solar eruptions  influencing the evolution of life  Scientists think they ve figured it out by using the Moon as critical evidence \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/3Hpjv8snz40/190617164640.htm",
      "updated": "2019-06-17 20:46:40"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "The brain consumes half of a child s energy    and that could matter for weight gain",
      "description": "A new study proposes that variation in the energy needs of brain development across kids    in terms of the timing  intensity and duration of energy use    could influence patterns of energy expenditure and weight gain \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/VjNfNBqjAmI/190617164629.htm",
      "updated": "2019-06-17 20:46:29"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Meteors help Martian clouds form",
      "description": "Researchers think they ve solved the long standing mystery of how Mars got all of its clouds \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/zL35r66Vbb4/190617125120.htm",
      "updated": "2019-06-17 16:51:20"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Global commodities trade and consumption place the world s primates at risk of extinction",
      "description": "A recent study highlights the fact that the economic benefits of commodity export for primate habitat countries has been limited relative to the extreme environmental costs of pollution  habitat degradation  loss of biodiversity  continued food insecurity and the threat of emerging diseases \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/L6SAsmw_K0o/190617071714.htm",
      "updated": "2019-06-17 11:17:14"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Small cluster of neurons is off on switch for mouse songs",
      "description": "Researchers have isolated a cluster of neurons in a mouse s brain that are crucial to making the squeaky  ultrasonic  songs  a male mouse produces when courting a potential mate \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/PsUNcioAukw/190614163153.htm",
      "updated": "2019-06-14 20:31:53"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Gut microbes eat our medication",
      "description": "Researchers have discovered one of the first concrete examples of how the microbiome can interfere with a drug s intended path through the body  Focusing on levodopa  L dopa   the primary treatment for Parkinson s disease  they identified which bacteria out of the trillions of species is responsible for degrading the drug and how to stop this microbial interference \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/DerbLlIDMUc/190613143629.htm",
      "updated": "2019-06-13 18:36:29"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Viruses found to use intricate  treadmill  to move cargo across bacterial cells",
      "description": "Using advanced technologies to explore the inner workings of bacteria  biologists have provided the first example of cargo within bacteriophage cells transiting along treadmill like structures  The discovery demonstrates that bacteria have more in common with sophisticated human cells than previously believed \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/EbyuAEryj0g/190613143511.htm",
      "updated": "2019-06-13 18:35:11"
    },
    {
      "name": "All Top News -- ScienceDaily",
      "category": "科学",
      "title": "Squid could thrive under climate change",
      "description": "When scientists subjected two toned pygmy squid and bigfin reef squid to carbon dioxide levels projected for the end of the century  they received some surprising results \n    \n",
      "link": "http://feeds.sciencedaily.com/~r/sciencedaily/top_news/top_science/~3/DarVzECpx_Q/190613123713.htm",
      "updated": "2019-06-13 16:37:13"
    }
  ],
  [
    {
      "name": "内科開業医のお勉強日記",
      "category": "医療系",
      "title": "COPD 在宅vs外来リハビリテーション",
      "description": "日本では、介護保険下でのリハビリテーションが、あたかも医療ベースのリハビリテーションと同等の有益性を有することが担保されているかのような行政が行われている維持期リハビリの介護保険への移行を月末に完了全日病ニュース・紙面PDF 年月日号あいかわらず、エビデンス無き行政が続くリハビリテーション特化専門職によるスーパーバイズされたリハビリテーションと、在宅での自己管理運動ではやはりその有効性が異なるだろう、それを示唆する報告Home versus outpatient pulmonary rehabilitation in COPD  a propensity matched cohort studyClaire Marie Nolan  et al supervised外来呼吸リハビリテーション PR と同等の戦略としての在宅ベースの運動療法が提案されているが、臨床実践の場に導入してトライアルで観察下での同様ベネフィット得られているかの検証は不明瞭。リアルワールドでの名COPD 在宅ベース運動療法とマッチ化したsupervised PRとの比較検証在宅ベース運動では、PRに比較して運動能力改善程度少なかったが、QOLの軽度改善ありここで、在宅運動療法と比較してのsupervised PRは標準ケアとしてやはりその地位は維持されることとなった、PRへのattend不能の場合にのみ有効性は低下するが在宅ベースの運動はその意味はある",
      "link": "https://kaigyoi.blogspot.com/2019/07/copdvs.html",
      "updated": "2019-07-08 00:14:46"
    },
    {
      "name": "内科開業医のお勉強日記",
      "category": "医療系",
      "title": "非結核性抗酸菌とアスペルギルスの共感染",
      "description": "免疫不全や既存肺疾患での両者の相互作用、特に、M  aviumとAspergillus fumigatusの関連性は重要なようだNontuberculous mycobacterial pulmonary disease and Aspergillus co infection  Bonnie and Clyde Kim Geurts  Sanne   et al 非結核性抗酸菌 NTM は治療困難な日和見感染で、多くは肺に多く感染する。COPD、のう胞性線維症、気管支拡張 日本では結核後遺症も含まれると思う 患者にNTMによる肺疾患 NTM PD がかかりやすく、同時に他の日和見感染、Aspergillus fumigatusも含まれる。NTMと慢性肺アスペルギルス症 CPA はオーバーラップするという報告    Griffith DE  Aksamit T  Brown Elliot BA  Catanzaro A  Daley C  Gordin F  Holland SM  Horsburgh R  Huitt G  Iademarco MF  Iseman M  Olivier K  Ruoss S  von Reyn CF  Wallace RJ Jr  Winthrop K  An official ATS IDSA statement  diagnosis  treatment  and prevention of nontuberculous mycobacterial diseases  Am J Respir Crit Care Med          Denning DW  Cadranel J  Beigelman Aubry C  Ader F  Chakrabarti A  Blot S  Ullmann AJ  Dimopoulos G  Lange C  Chronic pulmonary aspergillosis  rationale and clinical guidelines for diagnosis and management  Eur Respir J        文献上はNTMとアスペルギルスの同時感染で死亡率増加と関連    Jhun BW  Jung WJ  Hwang NY  Park HY  Jeon K  Kang ES  Koh WJ  Risk factors for the development of chronic pulmonary aspergillosis in patients with nontuberculous mycobacterial lung disease  PLoS One     e  doi   journal pone   従って、NTM PD診断work upの一貫として筆者等は、アスペルギルス血清検査を行っているとのこと年月から年月の間にNTM PDのAmerican Thoracic Society ATS 診断基準を満たし、NTM PDの診断時または紹介時     か月 にAspergillus IgGの血清学的検査結果が得られた患者の検討で、日本にとってありがたいことにのう胞性線維症を除外している。検査Positive IgG serology for Aspergillus was defined as  gt  mg l  as recommended by the manufacturer  ImmunoCAP  Phadia ThermoFisher  Landsmeer  the Netherlands ここでは名検討、女性 、平均年齢±歳アスペルギルスIgG血清学陽性名 M  avium complex  MAC で 、M  abscessusで  平均レベルは± mg  LアスペルギルスIgG陽性線維性空洞     、結節性気管支拡張     喀痰培養アスペルギルス陽性、唾液のみ 、BAL 、BAL及び喀痰人の患者は、陽性の血清学および培養に基づいてアゾール ボリコナゾール、イトラコナゾール、ポサコナゾール 療法を受けた。抗真菌治療は、NTM PDの培養転換率 p    または微生物学的治癒率 p    のいずれにも有意な影響を及ぼさなかった。全体として、人のNTM PD患者のうち人   がNTM PDの治療を受け、そのうち人   がヶ月以上  MAC、 M 膿瘍、 M  kansasii、 M  simiae  M xenopi 。人のMAC PD患者   が、リファマイシン エタンブトール マクロライド系レジメンで治療された。人   の患者がクロファジミン エタンブトール マクロライド系レジメンを受けた。人が追加のアミカシンおよび またはクロファジミンを受けた。ヶ月以上にわたって治療された患者におけるNTM培養転換は、アスペルギルスIgGが陰性であった患者  、   p    よりも、アスペルギルスIgGが陽性であった患者  、  でより少なかった。ヶ月以上治療患者NTM培養conversionはアスペルギルスIgG抗体陰性より陽性患者で低い      vs     p  。微生物学的治癒率もアスペルギルスIgG抗体陽性で低い     vs       p NTM喀痰培養陰性化までの期間は有意差無し、MAC PD患者ではアスペルギルスIgG陽性では培養陰転化率低い   vs   p  spergillus fumigatusは、M abscessus上清を添加した培地では成長速度が著しく低下した。avium結核菌上清はA  fumigatusの増殖速度を増加させた。この効果は、固定相上清で顕著。NTMの方の増殖はA  fumigatus上清の影響を受けずアスペルギルスの共感染についてNTM患者をスクリーニングすることは臨床的に意義があるようだ。特に、M aviumはアスペルギルスの増殖刺激作用がある。",
      "link": "https://kaigyoi.blogspot.com/2019/07/blog-post_5.html",
      "updated": "2019-07-05 02:59:14"
    },
    {
      "name": "内科開業医のお勉強日記",
      "category": "医療系",
      "title": "COPDへの新たな治療ターゲット  カテプシンS",
      "description": "Protein Phosphatase A Reduces Cigarette Smoke induced Cathepsin S and Loss of Lung FunctionDeclan F  Doherty   et al AJRCCM  Vol    No    Jul      nbsp   nbsp   nbsp PubMed  Rationale  CTSS  cathepsin S はcysteine proteaseで、COPD患者のBALF中・血中に高濃度に存在Objectives  nbsp  CTSSがタバコ喫煙誘因COPDの病因と関連し、upstream signalingのtargetingが疾患予防となるかMethods  CTSS expressionを動物・ヒト組織とCOPD細胞モデルで検証Ctss    miceを長期タバコ喫煙暴露し、強制オシレーションと呼気測定動物にPPA活性 protein phosphatase A  chemical modulatorを投与Measurements and Main Results 喫煙暴露後のマウス肺でのCTSS発現促進と活性増加を見いだした。Ctss    miceで喫煙暴露炎症、気道過敏性、気腔拡大、肺機能低下抵抗性あり。CTSS expressionは、健常ヒト非喫煙者とCOPDドナーからの分離気管支上皮細胞、探求由来マクロファージでPPAによりnegativeに調節されているPPA expressionあるいは活性調整、silencer siRNAや、化学的阻害、activatorで、急性喫煙暴露後肺内の炎症性反応やCTSS expressionやactivityをマウスにおいて調整できるPPA activity enhancementはマウスにおいて慢性喫煙由来COPD予防できたConclusions この研究から、COPDで生じるPPA活性の減少が肺内CTSS発現増加に寄与、それから肺機能障害をもたらすことを示している。 nbsp PPA活性を増強することは、CTSS活性を低下させそして煙誘発性肺疾患に対抗するための実行可能な治療アプローチを示すかも。",
      "link": "https://kaigyoi.blogspot.com/2019/07/copds.html",
      "updated": "2019-07-04 01:28:46"
    },
    {
      "name": "内科開業医のお勉強日記",
      "category": "医療系",
      "title": "COPD入院・死亡率イベント予測有用性は固定比で十分かもしれない",
      "description": "これで注意しないといけないのは、ここのスパイロメトリデータはすべて気管支拡張剤投与前の値であり、GOLDガイドライン推奨の拡張剤後FEV FVCではない年齢など考慮された正常下限 LLNの方を採用しようという動きがもたげていたが、利点はさほどないのかもしれないDiscriminative Accuracy of FEV FVC Thresholds for COPD Related Hospitalization and MortalitySurya P  Bhatt  et al JAMA         doi  jama  研究意義 nbsp 多数の現行ガイドラインによると、COPD診断にはFEV FVC 未満が必要だが、この固定閾値はexpert opinionに基づくだけで、議論の余地あるところである目的 nbsp  COPD関連入院・死亡率推定としてのFEV FVC固定閾値のdiscriminative accuracyの決定デザイン・セッティング・被験者National Heart  Lung  and Blood Institute  NHLBI  Pooled Cohorts Studyつの一般住民ベースコホート Atherosclerosis Risk in Communities Study  Cardiovascular Health Study  Health  Aging  and Body Composition Study  and Multi Ethnic Study of Atherosclerosis のharmonizedおよびpooled data解析 歳を 年登録、年まで長軸的フォローアップ暴露気流閉塞 airflow obstruction の存在 定義としてFEV FVC固定閾値レンジ 未満、Global Lung Initiative reference equations  LLN により正常下限定義未満主要アウトカム・測定項目プライマリアウトカム COPD入院、COPD関連死亡率 裁定、行政的クライテリア定義 至適固定FEV FVC閾値定義  COPD関連イベントに対する、非補正Cox比例ハザードモデルからのHarrel C統計を用石評価したbest discriminationnonparametric approachにてC統計の差は、未満とLLN未満を比較結果pool化コホート成人 登録時平均 SD 年齢    歳 女性   、非ヒスパニック白人    喫煙歴     、年時点で   フォローアップ完遂フォローアップ中央期間年間の間に、人年フォローアップ中COPD関連イベント、内、COPD関連入院、COPD関連死COPD 関連イベントdiscriminationに関し、至適固定閾値  至適固定閾値に対するC統計   は、閾値に対して有意差認めず difference      CI    to   、しかし、LLN閾値に比べaccuracyは優れていた difference      CI   to   閾値は喫煙既往サブグループや補正モデルでも至適性を堅持結論と知見 nbsp  FEV FVC 未満という気道閉塞定義は、他の指標と差は無いという意味でCOPD関連入院、死亡率の判別として役立ち、他の固定指標やLLNよりaccuracyが高いこれらはFEV FVC 未満が臨床的意義あるCOPDリスク状態個人を特定するのに役立つことが支持される。でも、これみると、早期発見を主眼とする場合、FEV FVC まで対象拡大も考慮すべきかも",
      "link": "https://kaigyoi.blogspot.com/2019/06/copd-07.html",
      "updated": "2019-06-27 02:37:26"
    },
    {
      "name": "内科開業医のお勉強日記",
      "category": "医療系",
      "title": "日本 食事炎症性指数と全死亡・心血管死亡率の関連性ありがん死亡は否定的",
      "description": "Japan Collaborative Cohort Study 名 nbsp Dietary Inflammatory Index  DII  scoreとの関連を全原因、総心血管疾患 CVD 、卒中、肝動脈性心疾患 CHD 、がん全部、消化器系がん、非がん性 非CVD死亡率で検討中央値年間フォローアップCVD死亡率多変量ハザードリスク最大値は、総CVD、卒中、CHDでそれぞれ、、DIIとがん全体リスクとの有意関連性見いだせず日本人成人DII最大値は全死亡率、CVD死亡率増加と関連が観察されたDietary Inflammatory Index Is Associated with Risk of All Cause and Cardiovascular Disease Mortality but Not with Cancer Mortality in Middle Aged and Older Japanese AdultsEmiko Okada nbsp    et al The Journal of Nutrition  nxz  Shivappa N  Steck SE  Hurley TG  Hussey JR  Hébert JR  Designing and developing a literature derived  population based dietary inflammatory index Public Health Nutr     米国NHANES、韓国コホート、ベルギー、イラン、オーストラリアの研究同様に、日本のデータでもDIIスコアとhs CRPと正相関ありYang Y  Hozawa A  Kogure M  Narita A  Hirata T  Nakamura T  Tsuchiya N  Nakaya N  Ninomiya T  Okuda N  et al Dietary inflammatory index positively associated with high senditivity C  reactive protein level in Japanese from NIPPON DATA   Internet   J Epidemiol   https   doi  jea JE nbsp  日本と西欧諸国では食生活は大きく異なるが、炎症誘発性の食事を示唆する高いDIIスコアと、全原因および全CVD死亡率のリスクとの関連性に関する我々の結果は、西欧集団の研究結果と一致DIIが炎症の間接的なマーカーであるにもかかわらず、炎症促進性および抗炎症性の可能性を予測する能力に起因する可能性がある今回のは限定的アンケートに過ぎないが、ライザップって、日ごとの食事内容データ集積してるんだよなぁって誰かが言ってたが・・・",
      "link": "https://kaigyoi.blogspot.com/2019/06/blog-post_26.html",
      "updated": "2019-06-26 01:08:10"
    },
    {
      "name": "内科開業医のお勉強日記",
      "category": "医療系",
      "title": "高齢者スタチン アドヒアランスと死亡率",
      "description": "高齢者スタチン・アドヒアランス傾向と高アドヒアランス関連決定因子の後顧的住民ベースコホート解析スタチンアドヒアランスと全死亡率の関連性検討南カリフォルニア地区の急性心筋梗塞入院、名の検討スタチン治療高アドヒアランスは で報告、一部アドヒアランス 、アドヒアランス無し 高アドヒアランスは男性、白人観察期間±年間、知見としては、高アドヒアランスは心筋梗塞後生存率改善と相関CLINICAL INVESTIGATION Statin Adherence and Mortality in Patients Aged  Years and Older After Acute Myocardial InfarctionJournal of the American Geriatrics Society     後顧的研究であり、さらに徹底的に共役要素補正されてるか疑問が残るが、  nbsp This association may not have been due only to adherence to statins but to other related factors as well   と結論づけされている。高齢者スタチン治療の安全性と有効性The Lancet ーArmitage J  et al    February   タチン治療は重大血管イベントに関しては年齢にかかわらず有意な減少効果を示すが、歳超患者では、閉塞性血管疾患のエビデンスを有さない場合、ベネフィット直接エビデンス減少。今後この限界に関して将来トライアル検討高齢者でもスタチン有効のサブグループとして“心筋梗塞既往 はその候補 “スタチン全て無用である宗教 はやっぱりダメだろ",
      "link": "https://kaigyoi.blogspot.com/2019/06/blog-post_24.html",
      "updated": "2019-06-24 06:01:14"
    }
  ],
  [
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "auがプリペイド携帯電話サービスを年月で終了 不景気 com",
      "description": "「KDDI」と「沖縄セルラー」は、年月日をもってauのプリペイド   ",
      "link": "https://www.fukeiki.com/2019/07/au-pullout-prepaid.html",
      "updated": "2019-07-07 19:45:50"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "サーラコーポレーションが子会社「エス・アール・ピー」を解散 不景気 com",
      "description": "東証・名証部上場で「中部ガス」や住宅建築・販売の「サーラ住宅」を傘下に持つ「サ   ",
      "link": "https://www.fukeiki.com/2019/07/sala-liquidate-srp.html",
      "updated": "2019-07-07 19:41:29"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "週刊不景気ニュース 、破産やリストラの話題多く 不景気 com",
      "description": "月日から日までに起こった不景気な出来事を総括する「週刊不景気ニュース」。破   ",
      "link": "https://www.fukeiki.com/2019/07/week-report-190707.html",
      "updated": "2019-07-07 11:22:43"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "ミニストップの年第四半期は億円の赤字、店舗閉店 不景気 com",
      "description": "東証部上場でイオングループのコンビニエンスストア「ミニストップ」は、年   ",
      "link": "https://www.fukeiki.com/2019/07/ministop-2020-1q-loss.html",
      "updated": "2019-07-06 05:08:23"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "イオンの第四半期は億円の最終赤字、子会社不正会計 不景気 com",
      "description": "東証部上場の流通大手「イオン」は、年月期第四半期決算短信を発表し、   ",
      "link": "https://www.fukeiki.com/2019/07/aeon-2020-1q-loss.html",
      "updated": "2019-07-06 04:53:02"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "TATERUが早期退職で名の削減へ、従業員分の相当 不景気 com",
      "description": "東証部上場でアパート経営支援などを手掛ける「TATERU」 旧商号 インベスタ   ",
      "link": "https://www.fukeiki.com/2019/07/tateru-cut-160-job.html",
      "updated": "2019-07-05 06:43:17"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "アパレル「ヰノセント」が弁護士一任し特別清算へ、負債億円 不景気 com",
      "description": "東京都渋谷区に本拠を置くアパレルメーカーの「株式会社ヰノセント」は、月日付で   ",
      "link": "https://www.fukeiki.com/2019/07/innocent.html",
      "updated": "2019-07-05 06:22:59"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "沖縄の通信回線販売取次「電信サービス」に破産決定 不景気 com",
      "description": "官報によると、沖縄県那覇市に本拠を置く通信回線販売取次の「電信サービス株式会社」   ",
      "link": "https://www.fukeiki.com/2019/07/denshin-service.html",
      "updated": "2019-07-05 05:43:57"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "山口・下関の船舶修繕「イノテック」に破産開始決定 不景気 com",
      "description": "官報によると、山口県下関市に本拠を置く船舶建造・修繕の「株式会社イノテック」は、   ",
      "link": "https://www.fukeiki.com/2019/07/inotec.html",
      "updated": "2019-07-04 19:18:33"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "奈良の飲料水製造・販売「三才堂」に破産開始決定 不景気 com",
      "description": "官報によると、奈良県東吉野村に本拠を置く飲料水製造・販売の「三才堂株式会社」は、   ",
      "link": "https://www.fukeiki.com/2019/07/sansaidou.html",
      "updated": "2019-07-04 18:33:44"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "高知の石油製品販売「土佐米油」に破産開始決定 不景気 com",
      "description": "官報によると、高知県高知市に本拠を置く石油製品販売の「土佐米油株式会社」は、月   ",
      "link": "https://www.fukeiki.com/2019/07/shinsengumi-tosa.html",
      "updated": "2019-07-04 15:50:05"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "福岡の油回収装置販売「亀屋」に破産開始決定 不景気 com",
      "description": "官報によると、福岡県春日市に本拠を置く油回収装置販売の「株式会社亀屋」は、月   ",
      "link": "https://www.fukeiki.com/2019/07/fukuoka-kameya.html",
      "updated": "2019-07-04 08:57:57"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "セレクトショップの「ハバダッシュリー」が全店舗を閉鎖へ 不景気 com",
      "description": "新潟県新潟市に本拠を置くセレクトショップ運営の「ハバダッシュリー」は、今年月ま   ",
      "link": "https://www.fukeiki.com/2019/07/haberdashery-close.html",
      "updated": "2019-07-07 11:27:18"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "神戸のギター買取販売「ヘンリー・トラスト」が自己破産申請へ 不景気 com",
      "description": "信用調査会社の帝国データバンクによると、兵庫県神戸市に本拠を置く楽器販売の「ヘン   ",
      "link": "https://www.fukeiki.com/2019/07/henry-trust.html",
      "updated": "2019-07-03 18:25:22"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "東京・南馬込の電気工事「フォスター」に破産開始決定 不景気 com",
      "description": "官報によると、東京都大田区南馬込に本拠を置く電気工事の「株式会社フォスター」は、   ",
      "link": "https://www.fukeiki.com/2019/07/foster-style.html",
      "updated": "2019-07-03 18:03:37"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "京都のガレージ・エクステリア工事「コーフク」に破産開始決定 不景気 com",
      "description": "官報によると、京都府宇治市に本拠を置くガレージ工事の「株式会社コーフク」は、月   ",
      "link": "https://www.fukeiki.com/2019/07/kofuku-solar.html",
      "updated": "2019-07-03 13:26:05"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "トライステージが子会社「日本ヘルスケアアドバイザーズ」を解散 不景気 com",
      "description": "東証マザーズ上場でテレビ通販を中心とするダイレクトマーケティング支援の「トライス   ",
      "link": "https://www.fukeiki.com/2019/07/tri-stage-liquidate-nha.html",
      "updated": "2019-07-03 08:42:03"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "リサイクルショップ運営の「サンセットコーポレイション」が民事再生 不景気 com",
      "description": "信用調査会社の東京商工リサーチによると、千葉県市川市に本拠を置くリサイクルショッ   ",
      "link": "https://www.fukeiki.com/2019/07/sunset-corporation.html",
      "updated": "2019-07-07 11:27:19"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "TOEICが「大学入学共通テスト」の参加申込を取り下げ 不景気 com",
      "description": "日本で英語の民間試験「TOEIC」を実施・運営する「国際ビジネスコミュニケーショ   ",
      "link": "https://www.fukeiki.com/2019/07/toeic-pullout-kyotsu-test.html",
      "updated": "2019-07-02 16:50:21"
    },
    {
      "name": "不景気.com",
      "category": "ビジネス",
      "title": "みちのく銀行が取立不能おそれ、取引先「千代田信用」破産で 不景気 com",
      "description": "東証部上場の地方銀行「みちのく銀行」は、保有する債権について取立不能または取立   ",
      "link": "https://www.fukeiki.com/2019/07/michinoku-bank-debt-collection3.html",
      "updated": "2019-07-07 11:27:26"
    }
  ],
  [
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "The lies our culture tells us about what matters     and a better way to live   David Brooks",
      "description": "Our society is in the midst of a social crisis  says op ed columnist and author David Brooks  we re trapped in a valley of isolation and fragmentation  How do we find our way out  Based on his travels across the United States    and his meetings with a range of exceptional people known as  weavers     Brooks lays out his vision for a cultural revolution that empowers us all to lead lives of greater meaning  purpose and joy \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=iB4MS1hsWXU",
      "updated": "2019-07-05 19:34:34"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": " Everything happens for a reason     and other lies I ve loved   Kate Bowler",
      "description": "In life s toughest moments  how do you go on living  Kate Bowler has been exploring this question ever since she was diagnosed with stage IV cancer at age   In a profound  heartbreaking and unexpectedly funny talk  she offers some answers    challenging the idea that  everything happens for a reason  and sharing hard won wisdom about how to make sense of the world after your life is suddenly  completely changed   I believe that in the darkness  even there  there will be beauty and there will be love   she says \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=DTcJmIbn5nw",
      "updated": "2019-07-05 19:32:23"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "Floating cities  the LEGO House and other architectural forms of the future   Bjarke Ingels",
      "description": "Design gives form to the future  says architect Bjarke Ingels  In this worldwide tour of his team s projects  journey to a waste to energy power plant  that doubles as an alpine ski slope  and the LEGO Home of the Brick in Denmark    and catch a glimpse of cutting edge flood resilience infrastructure in New York City as well as an ambitious plan to create floating  sustainable cities that are adapted to climate change \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=ieSV8-isy3M",
      "updated": "2019-07-07 19:59:57"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "The amazing brains and morphing skin of octopuses and other cephalopods   Roger Hanlon",
      "description": "Octopus  squid and cuttlefish    collectively known as cephalopods    have strange  massive  distributed brains  What do they do with all that neural power  Dive into the ocean with marine biologist Roger Hanlon  who shares astonishing footage of the camouflaging abilities of cephalopods  which can change their skin color and texture in a flash  Learn how their smart skin  and their ability to deploy it in sophisticated ways  could be evidence of an alternative form of intelligence    and how it could lead to breakthroughs in AI  fabrics  cosmetics and beyond \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=ogCIqaCe2zI",
      "updated": "2019-07-03 21:12:47"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "What it takes to launch a telescope   Erika Hamden",
      "description": "TED Fellow and astronomer Erika Hamden leads the team building FIREBall  a telescope that hangs from a giant balloon at the very edge of space and looks for clues about how stars are created  She takes us inside the roller coaster  decade long journey to get the telescope from an idea into orbit    and shows how failure is inevitable when you re pushing the limits of knowledge \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=aVOJA8qI7oo",
      "updated": "2019-06-29 23:32:36"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "How to deconstruct racism  one headline at a time   Baratunde Thurston",
      "description": "Baratunde Thurston explores the phenomenon of white Americans calling the police on black Americans who have committed the crimes of     eating  walking or generally  living while black   In this profound  thought provoking and often hilarious talk  he reveals the power of language to change stories of trauma into stories of healing    while challenging us all to level up \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=RZgkjEdMbSw",
      "updated": "2019-06-29 23:29:51"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "What prosecutors and incarcerated people can learn from each other   Jarrell Daniels",
      "description": "A few weeks before his release from prison  Jarrell Daniels took a class where incarcerated men learned alongside prosecutors  By simply sitting together and talking  they uncovered surprising truths about the criminal justice system and ideas for how real change happens  Now a scholar and activist  Daniels reflects on how collaborative education could transform the justice system and unlock solutions to social problems \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=jATCr-gQvPA",
      "updated": "2019-06-27 20:12:50"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "These bacteria eat plastic   Morgan Vague",
      "description": "Humans produce  million tons of new plastic each year    yet  despite our best efforts  less than  percent of it ends up being recycled  Is there a better way to deal with all this waste  Morgan Vague describes her research with microbiologist Jay Mellies on bacteria that have evolved the unexpected ability to eat plastic    and how they could help us solve our growing pollution problem \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=nbW4XWkJC6w",
      "updated": "2019-06-27 20:47:58"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "My identity is a superpower    not an obstacle   America Ferrera",
      "description": "Hollywood needs to stop resisting what the world actually looks like  says actor  director and activist America Ferrera  Tracing the contours of her career  she calls for more authentic representation of different cultures in media    and a shift in how we tell our stories   Presence creates possibility   she says   Who we see thriving in the world teaches us how to see ourselves  how to think about our own value  how to dream about our futures  \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=RjquHTj4HlY",
      "updated": "2019-06-25 14:56:13"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "The anti CEO playbook   Hamdi Ulukaya",
      "description": "Profit  money  shareholders  these are the priorities of most companies today  But at what cost  In an appeal to corporate leaders worldwide  Chobani founder Hamdi Ulukaya calls for an end to the business playbook of the past    and shares his vision for a new   anti CEO playbook  that prioritizes people over profits   This is the difference between profit and true wealth   he says \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=SGTMSV8QUrs",
      "updated": "2019-06-22 22:50:17"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "The next global agricultural revolution   Bruce Friedrich",
      "description": "Conventional meat production causes harm to our environment and presents risks to global health  but people aren t going to eat less meat unless we give them alternatives that cost the same  or less  and that taste the same  or better   In an eye opening talk  food innovator and TED Fellow Bruce Friedrich shows the plant  and cell based products that could soon transform the global meat industry    and your dinner plate \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=vZCGSP3A0Fo",
      "updated": "2019-06-25 23:14:34"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "Sloths  The strange life of the world s slowest mammal   Lucy Cooke",
      "description": "Sloths have been on this planet for more than  million years  What s the secret to their success  In a hilarious talk  zoologist Lucy Cooke takes us inside the strange life of the world s slowest mammal and shows what we can learn from their ingenious adaptations \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=m19jit19v9w",
      "updated": "2019-06-23 02:57:04"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "How to build your confidence    and spark it in others   Brittany Packnett",
      "description": " Confidence is the necessary spark before everything that follows   says educator and activist Brittany Packnett  In an inspiring talk  she shares three ways to crack the code of confidence    and her dream for a world where revolutionary confidence helps turn our most ambitious dreams into reality \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=b5ZESpOAolU",
      "updated": "2019-06-21 02:06:01"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "How supercharged plants could slow climate change   Joanne Chory",
      "description": "Plants are amazing machines    for millions of years  they ve taken carbon dioxide out of the air and stored it underground  keeping a crucial check on the global climate  Plant geneticist Joanne Chory is working to amplify this special ability  with her colleagues at the Salk Plant Molecular and Cellular Biology Laboratory  she s creating plants that can store more carbon  deeper underground  for hundreds of years  Learn more about how these supercharged plants could help slow climate change   This ambitious plan is a part of the Audacious Project  TED s initiative to inspire and fund global change  \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=pyFcr2WcOyo",
      "updated": "2019-06-16 21:32:19"
    },
    {
      "name": "TED",
      "category": "YouTube Channels",
      "title": "My life as a work of art   Daniel Lismore",
      "description": "Daniel Lismore s closet is probably a bit different than yours    his clothes are constructed out of materials ranging from beer cans and plastic crystals to diamonds  royal silks and  year old Roman rings  In this striking talk  Lismore shares the vision behind his elaborate ensembles and explores what it s like to live life as a work of art   Everyone is capable of creating their own masterpiece   he says   You should try it sometime  \n\nGet TED Talks recommended just for you  Learn more at \n\nThe TED Talks channel features the best talks and performances from the TED Conference  where the world s leading thinkers and doers give the talk of their lives in  minutes  or less   Look for talks on Technology  Entertainment and Design    plus science  business  global issues  the arts and more  You re welcome to link to or embed these videos  forward them to others and share these ideas with people you know  For more information on using TED for commercial purposes  e g  employee learning  in a film or online course   please submit a Media Request here  \n\nFollow TED on Twitter  \nLike TED on Facebook  \n\nSubscribe to our channel  ",
      "link": "https://www.youtube.com/watch?v=8q7D4EmbSCw",
      "updated": "2019-06-15 22:08:08"
    }
  ],
  [
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Here to help  Make today an adventure with Google Maps",
      "description": "Explore the world around you and turn every day into an adventure  with a little help from Google Maps \nMusic   You Make My Dreams  by Daryl Hall   John Oates\n\nSubscribe to our Channel  \nTweet with us on Twitter  \nFollow us on Instagram  \nJoin us on Facebook  ",
      "link": "https://www.youtube.com/watch?v=xVJljUYn8jc",
      "updated": "2019-07-07 19:51:25"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Bridestowe Lavender  Harvesting global business from rural Australia",
      "description": "Using basic digital marketing skills  the owners of an Australian lavender farm transformed their business into a successful destination attracting visitors from around the world to rural Tasmania  Find free training and tools to help you grow your skills  career or business at g co GrowAustralia\n\nSubscribe to our Channel  \nTweet with us on Twitter  \nFollow us on Instagram  \nJoin us on Facebook  ",
      "link": "https://www.youtube.com/watch?v=MoVs4rBhFzs",
      "updated": "2019-07-07 19:33:25"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If summer travelers made a phone plan",
      "description": "If summer travelers made a phone plan  they could share pics abroad for no extra charge  Even the not so great ones  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=NyETDuxp2eI",
      "updated": "2019-07-05 19:27:21"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If a   Fan made a phone plan",
      "description": "If a   fan made a phone plan  it would keep them connected with three G LTE networks in one  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=rwibUlo8mrg",
      "updated": "2019-07-03 01:58:49"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If a Band s   Fan made a phone plan",
      "description": "If a band s   fan made a phone plan  it would work abroad for no extra charge  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=Rtj0ws9S3ic",
      "updated": "2019-07-02 15:42:51"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If the Youngest of Four made a phone plan",
      "description": "If the youngest of four made a phone plan  it would work on his new phone  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=oZCef_1n5IQ",
      "updated": "2019-07-03 01:59:26"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If a Couple of Campers made a phone plan",
      "description": "If a couple of campers made a phone plan  they d only have to pay for the data they use  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=dhlk5FpORqc",
      "updated": "2019-07-03 02:00:05"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If this mom made a phone plan",
      "description": "If this mom made a phone plan  she d only have to pay for data she uses    which isn t much these days    So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=mTstsD7RCHc",
      "updated": "2019-07-07 19:34:51"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If a best friend made a phone plan",
      "description": "If a best friend made a phone plan  their human would only have to pay for data they use  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=Ccnt55gmLA4",
      "updated": "2019-07-02 15:39:55"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If The Umbrella Academy made a phone plan",
      "description": "If The Umbrella Academy made a phone plan  Number  could teleport and roam for no extra charge  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=fsVHyNMGKfc",
      "updated": "2019-07-03 02:01:05"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If the Hawkins AV Club made a phone plan",
      "description": "If the Hawkins AV Club made a phone plan  it would always keep them connected  in this dimension   So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=10LsN9kWwIY",
      "updated": "2019-07-07 19:34:22"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If boyfriends on social made a phone plan",
      "description": "If the boyfriends on social media made a phone plan  they could share all of these pics abroad for no extra charge  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=d1eT2Uge8H0",
      "updated": "2019-07-05 19:17:17"
    },
    {
      "name": "google",
      "category": "YouTube Channels",
      "title": "Google Fi  If a paraglider made a phone plan",
      "description": "If a paraglider made a phone plan  they could stay connected up alllll the way there  So that s how we made it  Learn more at fi google com",
      "link": "https://www.youtube.com/watch?v=5rVXGQfo1Rc",
      "updated": "2019-07-05 19:36:55"
    }
  ]
]